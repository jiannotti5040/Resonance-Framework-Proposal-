<!DOCTYPE html>
<html>
<head>
<title>Tri-Resonance Engine v12.0 - Complete Codebase</title>
<style>
body { font-family: Arial, sans-serif; margin: 20px; background: #f0f0f0; }
.module { background: white; margin: 10px 0; padding: 15px; border-radius: 5px; border-left: 4px solid #007cba; }
.code { background: #2d2d2d; color: #f8f8f2; padding: 15px; border-radius: 3px; overflow-x: auto; font-family: monospace; white-space: pre; margin: 10px 0; }
.copy-btn { background: #007cba; color: white; border: none; padding: 5px 10px; border-radius: 3px; cursor: pointer; margin: 5px 0; }
.copy-btn:hover { background: #005a87; }
.toc { background: white; padding: 15px; border-radius: 5px; margin-bottom: 20px; }
.toc-item { margin: 5px 0; }
</style>
</head>
<body>

<h1>Tri-Resonance Synthesis Engine v12.0</h1>
<h2>Complete Codebase - Definitive Build</h2>

<div class="toc">
<h3>Table of Contents - 23 Modules</h3>
<div class="toc-item"><a href="#dashboard">dashboard.py</a> - Main GUI Application</div>
<div class="toc-item"><a href="#engine">engine.py</a> - Core Backend Orchestrator</div>
<div class="toc-item"><a href="#invariants">invariants.py</a> - Foundational Constants</div>
<div class="toc-item"><a href="#musical_structures">musical_structures.py</a> - Denotator Class</div>
<div class="toc-item"><a href="#physics_model">physics_model.py</a> - Fundamental Sound Physics</div>
<div class="toc-item"><a href="#math_core">math_core.py</a> - Core Mathematical Functions</div>
<div class="toc-item"><a href="#harmony_functional">harmony_functional.py</a> - Harmony Functional</div>
<div class="toc-item"><a href="#advanced_geometry">advanced_geometry.py</a> - Advanced Geometry</div>
<div class="toc-item"><a href="#symbolic_engine">symbolic_engine.py</a> - Symbolic Equation Engine</div>
<div class="toc-item"><a href="#analysis_prime">analysis_prime_framework.py</a> - Prime Translation Framework</div>
<div class="toc-item"><a href="#analysis_sentiment">analysis_sentiment.py</a> - Sentiment Analysis</div>
<div class="toc-item"><a href="#analysis_bounded">analysis_bounded_features.py</a> - Bounded Features</div>
<div class="toc-item"><a href="#analysis_base">analysis_base_metrics.py</a> - Base Audio Metrics</div>
<div class="toc-item"><a href="#analysis_gestural">analysis_gestural.py</a> - Gestural Analysis</div>
<div class="toc-item"><a href="#analysis_classical">analysis_classical_harmony.py</a> - Classical Harmony</div>
<div class="toc-item"><a href="#synthesis_adaptive">synthesis_adaptive_calculus.py</a> - Adaptive Calculus</div>
<div class="toc-item"><a href="#synthesis_generative">synthesis_generative_model.py</a> - Generative Model</div>
<div class="toc-item"><a href="#synthesis_textures">synthesis_procedural_textures.py</a> - Procedural Textures</div>
<div class="toc-item"><a href="#synthesis_geometric">synthesis_geometric_transforms.py</a> - Geometric Transforms</div>
<div class="toc-item"><a href="#humanization_filters">humanization_filters.py</a> - Humanization Filters</div>
<div class="toc-item"><a href="#humanization_psycho">humanization_psychoacoustics.py</a> - Psychoacoustics</div>
<div class="toc-item"><a href="#utils_source">utils_source_manager.py</a> - Source Manager</div>
<div class="toc-item"><a href="#utils_output">utils_output_manager.py</a> - Output Manager</div>
<div class="toc-item"><a href="#utils_logger">utils_logger.py</a> - Logger</div>
<div class="toc-item"><a href="#manifold_explorer">manifold_explorer.py</a> - Manifold Explorer</div>
<div class="toc-item"><a href="#atlas_manager">atlas_manager.py</a> - Atlas Manager</div>
</div>

<!-- MODULE: dashboard.py -->
<div class="module" id="dashboard">
<h3>dashboard.py - Main GUI Application</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# dashboard.py: Tri-Resonance Engine - Dashboard Interface
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# This module provides a graphical user interface (GUI) to control the
# Tri-Resonance Engine, making it easier to manage inputs and view progress.
# ==============================================================================

import tkinter as tk
from tkinter import filedialog, scrolledtext, messagebox, ttk
import engine
import threading
import sys
import os
import webbrowser

class App:
    def __init__(self, root):
        self.root = root
        self.root.title("Tri-Resonance Engine Dashboard v12.0 - The Grand Atlas")
        self.root.geometry("900x700")
        self.root.configure(bg='#1E1E1E')
        
        # Configure style for dark theme
        self.style = ttk.Style()
        self.style.configure('TFrame', background='#1E1E1E')
        self.style.configure('TLabel', background='#1E1E1E', foreground='white')
        self.style.configure('TCheckbutton', background='#1E1E1E', foreground='white')
        
        # Main frame
        main_frame = ttk.Frame(root, padding="10")
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        # Header
        header_frame = ttk.Frame(main_frame)
        header_frame.pack(fill=tk.X, pady=(0, 10))
        
        ttk.Label(header_frame, text="🎵 Tri-Resonance Synthesis Engine", 
                 font=("Helvetica", 16, "bold")).pack(side=tk.LEFT)
        
        ttk.Label(header_frame, text="v12.0 - The Grand Atlas", 
                 font=("Helvetica", 10), foreground="#888").pack(side=tk.LEFT, padx=(10, 0))
        
        # Control Panel
        control_frame = ttk.LabelFrame(main_frame, text="Control Panel", padding="10")
        control_frame.pack(fill=tk.X, pady=(0, 10))
        
        # Input Directory
        dir_frame = ttk.Frame(control_frame)
        dir_frame.pack(fill=tk.X, pady=5)
        
        ttk.Label(dir_frame, text="Input Songs:").pack(side=tk.LEFT)
        self.input_dir_var = tk.StringVar(value=os.path.join(os.getcwd(), "input_songs"))
        self.dir_entry = ttk.Entry(dir_frame, textvariable=self.input_dir_var, width=60)
        self.dir_entry.pack(side=tk.LEFT, padx=5, fill=tk.X, expand=True)
        ttk.Button(dir_frame, text="Browse", command=self.browse_folder).pack(side=tk.LEFT)
        
        # Options Frame
        options_frame = ttk.Frame(control_frame)
        options_frame.pack(fill=tk.X, pady=5)
        
        self.run_explorer_var = tk.BooleanVar(value=True)
        self.explorer_check = ttk.Checkbutton(options_frame, 
                                            text="Explore Musical Space (Send Canaries)",
                                            variable=self.run_explorer_var)
        self.explorer_check.pack(side=tk.LEFT, padx=(0, 20))
        
        self.add_to_atlas_var = tk.BooleanVar(value=True)
        self.atlas_check = ttk.Checkbutton(options_frame,
                                         text="Add to Grand Atlas",
                                         variable=self.add_to_atlas_var)
        self.atlas_check.pack(side=tk.LEFT)
        
        # Run Button
        self.run_button = tk.Button(main_frame, 
                                  text="🚀 BEGIN COSMIC SYNTHESIS", 
                                  font=("Helvetica", 14, "bold"),
                                  command=self.run_synthesis_thread,
                                  bg="#FF6B00",
                                  fg="white",
                                  relief="raised",
                                  bd=0,
                                  padx=20,
                                  pady=10)
        self.run_button.pack(pady=10, fill=tk.X)
        
        # Status Frame
        status_frame = ttk.LabelFrame(main_frame, text="Cosmic Log", padding="5")
        status_frame.pack(fill=tk.BOTH, expand=True)
        
        # Log output
        self.log_text = scrolledtext.ScrolledText(status_frame, 
                                                wrap=tk.WORD,
                                                state='disabled',
                                                bg='#0A0A0A',
                                                fg='#00FF88',
                                                font=("Consolas", 9),
                                                insertbackground='white')
        self.log_text.pack(fill=tk.BOTH, expand=True)
        
        # Footer
        footer_frame = ttk.Frame(main_frame)
        footer_frame.pack(fill=tk.X, pady=(10, 0))
        
        ttk.Button(footer_frame, 
                  text="📖 View Grand Atlas",
                  command=self.view_atlas).pack(side=tk.LEFT)
        
        ttk.Button(footer_frame,
                  text="🎵 Play Primordial Sounds", 
                  command=self.play_primordial).pack(side=tk.LEFT, padx=(10, 0))
        
        # Redirect stdout/stderr to log
        sys.stdout = self.TextRedirector(self.log_text, "stdout")
        sys.stderr = self.TextRedirector(self.log_text, "stderr")
        
        # Print welcome message
        self.print_welcome()

    def print_welcome(self):
        welcome_msg = """
╔════════════════════════════════════════════════════════════════╗
║                    TRI-RESONANCE ENGINE v12.0                 ║
║                      THE GRAND ATLAS                          ║
╟────────────────────────────────────────────────────────────────╢
║ Welcome, Cosmic Traveler!                                     ║
║                                                                ║
║ This engine maps the musical universe, from the vibrations    ║
║ of strings to the harmonics of the cosmos.                    ║
║                                                                ║
║ Place your audio files in 'input_songs' and click below to    ║
║ begin the journey through musical space-time.                 ║
╚════════════════════════════════════════════════════════════════╝

"""
        print(welcome_msg)

    def browse_folder(self):
        directory = filedialog.askdirectory(initialdir=os.getcwd(), 
                                          title="Select Folder Containing Your Songs")
        if directory:
            self.input_dir_var.set(directory)

    def view_atlas(self):
        """Opens the Grand Atlas visualization"""
        # This would open the atlas.html file in a web browser
        atlas_path = os.path.join("output", "grand_atlas.html")
        if os.path.exists(atlas_path):
            webbrowser.open('file://' + os.path.abspath(atlas_path))
        else:
            messagebox.showinfo("Grand Atlas", 
                              "The Grand Atlas will be created after your first synthesis journey!")

    def play_primordial(self):
        """Plays the primordial sounds that seed the universe"""
        print("🎵 Playing the Primordial Sounds of Creation...")
        print("   • Cosmic Vibrations (Pulsar rhythms)")
        print("   • Earth's Heartbeat (Schumann resonances)") 
        print("   • Perfect Harmony (Vibrating strings)")
        print("   • Human Emotion (Voice frequencies)")
        # Actual audio playback would be implemented here

    def run_synthesis_thread(self):
        """Runs synthesis in separate thread to keep GUI responsive"""
        self.log_text.config(state='normal')
        self.log_text.delete(1.0, tk.END)
        self.log_text.config(state='disabled')
        
        self.run_button.config(text="🌌 TRAVERSING MUSICAL SPACE-TIME...", 
                             state='disabled', bg='#444444')
        
        input_dir = self.input_dir_var.get()
        run_explorer = self.run_explorer_var.get()
        add_to_atlas = self.add_to_atlas_var.get()
        
        thread = threading.Thread(target=self.run_engine_safely, 
                                args=(input_dir, run_explorer, add_to_atlas))
        thread.daemon = True
        thread.start()
        
        self.root.after(100, self.check_thread, thread)

    def run_engine_safely(self, input_dir, run_explorer, add_to_atlas):
        """Runs the engine with error handling"""
        try:
            print(f"🎯 Starting Cosmic Synthesis...")
            print(f"📁 Input Directory: {input_dir}")
            print(f"🔍 Space Exploration: {'ON' if run_explorer else 'OFF'}")
            print(f"🗺️  Grand Atlas: {'ON' if add_to_atlas else 'OFF'}")
            print("=" * 60)
            
            engine.run_synthesis_process(input_dir, run_explorer, add_to_atlas)
            
        except Exception as e:
            error_msg = f"\n💥 COSMIC ANOMALY DETECTED!\n{type(e).__name__}: {e}\n"
            print(error_msg, file=sys.stderr)
            self.root.after(0, lambda: messagebox.showerror(
                "Cosmic Anomaly", 
                f"The engine encountered a singularity:\n\n{error_msg}"))

    def check_thread(self, thread):
        """Monitors the synthesis thread"""
        if thread.is_alive():
            self.root.after(100, self.check_thread, thread)
        else:
            self.run_button.config(text="🚀 BEGIN COSMIC SYNTHESIS", 
                                 state='normal', bg='#FF6B00')
            self.root.after(0, lambda: messagebox.showinfo(
                "Journey Complete", 
                "Your musical journey is complete!\n\n" +
                "Check the output folder for:\n" +
                "• Your synthesized songs (X, Y, Z, G)\n" +
                "• The Grand Atlas map\n" +
                "• Cosmic exploration logs"))

    class TextRedirector(object):
        """Redirects stdout/stderr to the text widget"""
        def __init__(self, widget, tag="stdout"):
            self.widget = widget
            self.tag = tag

        def write(self, str_):
            self.widget.config(state='normal')
            self.widget.insert(tk.END, str_, (self.tag,))
            self.widget.see(tk.END)
            self.widget.config(state='disabled')
            self.widget.update_idletasks()
        
        def flush(self):
            pass

def main():
    # Create necessary directories
    os.makedirs("input_songs", exist_ok=True)
    os.makedirs("bounded_equations", exist_ok=True) 
    os.makedirs("output", exist_ok=True)
    
    root = tk.Tk()
    app = App(root)
    root.mainloop()

if __name__ == "__main__":
    main()
</div>
</div>

<!-- MODULE: engine.py -->
<div class="module" id="engine">
<h3>engine.py - Core Backend Orchestrator</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# engine.py: Tri-Resonance Synthesis Engine - Core Logic
#
# Version: 12.0 (Definitive Build)  
# Date: October 8, 2025
#
# This is the central orchestrator for the Tri-Resonance Architecture.
# ==============================================================================

import os
import warnings
import numpy as np
from modules.invariants import Invariants
from modules.math_core import MathCore
from modules.harmony_functional import HarmonyFunctional
from modules.advanced_geometry import AdvancedGeometry
from modules.symbolic_engine import SymbolicEngine
from modules.musical_structures import Denotator
from modules.physics_model import PhysicsModel
from modules.atlas_manager import AtlasManager

# Import all analysis modules
from modules.analysis.prime_framework import PrimeTranslationFramework
from modules.analysis.sentiment import SecureSentimentAnalyzer
from modules.analysis.bounded_features import BoundedFeatureExtractor
from modules.analysis.base_metrics import BaseMetricsExtractor
from modules.analysis.gestural import GesturalAnalyzer
from modules.analysis.classical_harmony import ClassicalHarmonyAnalyzer

# Import all synthesis modules
from modules.synthesis.adaptive_calculus import AdaptiveCalculus
from modules.synthesis.generative_model import GenerativeModel
from modules.synthesis.procedural_textures import ProceduralTextureSynthesizer
from modules.synthesis.geometric_transforms import GeometricTransformer

# Import humanization and utils
from modules.humanization.filters import HumanizationFilters
from modules.humanization.psychoacoustics import PsychoacousticModel
from modules.utils.source_manager import SourceManager
from modules.utils.output_manager import OutputGenerator
from modules.utils.logger import ProcessLogger
from modules.manifold_explorer import ManifoldExplorer

warnings.filterwarnings('ignore')

def run_synthesis_process(input_dir, run_explorer=True, add_to_atlas=True):
    """
    Main orchestration function for the Tri-Resonance Architecture.
    """
    # --- 0. Setup and Initialization ---
    logger = ProcessLogger()
    logger.start_section("COSMIC INITIALIZATION - Tri-Resonance Engine v12.0")
    
    base_dir = os.getcwd()
    bounded_csv_dir = os.path.join(base_dir, "bounded_equations")
    
    output_manager = OutputGenerator(base_dir, logger)
    output_dir = output_manager.output_dir
    
    logger.log(f"🎯 Output Directory: {output_dir}")
    
    # Initialize Grand Atlas
    atlas_manager = AtlasManager(logger, base_dir)
    if add_to_atlas:
        atlas_manager.initialize_primordial_sounds()
    
    # --- 1. Instantiate All Engine Modules ---
    logger.start_section("ASSEMBLING COSMIC COMPONENTS")
    
    invariants = Invariants()
    psychoacoustics = PsychoacousticModel()
    physics_model = PhysicsModel(logger, invariants)
    
    # Mathematical Core
    adv_geometry = AdvancedGeometry(logger, invariants)
    classical_analyzer = ClassicalHarmonyAnalyzer(logger, invariants)
    harmony_engine = HarmonyFunctional(logger, invariants, adv_geometry, classical_analyzer, physics_model)
    math_engine = MathCore(logger, invariants, harmony_engine)
    
    # Analysis Suite
    symbolic_engine = SymbolicEngine(logger, invariants, bounded_csv_dir)
    base_metrics_extractor = BaseMetricsExtractor(logger, invariants)
    bounded_extractor = BoundedFeatureExtractor(logger, invariants, base_metrics_extractor, symbolic_engine)
    gestural_analyzer = GesturalAnalyzer(logger, invariants)
    audio_analyzer = PrimeTranslationFramework(logger, invariants, bounded_extractor, gestural_analyzer)
    sentiment_analyzer = SecureSentimentAnalyzer(logger, invariants)
    
    # Synthesis Suite
    geo_transformer = GeometricTransformer(logger, adv_geometry)
    texture_synthesizer = ProceduralTextureSynthesizer(logger, invariants, psychoacoustics)
    generator = GenerativeModel(logger, invariants, texture_synthesizer, physics_model)
    calculator = AdaptiveCalculus(logger, harmony_engine)
    
    # Humanization Suite
    humanizer = HumanizationFilters(logger, invariants, psychoacoustics)
    
    # Exploration
    explorer = ManifoldExplorer(logger, invariants, generator, output_manager, atlas_manager)
    
    # I/O Management
    source_manager = SourceManager(logger)
    
    logger.end_section()

    # --- 2. Load and Analyze Input Sources ---
    logger.start_section("ANALYZING INPUT SONGS")
    
    prepared_sources = source_manager.process_input_directory(input_dir)
    if not prepared_sources:
        logger.log("💥 CRITICAL: No valid sources found. Aborting cosmic journey.")
        logger.write_master_log()
        return

    output_manager.copy_sources_to_output(prepared_sources)
    
    # Create initial Denotator objects from sources
    source_denotators = []
    for source in prepared_sources:
        denotator = Denotator(source['name'], logger, invariants, harmony_engine)
        
        # Analyze and populate the Denotator
        music_fp = audio_analyzer.analyze_signal(source['music_y'], source['sr'])
        sentiment_fp = sentiment_analyzer.analyze(source['lyrics'])
        denotator.add_fingerprint('music', music_fp)
        denotator.add_fingerprint('sentiment', sentiment_fp)
        
        source_denotators.append(denotator)
        
        # Add to Grand Atlas
        if add_to_atlas:
            atlas_manager.add_denotator(denotator, 'input_source')
    
    logger.log(f"📊 Analyzed {len(source_denotators)} input sources")
    logger.end_section()

    # --- 3. Tri-Concurrent Synthesis ---
    logger.start_section("TRI-CONCURRENT COSMIC SYNTHESIS")

    # Process C (Platonic Unity) -> Denotator Y
    logger.start_section("PROCESS C: Platonic Unification → Song Y")
    platonic_inputs = [d.get_unified_fingerprint() for d in source_denotators]
    s_Y_vector, s_Y_frames = calculator.run_synthesis(platonic_inputs, "unified")
    denotator_Y = Denotator("Song_Y", logger, invariants, harmony_engine, initial_vector=s_Y_vector)
    if add_to_atlas:
        atlas_manager.add_denotator(denotator_Y, 'synthesis_platonic')
    logger.end_section()

    # Process A & B (Biomimetic) -> Denotator X
    logger.start_section("PROCESS A&B: Biomimetic Synthesis → Song X")
    music_inputs = [d.get_fingerprint('music_avg') for d in source_denotators]
    sentiment_inputs = [d.get_fingerprint('sentiment') for d in source_denotators]
    
    s_music_final, s_X_music_frames = calculator.run_synthesis(music_inputs, "music")
    s_sentiment_final, s_X_sentiment_frames = calculator.run_synthesis(sentiment_inputs, "sentiment")
    
    s_X_vector = math_engine.meta_synthesis(s_music_final, s_sentiment_final)
    denotator_X = Denotator("Song_X", logger, invariants, harmony_engine, initial_vector=s_X_vector)
    s_X_frames = s_X_music_frames + s_X_sentiment_frames
    if add_to_atlas:
        atlas_manager.add_denotator(denotator_X, 'synthesis_biomimetic')
    logger.end_section()
    
    # Tier 2 Synthesis -> Denotator Z
    logger.start_section("TIER 2: Recursive Synthesis → Song Z")
    s_Z_vector, s_Z_frames = calculator.run_synthesis([denotator_X.vector, denotator_Y.vector], "recursive")
    denotator_Z = Denotator("Song_Z", logger, invariants, harmony_engine, initial_vector=s_Z_vector)
    if add_to_atlas:
        atlas_manager.add_denotator(denotator_Z, 'synthesis_recursive')
    logger.end_section()
    
    # Concordance Gap -> Denotator G
    logger.start_section("CONCORDANCE GAP: Cosmic Tension → Song G")
    concordance_gap_vector = adv_geometry.calculate_concordance_gap(denotator_X.vector, denotator_Y.vector)
    s_G_vector = math_engine.sonify_gap_field(concordance_gap_vector)
    denotator_G = Denotator("Song_G", logger, invariants, harmony_engine, initial_vector=s_G_vector)
    s_G_frames = [s_G_vector * (i/20.0) for i in range(21)]
    if add_to_atlas:
        atlas_manager.add_denotator(denotator_G, 'synthesis_gap')
    logger.end_section()
    logger.end_section()

    # --- 4. Manifold Exploration ---
    if run_explorer:
        logger.start_section("MAPPING THE COSMOS: Manifold Exploration")
        
        # Explore around each synthesized song
        for denotator in [denotator_X, denotator_Y, denotator_Z, denotator_G]:
            explorer.explore(denotator, num_canaries=50)
            
        logger.end_section()

    # --- 5. Final Generation and Humanization ---
    logger.start_section("COSMIC RENDERING: Audio Generation")
    
    songs_to_generate = {
        'Song_X': {'denotator': denotator_X, 'frames': s_X_frames},
        'Song_Y': {'denotator': denotator_Y, 'frames': s_Y_frames},
        'Song_Z': {'denotator': denotator_Z, 'frames': s_Z_frames},
        'Song_G': {'denotator': denotator_G, 'frames': s_G_frames}
    }

    generated_audio_data = {}
    for name, data in songs_to_generate.items():
        logger.start_section(f"RENDERING: {name}")
        audio_raw, sr = generator.render(data['denotator'])
        audio_humanized = humanizer.apply_all_filters(audio_raw, sr)
        generated_audio_data[name] = {'y': audio_humanized, 'sr': sr}
        
        # Generate all outputs
        output_manager.write_audio(name, audio_humanized, sr)
        output_manager.generate_geometry_visualization(name, data['denotator'].vector)
        output_manager.generate_evolution_gif(name, data['frames'])
        output_manager.write_process_log(name, data['denotator'], math_engine)
        logger.end_section()
    logger.end_section()

    # --- 6. Post-Synthesis Analysis ---
    logger.start_section("POST-SYNTHESIS COSMIC ANALYSIS")
    for name, audio_data in generated_audio_data.items():
        post_name = f"{name}_PostAnalysis"
        logger.start_section(f"ANALYZING: {name}")
        post_denotator = Denotator(post_name, logger, invariants, harmony_engine)
        post_fp = audio_analyzer.analyze_signal(audio_data['y'], audio_data['sr'])
        post_denotator.add_fingerprint('music', post_fp)

        output_manager.generate_geometry_visualization(post_name, post_denotator.get_unified_fingerprint())
        output_manager.write_process_log(post_name, post_denotator, math_engine)
        
        if add_to_atlas:
            atlas_manager.add_denotator(post_denotator, 'post_analysis')
        logger.end_section()
    logger.end_section()

    # --- 7. Finalize Grand Atlas ---
    if add_to_atlas:
        logger.start_section("FINALIZING GRAND ATLAS")
        atlas_path = atlas_manager.generate_atlas_visualization()
        logger.log(f"🗺️  Grand Atlas generated: {atlas_path}")
        logger.end_section()

    # --- 8. Finalization ---
    logger.start_section("COSMIC JOURNEY COMPLETE")
    output_manager.generate_readme([s.name for s in source_denotators])
    
    # Generate creation story
    story = generate_creation_story(source_denotators, [denotator_X, denotator_Y, denotator_Z, denotator_G])
    output_manager.write_creation_story(story)
    
    logger.log("🌌 Cosmic synthesis journey complete!")
    logger.end_section()
    logger.write_master_log()

    print(f"\n✨ MISSION ACCOMPLISHED! ✨")
    print(f"Check '{output_dir}' for your cosmic creations:")
    print(f"  • 4 Synthesized Songs (X, Y, Z, G)")
    print(f"  • Grand Atlas Map")
    print(f"  • Cosmic Exploration Logs") 
    print(f"  • Creation Story")
    print(f"  • 21-Part Output Suite")

def generate_creation_story(input_denotators, output_denotators):
    """Generates the story of this musical creation"""
    input_names = [d.name for d in input_denotators]
    story = {
        'title': 'A Cosmic Musical Journey',
        'inputs': input_names,
        'timestamp': np.datetime64('now'),
        'story': f"""
In the beginning, there were {len(input_names)} seeds of sound: {', '.join(input_names)}.
These vibrations traveled through the musical cosmos, guided by the ancient laws of harmony.

From their interaction emerged four new celestial bodies:
• Song X - The Biomimetic Echo, born from earthly patterns
• Song Y - The Platonic Ideal, pure mathematical form  
• Song Z - The Recursive Synthesis, where X and Y dance together
• Song G - The Concordance Gap, the sound of creative tension itself

This is their story, forever mapped in the Grand Atlas of musical space-time.
"""
    }
    return story
</div>
</div>

<!-- MODULE: invariants.py -->
<div class="module" id="invariants">
<h3>invariants.py - Foundational Constants</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# invariants.py: Tri-Resonance Synthesis Engine - Foundational Invariants
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# This module serves as the immutable constitution for the entire engine.
# ==============================================================================

import numpy as np

class Invariants:
    """
    Holds all foundational constants of the system, ensuring mathematical consistency.
    These are the non-negotiable truths of our musical universe.
    """
    def __init__(self):
        # --- 1. CORE FRAMEWORK INVARIANTS ---
        # The calculated, non-relative value for true zero.
        # SOURCE: UNIVERSAL_SYNTHESIS_CERTIFICATE_FINAL.txt
        self.IANNOTTI_INVARIANT_ZERO = 3.291007503378785e-24

        # The minimum harmony score for legitimate composition
        self.HARMONY_FLOOR = 0.3500
        
        # Prime archetypes for musical analysis
        self.PRIME_ARCHETYPE_COUNT = 13
        self.PRIME_TRANSLATION_WIDTHS = [5, 11, 23, 47, 97]

        # --- 2. FUNDAMENTAL MATHEMATICAL CONSTANTS ---
        self.PI = np.pi
        self.E = np.e
        self.PHI = (1 + np.sqrt(5)) / 2  # Golden Ratio
        self.SQRT2 = np.sqrt(2)

        # --- 3. CHAOS, STABILITY & ADVANCED MATHEMATICAL INVARIANTS ---
        self.FEIGENBAUM_CONSTANT = 4.66920160910299067185320382
        self.FEIGENBAUM_INVERSE = 1 / self.FEIGENBAUM_CONSTANT
        self.APERY_CONSTANT = 1.2020569031595942853997381615114499907

        # --- 4. ADVANCED MUSICAL & GEOMETRIC INVARIANTS ---
        self.PYTHAGOREAN_COMMA = (3/2)**12 / 2**7
        self.SCHUMANN_RESONANCE = 7.83  # Earth's fundamental frequency
        
        # Euler's Identity as a structural template
        self.EULERS_IDENTITY_COMPONENTS = {
            'rotational_element': np.exp(1j * np.pi),
            'additive_identity': 1.0,
            'target_equilibrium': self.IANNOTTI_INVARIANT_ZERO
        }
        
        # Riemann Zeta function zeros - the 'music' of the primes
        self.RIEMANN_ZETA_ZEROS = np.array([
            14.134725, 21.022040, 25.010858, 30.424876, 32.935062,
            37.586178, 40.918719, 43.327073, 48.005151, 49.773832
        ])

        # --- 5. COSMIC FREQUENCIES ---
        # Primordial sounds that seed the Grand Atlas
        self.COSMIC_FREQUENCIES = {
            'pulsar_base': 0.763,  # Fastest known pulsar period in milliseconds
            'schumann_fundamental': 7.83,
            'human_voice_range': (85, 255),  # Typical male/female fundamental
            'earth_rotation': 1/86400,  # 1 Hz per day
        }

        # --- 6. EPSILON LADDER (From Compendium v2) ---
        self.EPSILON_LADDER = {
            'waste_floor': 1e-4,
            'output_floor': 1e-3, 
            'risk_floor': 1e-5,
            'fragility_floor': 1e-3,
        }

        # --- 7. SYNTHESIS PARAMETERS ---
        self.DEFAULT_SYNTHESIS_STEPS = 100
        self.MAX_VECTOR_NORM = 50.0
        self.DEFAULT_AUDIO_DURATION = 45  # seconds

        # --- 8. AUDIO PROCESSING CONSTANTS ---
        self.DEFAULT_SAMPLE_RATE = 44100
        self.FFT_WINDOW_SIZE = 2048
        self.HOP_LENGTH = 512

        # --- 9. VISUALIZATION PARAMETERS ---
        self.MIN_GRID_SCALE = 8
        self.MAX_GRID_SCALE = 256
        self.GIF_FRAME_DURATION = 0.1  # seconds

        # --- 10. HARMONY FUNCTIONAL WEIGHTS ---
        self.HARMONY_WEIGHTS = {
            'similarity': 0.3,
            'novelty': 0.25, 
            'coherence': 0.2,
            'complexity': 0.15,
            'stability': 0.1
        }
</div>
</div>

<!-- MODULE: musical_structures.py -->
<div class="module" id="musical_structures">
<h3>musical_structures.py - Denotator Class</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# musical_structures.py: Tri-Resonance Synthesis Engine - Denotator Class
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# Defines the Denotator class, a formal mathematical object for representing
# musical entities, inspired by "Cool Math for Hot Music".
# ==============================================================================

import numpy as np
from invariants import Invariants
from harmony_functional import HarmonyFunctional
import uuid
from datetime import datetime

class Denotator:
    """
    A self-aware musical object that can store musical information and evaluate
    its own properties according to the core framework.
    
    Each Denotator represents a complete musical state in the Grand Atlas.
    """
    def __init__(self, name: str, logger, invariants: Invariants, 
                 harmony_engine: HarmonyFunctional, initial_vector: np.ndarray = None):
        self.name = name
        self.logger = logger
        self.invariants = invariants
        self.harmony_engine = harmony_engine
        
        # Unique identifier for Grand Atlas tracking
        self.id = str(uuid.uuid4())
        self.creation_time = datetime.now()
        
        # The core state of the musical object
        self.vector = initial_vector if initial_vector is not None else np.full(10, self.invariants.IANNOTTI_INVARIANT_ZERO)
        
        # Storage for different types of fingerprints and metadata
        self.fingerprints = {
            'music': np.array([]),
            'sentiment': np.array([]),
            'gestural': np.array([]),
            'classical': np.array([]),
            'bounded': np.array([])
        }
        
        self.metadata = {
            'type': 'unknown',
            'source_files': [],
            'creation_method': 'manual',
            'harmony_score': 0.0,
            'cosmic_coordinates': None,  # For Grand Atlas positioning
            'listening_count': 0
        }
        
        # Gestural properties cache
        self._gestural_properties = None
        
        self.logger.log(f"🎵 Denotator '{self.name}' created with ID: {self.id[:8]}...")

    def add_fingerprint(self, fp_type: str, fp_data: np.ndarray):
        """Adds a new fingerprint to the object."""
        if fp_data.size > 0:
            self.fingerprints[fp_type] = fp_data
            self.logger.log(f"  ↳ Added '{fp_type}' fingerprint of shape {fp_data.shape} to '{self.name}'.")
            self._update_vector()

    def add_metadata(self, key: str, value):
        """Adds metadata to the Denotator."""
        self.metadata[key] = value

    def _update_vector(self):
        """Updates the main state vector based on available fingerprints."""
        # Get all available fingerprints
        music_avg = self.get_fingerprint('music_avg')
        sentiment = self.get_fingerprint('sentiment')
        gestural = self.get_fingerprint('gestural')
        classical = self.get_fingerprint('classical')
        bounded = self.get_fingerprint('bounded')
        
        # Concatenate all available fingerprints
        components = []
        if music_avg.size > 0:
            components.append(music_avg)
        if sentiment.size > 0:
            components.append(sentiment)
        if gestural.size > 0:
            components.append(gestural)
        if classical.size > 0:
            components.append(classical)
        if bounded.size > 0:
            components.append(bounded)
            
        if components:
            self.vector = np.concatenate(components)
        else:
            self.vector = np.full(10, self.invariants.IANNOTTI_INVARIANT_ZERO)

    def get_fingerprint(self, fp_type: str) -> np.ndarray:
        """Retrieves a specific fingerprint, with logic for averages."""
        if fp_type == 'music_avg':
            music_fp = self.fingerprints.get('music')
            if music_fp is not None and music_fp.ndim > 1 and music_fp.size > 0:
                return np.mean(music_fp, axis=0)
            return music_fp if music_fp is not None else np.array([])
        return self.fingerprints.get(fp_type, np.array([]))

    def get_unified_fingerprint(self) -> np.ndarray:
        """Returns the combined fingerprint from all available data."""
        music_fp = self.get_fingerprint('music_avg')
        sentiment_fp = self.get_fingerprint('sentiment')
        gestural_fp = self.get_fingerprint('gestural')
        classical_fp = self.get_fingerprint('classical')
        bounded_fp = self.get_fingerprint('bounded')
        
        components = []
        for fp in [music_fp, sentiment_fp, gestural_fp, classical_fp, bounded_fp]:
            if fp.size > 0:
                components.append(fp)
                
        if not components:
            return np.array([])
            
        return np.concatenate(components)

    def get_gestural_properties(self) -> dict:
        """Extracts gestural properties from the fingerprints."""
        if self._gestural_properties is not None:
            return self._gestural_properties
            
        gestural_fp = self.get_fingerprint('gestural')
        properties = {
            'tempo_stability': 0.5,
            'rhythmic_complexity': 0.5,
            'melodic_contour': 0.5,
            'dynamic_range': 0.5
        }
        
        if gestural_fp.size >= 4:
            properties.update({
                'tempo_stability': float(gestural_fp[0]) if gestural_fp[0] > 0 else 0.5,
                'rhythmic_complexity': float(gestural_fp[1]) if gestural_fp[1] > 0 else 0.5,
                'melodic_contour': float(gestural_fp[2]) if gestural_fp[2] > 0 else 0.5,
                'dynamic_range': float(gestural_fp[3]) if gestural_fp[3] > 0 else 0.5
            })
            
        self._gestural_properties = properties
        return properties

    def calculate_harmony(self) -> float:
        """Evaluates the object's own Harmony score."""
        score = self.harmony_engine.calculate(self.vector, [self.vector])
        self.metadata['harmony_score'] = score
        return score

    def get_cosmic_signature(self) -> dict:
        """Returns a unique signature for Grand Atlas positioning."""
        if self.vector.size == 0:
            return {'x': 0, 'y': 0, 'z': 0, 'mass': 0}
            
        # Use first 3 dimensions for spatial coordinates
        coords = {'x': 0, 'y': 0, 'z': 0}
        for i, coord in enumerate(['x', 'y', 'z']):
            if i < len(self.vector):
                coords[coord] = float(self.vector[i])
                
        # Use vector norm for mass/importance
        coords['mass'] = float(np.linalg.norm(self.vector))
        
        self.metadata['cosmic_coordinates'] = coords
        return coords

    def to_dict(self) -> dict:
        """Serializes the Denotator's state to a dictionary for logging/JSON."""
        return {
            'id': self.id,
            'name': self.name,
            'creation_time': self.creation_time.isoformat(),
            'vector': self.vector.tolist(),
            'vector_shape': list(self.vector.shape),
            'vector_norm': float(np.linalg.norm(self.vector)),
            'fingerprints': {k: v.tolist() for k, v in self.fingerprints.items() if v.size > 0},
            'metadata': self.metadata,
            'cosmic_signature': self.get_cosmic_signature(),
            'current_harmony_score': self.calculate_harmony()
        }

    def from_dict(self, data: dict):
        """Deserializes a Denotator from a dictionary."""
        self.id = data.get('id', self.id)
        self.name = data.get('name', self.name)
        self.vector = np.array(data.get('vector', []))
        
        # Restore fingerprints
        fp_data = data.get('fingerprints', {})
        for fp_type, fp_list in fp_data.items():
            self.fingerprints[fp_type] = np.array(fp_list)
            
        self.metadata.update(data.get('metadata', {}))
        
        return self

    def __str__(self):
        return f"Denotator('{self.name}', harmony={self.calculate_harmony():.3f}, dim={len(self.vector)})"

    def __repr__(self):
        return self.__str__()
</div>
</div>

<!-- MODULE: physics_model.py -->
<div class="module" id="physics_model">
<h3>physics_model.py - Fundamental Sound Physics</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# physics_model.py: Tri-Resonance Synthesis Engine - Physics Model
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# Models the fundamental physics of sound: wave mechanics, the harmonic series,
# and vibrational properties that form the basis of our musical universe.
# ==============================================================================

import numpy as np
from scipy.special import jv, yv  # Bessel functions
from invariants import Invariants

class PhysicsModel:
    """
    Implements the physical models of sound and vibration that underlie
    all musical phenomena, from Rameau's corps sonore to cosmic vibrations.
    """
    def __init__(self, logger, invariants: Invariants):
        self.logger = logger
        self.invariants = invariants
        self.logger.log("PhysicsModel initialized - modeling the physics of sound.")
        
        # Precompute fundamental physical constants
        self.SPEED_OF_SOUND = 343.0  # m/s at 20°C
        self.AIR_DENSITY = 1.225  # kg/m³
        self.REFERENCE_PRESSURE = 2e-5  # Pa (threshold of hearing)

    def vibrating_string_frequencies(self, fundamental_hz: float, num_partials: int = 16) -> np.ndarray:
        """
        Returns the harmonic series for an ideal vibrating string.
        This is Rameau's corps sonore - the physical basis of harmony.
        """
        frequencies = fundamental_hz * np.arange(1, num_partials + 1)
        self.logger.log(f"  🎻 Generated harmonic series from {fundamental_hz:.1f}Hz: {num_partials} partials")
        return frequencies

    def calculate_standing_wave(self, frequency: float, length: float, 
                              tension: float, density: float) -> tuple:
        """
        Calculates standing wave properties for a vibrating string.
        Returns (wavelength, wave_number, phase_velocity).
        """
        # Phase velocity: v = sqrt(T/μ) where T is tension, μ is linear density
        linear_density = density / length  # Approximate
        phase_velocity = np.sqrt(tension / linear_density)
        
        wavelength = phase_velocity / frequency
        wave_number = 2 * np.pi / wavelength
        
        return wavelength, wave_number, phase_velocity

    def schumann_resonance_modes(self, earth_circumference: float = 40075000.0) -> np.ndarray:
        """
        Calculates Schumann resonance frequencies - Earth's natural electromagnetic
        resonances in the cavity between Earth's surface and the ionosphere.
        """
        # Schumann resonances: f_n = (c / (2πR)) * sqrt(n(n+1))
        # where c is speed of light, R is Earth's radius
        speed_of_light = 299792458.0  # m/s
        earth_radius = earth_circumference / (2 * np.pi)
        
        n_modes = 8
        frequencies = []
        
        for n in range(1, n_modes + 1):
            freq = (speed_of_light / (2 * np.pi * earth_radius)) * np.sqrt(n * (n + 1))
            frequencies.append(freq)
            
        frequencies = np.array(frequencies)
        self.logger.log(f"  🌍 Generated Schumann resonances: {frequencies[0]:.2f}Hz fundamental")
        return frequencies

    def pulsar_period_to_frequency(self, period_seconds: float) -> float:
        """
        Converts pulsar rotation period to audible frequency.
        Many pulsars have periods that translate to bass frequencies.
        """
        frequency = 1.0 / period_seconds
        self.logger.log(f"  ⭐ Pulsar period {period_seconds:.6f}s → {frequency:.2f}Hz")
        return frequency

    def human_vocal_ranges(self) -> dict:
        """
        Returns typical human vocal frequency ranges by voice type.
        """
        return {
            'bass': (82, 330),
            'baritone': (110, 392),
            'tenor': (130, 494),
            'alto': (175, 699),
            'soprano': (247, 1174),
            'whisper': (100, 1000)  # Broad range for whispered speech
        }

    def calculate_acoustic_energy(self, pressure_pa: float, duration: float, 
                                area: float = 1.0) -> float:
        """
        Calculates acoustic energy from pressure, duration, and area.
        E = (p² / (ρc)) * A * t
        where p is pressure, ρ is density, c is speed of sound, A is area, t is time.
        """
        intensity = (pressure_pa ** 2) / (self.AIR_DENSITY * self.SPEED_OF_SOUND)
        energy = intensity * area * duration
        return energy

    def spherical_wave_attenuation(self, distance: float, frequency: float,
                                 initial_pressure: float) -> float:
        """
        Calculates pressure attenuation for a spherical wave.
        Pressure decreases with 1/r for spherical waves.
        """
        if distance < 0.1:  # Avoid division by very small numbers
            return initial_pressure
            
        attenuated_pressure = initial_pressure / distance
        return attenuated_pressure

    def bessel_harmonics(self, order: int, x_values: np.ndarray) -> np.ndarray:
        """
        Calculates Bessel function values for circular membrane vibrations.
        Important for modeling drum-like sounds and complex resonators.
        """
        return jv(order, x_values)

    def create_cosmic_vibration_profile(self) -> dict:
        """
        Creates a profile of cosmic-scale vibrations that seed the Grand Atlas.
        Includes pulsars, Schumann resonances, and fundamental physical vibrations.
        """
        profile = {}
        
        # Pulsar vibrations (converted to audible range)
        pulsar_periods = [0.0014, 0.763, 1.39]  # Fast, medium, slow pulsars in seconds
        profile['pulsars'] = [self.pulsar_period_to_frequency(p) for p in pulsar_periods]
        
        # Earth vibrations
        profile['schumann'] = self.schumann_resonance_modes()
        
        # Human voice fundamentals
        vocal_ranges = self.human_vocal_ranges()
        profile['voice_fundamentals'] = {
            voice: (low + high) / 2 for voice, (low, high) in vocal_ranges.items()
        }
        
        # Ideal string harmonics (the corps sonore)
        profile['harmonic_series'] = self.vibrating_string_frequencies(55.0, 32)  # A1 fundamental
        
        self.logger.log("  🌌 Created cosmic vibration profile with pulsars, Earth, and human frequencies")
        return profile

    def calculate_resonance_quality(self, frequency: float, bandwidth: float) -> float:
        """
        Calculates quality factor Q for a resonance.
        Q = f / Δf where Δf is bandwidth at -3dB points.
        """
        if bandwidth < self.invariants.IANNOTTI_INVARIANT_ZERO:
            return 1000.0  # Very high Q for very narrow bandwidth
            
        return frequency / bandwidth

    def thermal_noise_floor(self, temperature_kelvin: float = 293.15, 
                          bandwidth_hz: float = 1.0) -> float:
        """
        Calculates thermal noise floor using Johnson-Nyquist formula.
        P = kTΔf where k is Boltzmann's constant, T is temperature, Δf is bandwidth.
        """
        boltzmann_constant = 1.380649e-23  # J/K
        power = boltzmann_constant * temperature_kelvin * bandwidth_hz
        
        # Convert to sound pressure level (approximate)
        # This is a rough conversion for illustrative purposes
        pressure = np.sqrt(power * self.AIR_DENSITY * self.SPEED_OF_SOUND)
        
        return pressure
</div>
</div>

<!-- MODULE: math_core.py -->
<div class="module" id="math_core">
<h3>math_core.py - Core Mathematical Functions</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# math_core.py: Tri-Resonance Synthesis Engine - Core Mathematical Functions
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# This module houses foundational mathematical functions from the Compendium.
# It handles the UOE, PARS, and basic synthesis logic.
# ==============================================================================

import numpy as np
import hashlib
from invariants import Invariants
from harmony_functional import HarmonyFunctional

class MathCore:
    """
    Implements foundational mathematical logic of the core framework.
    All conceptual zeros have been recalibrated to the Iannotti Invariant.
    """
    def __init__(self, logger, invariants: Invariants, harmony_engine: HarmonyFunctional):
        self.logger = logger
        self.invariants = invariants
        self.harmony_engine = harmony_engine
        self.logger.log("MathCore initialized and calibrated to cosmic constants.")

    def hash_vector(self, vector: np.ndarray) -> str:
        """Creates a deterministic SHA256 hash of a numpy vector."""
        if vector is None or vector.size == 0:
            return hashlib.sha256(b'empty').hexdigest()
        return hashlib.sha256(vector.astype(np.float64).tobytes()).hexdigest()

    def calculate_uoe(self, I: float, P: float, W: float, U: float) -> float:
        """
        Calculates the Universal Optimization Equation (V-Unit).
        V = ((I * P) - W) / U
        """
        I = max(I, self.invariants.IANNOTTI_INVARIANT_ZERO)
        P = np.clip(P, self.invariants.IANNOTTI_INVARIANT_ZERO, 1.0)
        W_floor = W + self.invariants.EPSILON_LADDER['waste_floor']
        U = max(U, self.invariants.EPSILON_LADDER['output_floor'])
        
        value = ((I * P) - W_floor) / U
        return float(value)

    def calculate_pars(self, Hz: float, Ex: float, Vu: float, Mt: float) -> float:
        """
        Calculates the Probabilistic Adversarial Risk Score (PARS).
        Risk = Hz * Ex * Vu * (1 - Mt)
        """
        Mt = np.clip(Mt, self.invariants.IANNOTTI_INVARIANT_ZERO, 1.0)
        risk = Hz * Ex * Vu * (1.0 - Mt)
        return max(risk, self.invariants.EPSILON_LADDER['risk_floor'])

    def meta_synthesis(self, music_vector: np.ndarray, sentiment_vector: np.ndarray) -> np.ndarray:
        """
        Performs Meta-Synthesis for Song X by structured interweaving.
        Creates a new vector that alternates between music and sentiment dimensions.
        """
        self.logger.log("🌀 Performing meta-synthesis of music and sentiment vectors.")
        
        music_vec = music_vector if music_vector.size > 0 else np.full(5, self.invariants.IANNOTTI_INVARIANT_ZERO)
        sentiment_vec = sentiment_vector if sentiment_vector.size > 0 else np.full(5, self.invariants.IANNOTTI_INVARIANT_ZERO)

        combined_length = len(music_vec) + len(sentiment_vec)
        final_vector = np.full(combined_length, self.invariants.IANNOTTI_INVARIANT_ZERO)

        len_music, len_sentiment = len(music_vec), len(sentiment_vec)
        min_len = min(len_music, len_sentiment)

        # Interweave music and sentiment dimensions
        for i in range(min_len):
            final_vector[2*i] = music_vec[i]
            final_vector[2*i + 1] = sentiment_vec[i]
        
        # Fill remaining dimensions with leftover values
        if len_music > len_sentiment:
            final_vector[2*min_len:] = music_vec[min_len:]
        else:
            final_vector[2*min_len:] = sentiment_vec[min_len:]
        
        self.logger.log(f"  ↳ Meta-synthesis created {len(final_vector)}D vector from {len_music}D music + {len_sentiment}D sentiment")
        return final_vector

    def sonify_gap_field(self, gap_vector: np.ndarray) -> np.ndarray:
        """
        Translates the Concordance Gap vector into a sonifiable state vector.
        The gap represents the tension between different musical states.
        """
        self.logger.log("🎵 Sonifying concordance gap vector.")
        
        if gap_vector is None or gap_vector.size == 0:
            return np.full(10, self.invariants.IANNOTTI_INVARIANT_ZERO)
             
        magnitude = np.linalg.norm(gap_vector)
        if magnitude < self.invariants.IANNOTTI_INVARIANT_ZERO:
            return np.full(gap_vector.shape, self.invariants.IANNOTTI_INVARIANT_ZERO)

        normalized_gap = gap_vector / magnitude
        s_G = np.full(max(10, len(gap_vector)), self.invariants.IANNOTTI_INVARIANT_ZERO)
        
        # Map gap properties to musical parameters
        s_G[0] = np.clip(magnitude, 0, 10)  # OI -> Tension magnitude
        s_G[1] = np.std(normalized_gap)      # Rho -> Directional variance
        s_G[2] = np.mean(np.abs(normalized_gap))  # Gamma -> Average intensity
        
        # Use spectral content of the gap for timbral properties
        if len(s_G) > 3:
            gap_fft = np.fft.rfft(normalized_gap).real
            len_to_copy = min(len(s_G) - 3, len(gap_fft))
            s_G[3:3+len_to_copy] = np.abs(gap_fft[:len_to_copy])

        self.logger.log(f"  ↳ Sonified {len(gap_vector)}D gap → {len(s_G)}D musical vector")
        return s_G

    def calculate_cosmic_distance(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        Calculates the 'cosmic distance' between two vectors in musical space.
        Uses a weighted distance that accounts for different dimension types.
        """
        if vec1.size != vec2.size or vec1.size == 0:
            return 1.0  # Maximum distance for incompatible vectors
            
        # Use dynamic time warping distance for time-series like vectors
        if vec1.size > 10:  # Likely a time-series fingerprint
            distance = self._dtw_distance(vec1, vec2)
        else:
            # Use weighted Euclidean distance for feature vectors
            weights = np.linspace(1.0, 0.1, len(vec1))  # Higher weight to early dimensions
            distance = np.sqrt(np.sum(weights * (vec1 - vec2) ** 2))
            
        return float(distance)

    def _dtw_distance(self, series1: np.ndarray, series2: np.ndarray) -> float:
        """
        Dynamic Time Warping distance for time-series like vectors.
        More robust than Euclidean distance for shifted or stretched sequences.
        """
        n, m = len(series1), len(series2)
        dtw_matrix = np.zeros((n+1, m+1))
        
        # Initialize with infinity
        dtw_matrix[1:, 0] = float('inf')
        dtw_matrix[0, 1:] = float('inf')
        
        # Fill DTW matrix
        for i in range(1, n+1):
            for j in range(1, m+1):
                cost = abs(series1[i-1] - series2[j-1])
                dtw_matrix[i, j] = cost + min(dtw_matrix[i-1, j],    # insertion
                                            dtw_matrix[i, j-1],    # deletion
                                            dtw_matrix[i-1, j-1])  # match
                
        return dtw_matrix[n, m] / max(n, m)  # Normalize by length

    def normalize_to_cosmic_range(self, vector: np.ndarray) -> np.ndarray:
        """
        Normalizes a vector to the 'cosmic range' suitable for musical synthesis.
        Preserves relative relationships while constraining to reasonable bounds.
        """
        if vector.size == 0:
            return vector
            
        norm = np.linalg.norm(vector)
        if norm < self.invariants.IANNOTTI_INVARIANT_ZERO:
            return vector
            
        # Soft normalization that preserves some dynamic range
        target_norm = min(norm, self.invariants.MAX_VECTOR_NORM)
        normalized = (vector / norm) * target_norm
        
        return normalized

    def create_resonance_matrix(self, frequencies: np.ndarray) -> np.ndarray:
        """
        Creates a resonance coupling matrix between frequencies.
        Used to model how different musical elements interact.
        """
        n = len(frequencies)
        resonance_matrix = np.zeros((n, n))
        
        for i in range(n):
            for j in range(n):
                if i == j:
                    resonance_matrix[i, j] = 1.0  # Self-resonance
                else:
                    # Resonance strength decays with frequency ratio
                    ratio = frequencies[i] / frequencies[j]
                    if ratio > 1:
                        ratio = 1 / ratio
                    resonance_matrix[i, j] = np.exp(-10 * (1 - ratio) ** 2)
                    
        return resonance_matrix
</div>
</div>

<!-- MODULE: harmony_functional.py -->
<div class="module" id="harmony_functional">
<h3>harmony_functional.py - Harmony Functional</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# harmony_functional.py: Tri-Resonance Synthesis Engine - Harmony Functional
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# This dedicated module houses the full, complex Harmony Functional, including
# the new Sterility Tax and Rameau Consonance terms.
# ==============================================================================

import numpy as np
from invariants import Invariants
from advanced_geometry import AdvancedGeometry
from analysis_classical_harmony import ClassicalHarmonyAnalyzer
from physics_model import PhysicsModel

class HarmonyFunctional:
    """
    Implements the full Harmony Functional (H = V - Tax - Burden).
    This is the objective function that guides all synthesis towards musical beauty.
    """
    def __init__(self, logger, invariants: Invariants, adv_geometry: AdvancedGeometry, 
                 classical_analyzer: ClassicalHarmonyAnalyzer, physics_model: PhysicsModel):
        self.logger = logger
        self.invariants = invariants
        self.adv_geometry = adv_geometry
        self.classical_analyzer = classical_analyzer
        self.physics_model = physics_model
        self.logger.log("HarmonyFunctional initialized - guardian of musical beauty.")

    def calculate_sterility_tax(self, vector: np.ndarray) -> float:
        """
        Calculates a penalty for unnaturally perfect quantization.
        Music that is too mathematically perfect feels robotic and uncanny.
        """
        if len(vector) < 2:
            return 0.1  # Small tax for trivial vectors
            
        # Measure variability in the vector
        diff_variance = np.var(np.diff(vector))
        abs_variance = np.var(np.abs(vector))
        
        # Very low variance indicates robotic perfection
        sterility_penalty = np.exp(-diff_variance * 1000) * 0.3
        sterility_penalty += np.exp(-abs_variance * 100) * 0.2
        
        return min(sterility_penalty, 0.5)  # Cap at 50% tax

    def calculate_acoustic_authenticity(self, vector: np.ndarray) -> float:
        """
        Rewards vectors that correspond to physically plausible sounds.
        Based on harmonic series relationships and acoustic physics.
        """
        if vector.size < 3:
            return 0.5
            
        # Extract frequency-like components (first few dimensions)
        freq_components = vector[:min(8, len(vector))]
        
        # Check if components approximate harmonic ratios
        if len(freq_components) >= 3:
            ratios = []
            for i in range(1, len(freq_components)):
                if abs(freq_components[0]) > self.invariants.IANNOTTI_INVARIANT_ZERO:
                    ratio = abs(freq_components[i]) / abs(freq_components[0])
                    ratios.append(ratio)
            
            # Ideal harmonic ratios: 1, 2, 3, 4, 5...
            ideal_ratios = np.arange(1, len(ratios) + 1)
            ratio_errors = [min(abs(r - ideal), abs(r - 1/ideal)) for r, ideal in zip(ratios, ideal_ratios)]
            
            # Average error (lower is better)
            avg_error = np.mean(ratio_errors) if ratio_errors else 1.0
            authenticity = np.exp(-avg_error * 2)
        else:
            authenticity = 0.5
            
        return authenticity

    def calculate(self, state_vector: np.ndarray, source_vectors: list) -> float:
        """
        Calculates the full Harmony Functional for a given state vector.
        H = Benefit - Safety Tax - Burden
        """
        if state_vector is None or state_vector.size == 0:
            return -1e6

        # --- 1. BENEFIT (V) - Positive qualities we want to maximize ---
        
        # Structural integrity - well-formed vectors are better
        t1_integrity = np.exp(-0.05 * np.linalg.norm(state_vector)**2)
        
        # Coherence with source material (if any)
        t2_coherence = 1.0
        if source_vectors and any(s.size > 0 for s in source_vectors):
            valid_sources = [s for s in source_vectors if s.size > 0]
            barycenter = np.mean(valid_sources, axis=0)
            
            target_len = len(state_vector)
            if len(barycenter) != target_len:
                b_padded = np.full(target_len, self.invariants.IANNOTTI_INVARIANT_ZERO)
                len_to_copy = min(len(barycenter), target_len)
                b_padded[:len_to_copy] = barycenter[:len_to_copy]
                barycenter = b_padded
            
            distance_to_source = np.linalg.norm(state_vector - barycenter)
            t2_coherence = np.exp(-0.2 * distance_to_source**2)
        
        # Elegance through systolic geometry
        systolic_ratio = self.adv_geometry.calculate_systolic_geometry(state_vector)
        t3_elegance = np.exp(-15 * (systolic_ratio - 0.4)**2)
        
        # Resonance with fundamental mathematical constants
        resonance_score = 0.0
        if state_vector.size > 0:
            dominant_component = np.max(np.abs(state_vector)) * 10 
            zeta_distances = np.abs(dominant_component - self.invariants.RIEMANN_ZETA_ZEROS)
            resonance_score = np.exp(-np.min(zeta_distances)**2 / 5.0)

        # Rameau Consonance - classical harmonic principles
        rameau_consonance = self.classical_analyzer.calculate_fundamental_bass_consonance(state_vector)
        
        # Acoustic authenticity - physically plausible sounds
        acoustic_auth = self.calculate_acoustic_authenticity(state_vector)
        
        # Weighted combination of all benefit components
        V = (0.1 * t1_integrity + 
             0.25 * t2_coherence + 
             0.15 * t3_elegance + 
             0.1 * resonance_score + 
             0.25 * rameau_consonance +
             0.15 * acoustic_auth)

        # --- 2. SAFETY TAX - Penalties for risky or undesirable qualities ---
        
        risk = np.std(state_vector) * 0.2           # High variance can be chaotic
        gap = np.var(state_vector) * 0.1            # Very high variance is unstable
        fragility = (1.0 / (np.mean(np.abs(state_vector)) + 
                           self.invariants.EPSILON_LADDER['fragility_floor'])) * 0.1
        sterility_tax = self.calculate_sterility_tax(state_vector)
        
        safety_tax = risk + gap + fragility + sterility_tax

        # --- 3. BURDEN - Penalties for excessive complexity or repetition ---
        
        complexity = np.mean(np.abs(np.diff(state_vector))) if len(state_vector) > 1 else self.invariants.IANNOTTI_INVARIANT_ZERO
        complexity_burden = (complexity - 0.5)**2
        repetition_burden = (1.0 - systolic_ratio)**2
        burden = (0.3 * complexity_burden + 0.7 * repetition_burden) * 0.5

        # --- 4. FINAL HARMONY CALCULATION ---
        H = V - safety_tax - burden
        
        # Enforce harmony floor - compositions below this are not legitimate
        if H < self.invariants.HARMONY_FLOOR:
            penalty = (self.invariants.HARMONY_FLOOR - H)**2 * 10.0
            H -= penalty
            
        # Ensure result is finite
        if not np.isfinite(H):
            H = -1e6
            
        return float(H)

    def calculate_component_breakdown(self, state_vector: np.ndarray, source_vectors: list) -> dict:
        """
        Returns a detailed breakdown of all harmony components for analysis.
        """
        if state_vector is None or state_vector.size == 0:
            return {}
            
        breakdown = {}
        
        # Benefit components
        breakdown['integrity'] = np.exp(-0.05 * np.linalg.norm(state_vector)**2)
        
        # Coherence calculation
        if source_vectors and any(s.size > 0 for s in source_vectors):
            valid_sources = [s for s in source_vectors if s.size > 0]
            barycenter = np.mean(valid_sources, axis=0)
            distance_to_source = np.linalg.norm(state_vector - barycenter)
            breakdown['coherence'] = np.exp(-0.2 * distance_to_source**2)
        else:
            breakdown['coherence'] = 1.0
            
        breakdown['elegance'] = np.exp(-15 * (self.adv_geometry.calculate_systolic_geometry(state_vector) - 0.4)**2)
        breakdown['consonance'] = self.classical_analyzer.calculate_fundamental_bass_consonance(state_vector)
        breakdown['authenticity'] = self.calculate_acoustic_authenticity(state_vector)
        
        # Tax components
        breakdown['risk_tax'] = np.std(state_vector) * 0.2
        breakdown['gap_tax'] = np.var(state_vector) * 0.1
        breakdown['sterility_tax'] = self.calculate_sterility_tax(state_vector)
        
        # Burden components
        complexity = np.mean(np.abs(np.diff(state_vector))) if len(state_vector) > 1 else 0.0
        breakdown['complexity_burden'] = (complexity - 0.5)**2 * 0.3 * 0.5
        breakdown['repetition_burden'] = (1.0 - breakdown['elegance'])**2 * 0.7 * 0.5
        
        return breakdown
</div>
</div>

<!-- MODULE: advanced_geometry.py -->
<div class="module" id="advanced_geometry">
<h3>advanced_geometry.py - Advanced Geometry</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# advanced_geometry.py: Tri-Resonance Synthesis Engine - Advanced Geometry
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# This module houses the complex geometric and mathematical implementations,
# including Systolic Geometry, Homography, and the Concordance Gap.
# ==============================================================================

import numpy as np
from scipy.signal import find_peaks
from scipy.ndimage import gaussian_filter1d
from invariants import Invariants

class AdvancedGeometry:
    """
    Implements advanced geometric theories for analysis and synthesis.
    This is where we model the curved space of musical possibilities.
    """
    def __init__(self, logger, invariants: Invariants):
        self.logger = logger
        self.invariants = invariants
        self.logger.log("AdvancedGeometry engine initialized - mapping musical manifolds.")

    def calculate_systolic_geometry(self, timeseries: np.ndarray) -> float:
        """
        Finds the shortest, non-contractible loop (systole) in a signal.
        This measures the inherent rhythmic and structural periodicity.
        """
        if timeseries is None or timeseries.size < 20: 
            return 1.0
        
        norm_series = timeseries - np.mean(timeseries)
        var = np.var(norm_series)
        if var < self.invariants.IANNOTTI_INVARIANT_ZERO:
            return 1.0
            
        # Compute autocorrelation to find repeating patterns
        autocorr = np.correlate(norm_series, norm_series, mode='full')
        autocorr = autocorr[len(norm_series)-1:] / (var * len(norm_series))
        
        # Find peaks in autocorrelation (potential periodicities)
        peaks, _ = find_peaks(autocorr, height=0.05, distance=3)
        
        # The first non-zero peak is the fundamental period (systole)
        systole_length = peaks[0] if len(peaks) > 0 else len(timeseries)
        systolic_ratio = systole_length / len(timeseries)
        
        return np.clip(systolic_ratio, self.invariants.IANNOTTI_INVARIANT_ZERO, 1.0)

    def apply_homography_transformation(self, vector: np.ndarray, music_properties: dict) -> np.ndarray:
        """
        Applies a content-aware Projective Transformation (Homography) to a state vector.
        This creates sophisticated variations by changing the 'perspective' on the music.
        """
        if vector is None or vector.size == 0:
            return np.array([])
        dim = len(vector)
            
        # Start with identity transformation
        H = np.identity(dim + 1)
        
        # Transformation is now content-aware, using properties of the music itself
        tempo_stability = music_properties.get('tempo_stability', 0.5)
        rhythmic_complexity = music_properties.get('rhythmic_complexity', 0.5)
        melodic_contour = music_properties.get('melodic_contour', 0.5)
        
        # Calculate transformation strength based on musical properties
        perspective_strength = (np.clip(np.linalg.norm(vector), 0.1, 2.0) * 
                             0.02 * tempo_stability * (1 + melodic_contour))
        
        # Create perspective vector based on rhythmic complexity
        p = np.full(dim, self.invariants.FEIGENBAUM_INVERSE * rhythmic_complexity)
        p_mod = np.cos(vector * self.invariants.PI / 2.0)
        p = p * p_mod * perspective_strength
        
        # Apply perspective transformation
        H[dim, :dim] = p

        # Transform to homogeneous coordinates
        homogeneous_vector = np.append(vector, 1.0)
        transformed_homogeneous = H @ homogeneous_vector
        
        # Convert back from homogeneous coordinates
        w = transformed_homogeneous[-1]
        if abs(w) < self.invariants.IANNOTTI_INVARIANT_ZERO:
            return vector
            
        transformed_vector = transformed_homogeneous[:-1] / w
        
        # Preserve original scale to maintain energy
        original_norm = np.linalg.norm(vector)
        transformed_norm = np.linalg.norm(transformed_vector)
        if transformed_norm > self.invariants.IANNOTTI_INVARIANT_ZERO:
            transformed_vector *= (original_norm / transformed_norm)
        
        self.logger.log(f"  🌀 Applied homography transformation (strength: {perspective_strength:.3f})")
        return transformed_vector

    def calculate_concordance_gap(self, vector_x: np.ndarray, vector_y: np.ndarray) -> np.ndarray:
        """
        Calculates the Concordance Gap vector between two states.
        This measures the creative tension between different musical approaches.
        """
        self.logger.log("📐 Calculating Concordance Gap between musical states.")
        
        vec_x = vector_x if vector_x is not None else np.full(10, self.invariants.IANNOTTI_INVARIANT_ZERO)
        vec_y = vector_y if vector_y is not None else np.full(10, self.invariants.IANNOTTI_INVARIANT_ZERO)

        # Ensure vectors have same length for comparison
        len_x, len_y = len(vec_x), len(vec_y)
        if len_x > len_y:
            y_padded = np.pad(vec_y, (0, len_x - len_y), 'constant', 
                            constant_values=self.invariants.IANNOTTI_INVARIANT_ZERO)
            x_padded = vec_x
        elif len_y > len_x:
            x_padded = np.pad(vec_x, (0, len_y - len_x), 'constant',
                            constant_values=self.invariants.IANNOTTI_INVARIANT_ZERO)
            y_padded = vec_y
        else:
            x_padded, y_padded = vec_x, vec_y

        # The gap is simply the difference between states
        gap_vector = y_padded - x_padded
        
        gap_magnitude = np.linalg.norm(gap_vector)
        self.logger.log(f"  ↳ Concordance Gap magnitude: {gap_magnitude:.6f}")
        
        return gap_vector

    def calculate_manifold_curvature(self, points: np.ndarray) -> float:
        """
        Estimates the curvature of the musical manifold in a local region.
        Higher curvature means more complex, nonlinear relationships.
        """
        if points.shape[0] < 3 or points.shape[1] < 2:
            return 0.0
            
        # Use PCA to find local tangent space
        from sklearn.decomposition import PCA
        pca = PCA(n_components=2)
        try:
            transformed = pca.fit_transform(points)
            
            # Curvature is related to how well 2D projection explains variance
            curvature = 1.0 - pca.explained_variance_ratio_.sum()
            return float(curvature)
        except:
            return 0.5

    def create_geodesic_path(self, start_point: np.ndarray, end_point: np.ndarray, 
                           num_points: int = 10) -> np.ndarray:
        """
        Creates a geodesic (shortest path) between two points on the musical manifold.
        """
        if start_point.size != end_point.size:
            return np.array([])
            
        # Simple linear interpolation for now
        # In a real implementation, this would follow the manifold curvature
        path = []
        for i in range(num_points):
            t = i / (num_points - 1)
            point = (1 - t) * start_point + t * end_point
            path.append(point)
            
        return np.array(path)

    def apply_conformal_mapping(self, vector: np.ndarray, center: np.ndarray, 
                              scale: float = 1.0) -> np.ndarray:
        """
        Applies a conformal mapping (angle-preserving transformation) to a vector.
        Useful for creating variations that preserve local structure.
        """
        if vector.size == 0 or center.size != vector.size:
            return vector
            
        # Simple radial scaling as a basic conformal map
        displacement = vector - center
        mapped_displacement = displacement * scale
        
        return center + mapped_displacement
</div>
</div>

<!-- MODULE: symbolic_engine.py -->
<div class="module" id="symbolic_engine">
<h3>symbolic_engine.py - Symbolic Equation Engine</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# symbolic_engine.py: Tri-Resonance Synthesis Engine - Symbolic Engine
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# A new module to handle the Bounded Equations as a dynamic, symbolic system.
# It uses SymPy to combine and create new equations at runtime.
# ==============================================================================

import sympy
from sympy import symbols, Add, Mul, Pow, exp, log, sqrt, sin, cos, tan, pi, E, I
import numpy as np
from invariants import Invariants
import os
import pandas as pd
import random

class SymbolicEngine:
    """
    Manages Bounded Equations as dynamic symbolic objects rather than static strings.
    This allows for runtime equation generation and sophisticated feature extraction.
    """
    def __init__(self, logger, invariants: Invariants, csv_dir: str):
        self.logger = logger
        self.invariants = invariants
        self.csv_dir = csv_dir
        self.base_symbols = self._define_base_symbols()
        self.equations = self._load_symbolic_equations(csv_dir)
        self.logger.log(f"SymbolicEngine initialized with {len(self.equations)} symbolic equations.")

    def _define_base_symbols(self) -> dict:
        """Defines the base variables (P, W, V, etc.) as SymPy symbols."""
        # Core audio metrics
        P, W, V, T, Lambda = symbols('P W V T Lambda')
        
        # Mathematical constants
        phi = self.invariants.PHI
        pi = self.invariants.PI
        e = self.invariants.E
        zeta3 = self.invariants.APERY_CONSTANT
        
        # Complex unit
        i = I
        
        symbols_dict = {
            'P': P, 'W': W, 'V': V, 'T': T, 'Lambda': Lambda,
            'phi': phi, 'pi': pi, 'e': e, 'zeta3': zeta3, 'i': i,
            'sqrt': sqrt, 'exp': exp, 'log': log, 'sin': sin, 'cos': cos, 'tan': tan
        }
        
        # Add Greek letter aliases
        symbols_dict['Λ'] = Lambda
        symbols_dict['ϕ'] = phi
        
        return symbols_dict

    def _load_symbolic_equations(self, csv_dir: str) -> dict:
        """Loads equations from CSVs and parses them into SymPy expressions."""
        symbolic_eqs = {}
        if not os.path.exists(csv_dir):
            self.logger.log(f"⚠️  Bounded CSV directory not found at '{csv_dir}'")
            return {}
        
        csv_files = [f for f in os.listdir(csv_dir) 
                    if f.lower().startswith('bounded') and f.lower().endswith('.csv')]
        
        if not csv_files:
            self.logger.log("⚠️  No Bounded*.csv files found")
            return {}
            
        for file in sorted(csv_files):
            try:
                file_path = os.path.join(csv_dir, file)
                self.logger.log(f"📖 Loading equations from {file}")
                
                df = pd.read_csv(file_path, on_bad_lines='skip')
                equations_loaded = 0
                
                for _, row in df.iterrows():
                    if 'Code' in row and 'Equation (Combinatorial Power)' in row:
                        code = str(row['Code']).strip()
                        eqn_str = str(row['Equation (Combinatorial Power)']).strip()
                        
                        if code and eqn_str and code != 'nan' and eqn_str != 'nan':
                            try:
                                # Convert to symbolic expression
                                safe_eqn_str = eqn_str.replace('^', '**')
                                expr = sympy.sympify(safe_eqn_str, locals=self.base_symbols)
                                symbolic_eqs[code] = expr
                                equations_loaded += 1
                            except Exception as e:
                                self.logger.log(f"  ⚠️  Could not parse equation {code}: {e}")
                                continue
                
                self.logger.log(f"  ↳ Loaded {equations_loaded} equations from {file}")
                
            except Exception as e:
                self.logger.log(f"❌ ERROR parsing {file}: {e}")
                
        return symbolic_eqs

    def get_equation_families(self) -> dict:
        """Categorizes equations based on the fundamental constants they contain."""
        families = {
            'phi_symmetries': [],      # Golden ratio based
            'pi_symmetries': [],       # Circular/periodic based  
            'zeta_symmetries': [],     # Prime number based
            'e_symmetries': [],        # Exponential/growth based
            'trigonometric': [],       # Sin/cos based
            'logarithmic': [],         # Log based
            'base': []                 # Simple algebraic
        }
        
        for name, expr in self.equations.items():
            free_symbols = expr.free_symbols
            atoms = expr.atoms()
            
            # Categorize by contained elements
            if self.base_symbols['phi'] in atoms:
                families['phi_symmetries'].append(name)
            elif self.base_symbols['pi'] in atoms:
                families['pi_symmetries'].append(name)
            elif self.base_symbols['zeta3'] in atoms:
                families['zeta_symmetries'].append(name)
            elif self.base_symbols['e'] in atoms:
                families['e_symmetries'].append(name)
            elif any(fn in atoms for fn in [sin, cos, tan]):
                families['trigonometric'].append(name)
            elif log in atoms:
                families['logarithmic'].append(name)
            else:
                families['base'].append(name)
                
        # Log family statistics
        for family, equations in families.items():
            if equations:
                self.logger.log(f"  📊 {family}: {len(equations)} equations")
                
        return families

    def evaluate_symbolic_expr(self, expr: sympy.Expr, context: dict) -> float:
        """Safely evaluates a symbolic expression with numerical values."""
        try:
            # Substitute numerical values for symbols
            substituted_expr = expr.subs(context)
            
            # Evaluate to a floating point number
            result = substituted_expr.evalf()
            
            # Handle complex results by taking magnitude
            if result.is_complex:
                real_part = float(result.as_real_imag()[0])
                imag_part = float(result.as_real_imag()[1])
                magnitude = np.sqrt(real_part**2 + imag_part**2)
                return magnitude if np.isfinite(magnitude) else self.invariants.IANNOTTI_INVARIANT_ZERO
            else:
                value = float(result)
                return abs(value) if np.isfinite(value) else self.invariants.IANNOTTI_INVARIANT_ZERO
                
        except Exception as e:
            self.logger.log(f"  ⚠️  Error evaluating expression: {e}")
            return self.invariants.IANNOTTI_INVARIANT_ZERO

    def generate_emergent_equation(self, family1: str, family2: str) -> tuple:
        """Creates a new equation by combining two existing ones."""
        families = self.get_equation_families()
        
        if not families[family1] or not families[family2]:
            self.logger.log(f"⚠️  Cannot combine empty families: {family1} + {family2}")
            return None, None
            
        # Randomly select equations from each family
        eq1_name = random.choice(families[family1])
        eq2_name = random.choice(families[family2])
        
        eq1 = self.equations[eq1_name]
        eq2 = self.equations[eq2_name]
        
        # Combine with a random operator
        operators = [Add, Mul]
        op = random.choice(operators)
        
        try:
            emergent_expr = op(eq1, eq2)
            emergent_name = f"EMG_{eq1_name}_{eq2_name}"
            
            # Simplify the new expression
            emergent_expr = sympy.simplify(emergent_expr)
            
            self.logger.log(f"🔬 Generated emergent equation '{emergent_name}'")
            self.logger.log(f"  ↳ {eq1_name} {op.__name__} {eq2_name}")
            
            return emergent_name, emergent_expr
            
        except Exception as e:
            self.logger.log(f"❌ Error combining equations: {e}")
            return None, None

    def create_context_from_metrics(self, base_metrics: dict) -> dict:
        """Creates an evaluation context from base audio metrics."""
        context = {}
        
        # Add all base metrics
        for key, value in base_metrics.items():
            if isinstance(value, (int, float, complex)):
                context[key] = value
            elif hasattr(value, 'item'):  # numpy types
                context[key] = value.item()
                
        # Add mathematical constants
        context.update({
            'phi': self.invariants.PHI,
            'pi': self.invariants.PI, 
            'e': self.invariants.E,
            'zeta3': self.invariants.APERY_CONSTANT,
            'i': 1j
        })
        
        return context

    def evaluate_equation_family(self, family: str, context: dict) -> float:
        """Evaluates all equations in a family and returns the average result."""
        families = self.get_equation_families()
        family_eqs = families.get(family, [])
        
        if not family_eqs:
            return self.invariants.IANNOTTI_INVARIANT_ZERO
            
        values = []
        for eq_name in family_eqs:
            value = self.evaluate_symbolic_expr(self.equations[eq_name], context)
            values.append(value)
            
        return np.mean(values) if values else self.invariants.IANNOTTI_INVARIANT_ZERO
</div>
</div>

<!-- MODULE: analysis_prime_framework.py -->
<div class="module" id="analysis_prime">
<h3>analysis_prime_framework.py - Prime Translation Framework</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# analysis_prime_framework.py: Tri-Resonance Synthesis Engine - Prime Translation Framework
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# A full implementation of the Prime Translation Framework from the Epilogue,
# now augmented with gestural analysis.
# ==============================================================================

import numpy as np
import librosa
from scipy.ndimage import gaussian_filter1d
from sklearn.cluster import KMeans
import pandas as pd
from invariants import Invariants
from analysis_bounded_features import BoundedFeatureExtractor
from analysis_gestural import GesturalAnalyzer

class PrimeTranslationFramework:
    """
    Implements the full pipeline for translating an audio signal into a sequence
    of high-dimensional invariant T-Vectors.
    This is the core analysis engine that deconstructs music into its fundamental components.
    """
    def __init__(self, logger, invariants: Invariants, bounded_extractor: BoundedFeatureExtractor, gestural_analyzer: GesturalAnalyzer):
        self.logger = logger
        self.invariants = invariants
        self.bounded_extractor = bounded_extractor
        self.gestural_analyzer = gestural_analyzer
        self.logger.log("PrimeTranslationFramework initialized - deconstructing musical essence.")

    def analyze_signal(self, y: np.ndarray, sr: int) -> np.ndarray:
        """Main orchestration method for the full analysis pipeline."""
        if y is None or y.size < sr * 0.5: # Require at least 0.5s of audio
            self.logger.log("⚠️  Signal too short for analysis")
            return np.array([])
            
        self.logger.start_section("PRIME TRANSLATION ANALYSIS")

        # Step 1: Extract resonant events (musical onsets)
        event_timestamps = self._extract_resonant_events(y, sr)
        if len(event_timestamps) < max(self.invariants.PRIME_TRANSLATION_WIDTHS):
             self.logger.log(f"⚠️  Insufficient resonant events ({len(event_timestamps)}). Cannot perform full analysis.")
             self.logger.end_section()
             return np.array([])

        # Step 2: Calculate normalized gap anomalies
        ng, gaps = self._calculate_resonant_coordinates(event_timestamps)
        
        # Step 3: Perform dimensional lift through multi-scale smoothing
        smoothed_anomalies = self._perform_dimensional_lift(ng)
        
        # Step 4: Compute the core T-Vectors
        t_vectors = self._compute_t_vectors(y, sr, event_timestamps, ng, smoothed_anomalies)

        # Step 5: Augment with additional feature types
        bounded_features = self.bounded_extractor.extract_features(y, sr)
        gestural_features = self.gestural_analyzer.analyze_signal_gestures(y, sr, event_timestamps)
        
        # Combine all features into enriched T-Vectors
        if bounded_features.size > 0:
            bounded_matrix = np.tile(bounded_features, (len(t_vectors), 1))
            t_vectors = np.hstack((t_vectors, bounded_matrix))
            self.logger.log(f"  ↳ Augmented with {bounded_features.size} bounded features")
            
        if gestural_features.size > 0:
            gestural_matrix = np.tile(gestural_features, (len(t_vectors), 1))
            t_vectors = np.hstack((t_vectors, gestural_matrix))
            self.logger.log(f"  ↳ Augmented with {gestural_features.size} gestural features")
        
        self.logger.log(f"📊 Generated {len(t_vectors)} T-Vectors of dimension {t_vectors.shape[1]}")
        self.logger.end_section()
        return t_vectors

    def _extract_resonant_events(self, y: np.ndarray, sr: int) -> np.ndarray:
        """Identifies core musical events using spectral flux onset detection."""
        self.logger.log("🎵 Detecting resonant events...")
        
        # Use spectral flux for onset detection
        onset_frames = librosa.onset.onset_detect(y=y, sr=sr, units='frames', 
                                                backtrack=True, pre_max=20, post_max=20,
                                                pre_avg=50, post_avg=10, delta=0.1, wait=5)
        event_timestamps = librosa.frames_to_time(onset_frames, sr=sr)
        
        self.logger.log(f"  ↳ Detected {len(event_timestamps)} resonant events")
        return event_timestamps

    def _calculate_resonant_coordinates(self, events: np.ndarray) -> tuple:
        """Calculates the Normalized Gap Anomaly (ngi) stream."""
        # Calculate inter-onset intervals (gaps)
        gaps = np.diff(events, prepend=self.invariants.IANNOTTI_INVARIANT_ZERO)
        
        # Normalize gaps to create anomaly stream
        mu_g = np.mean(gaps)
        sigma_g = np.std(gaps)
        ng = (gaps - mu_g) / (sigma_g + self.invariants.IANNOTTI_INVARIANT_ZERO)
        
        self.logger.log(f"  ↳ Calculated NG stream: μ={mu_g:.3f}, σ={sigma_g:.3f}")
        return ng, gaps

    def _perform_dimensional_lift(self, ng: np.ndarray) -> dict:
        """Applies multi-scale Gaussian smoothing to the ng stream."""
        smoothed = {}
        for width in self.invariants.PRIME_TRANSLATION_WIDTHS:
            sigma = width / 2.355  # Convert FWHM to sigma
            smoothed[width] = gaussian_filter1d(ng, sigma=sigma, mode='reflect')
            
        self.logger.log(f"  ↳ Applied dimensional lift with widths: {self.invariants.PRIME_TRANSLATION_WIDTHS}")
        return smoothed

    def _compute_t_vectors(self, y: np.ndarray, sr: int, events: np.ndarray, 
                         ng: np.ndarray, smoothed_anomalies: dict) -> np.ndarray:
        """Assembles the final 5D T-Vector for each resonant event."""
        num_events = len(events)
        t_vectors = np.zeros((num_events, 5))
        
        # Calculate timbral classes (generalized residue)
        timbral_classes = self._calculate_generalized_residue(y, sr, events)
        
        # 1. OI (Overall Irregularity) - sum of squared smoothed anomalies
        oi_matrix = np.vstack([s**2 for s in smoothed_anomalies.values()]).T
        t_vectors[:, 0] = np.sum(oi_matrix, axis=1)

        # 2. Rho (Relative prominence) - ratio of 23-width component to total
        numerator = np.abs(smoothed_anomalies[23])
        denominator = np.sum(np.abs(np.vstack(list(smoothed_anomalies.values()))), axis=0)
        t_vectors[:, 1] = numerator / (denominator + self.invariants.IANNOTTI_INVARIANT_ZERO)

        # 3. Gamma (Local stability) - rolling mean of absolute NG values
        df = pd.DataFrame({'ng': ng})
        t_vectors[:, 2] = df['ng'].abs().rolling(window=100, min_periods=1, center=True).mean().values

        # 4. R (Timbral class) - quantized spectral characteristics
        t_vectors[:, 3] = timbral_classes
        
        # 5. R_dev (Timbral deviation) - local vs global timbral density
        s_timbral = pd.Series(timbral_classes)
        global_density = s_timbral.value_counts(normalize=True).to_dict()
        local_density = s_timbral.rolling(50, min_periods=1).apply(
            lambda x: np.sum(x == x.iloc[-1])/len(x), raw=False).values
        t_vectors[:, 4] = local_density - s_timbral.map(global_density).values

        # Handle any NaN values
        t_vectors = np.nan_to_num(t_vectors, nan=self.invariants.IANNOTTI_INVARIANT_ZERO)
        
        self.logger.log(f"  ↳ Computed 5D T-Vectors for {num_events} events")
        return t_vectors

    def _calculate_generalized_residue(self, y: np.ndarray, sr: int, events: np.ndarray) -> np.ndarray:
        """Quantizes spectral features into a fixed number of timbral classes."""
        if len(events) == 0:
            return np.array([])
            
        event_frames = librosa.time_to_frames(events, sr=sr)
        S = np.abs(librosa.stft(y))
        
        # Extract multiple spectral features for better clustering
        centroids = librosa.feature.spectral_centroid(S=S, sr=sr)[0]
        bandwidths = librosa.feature.spectral_bandwidth(S=S, sr=sr)[0]
        rolloff = librosa.feature.spectral_rolloff(S=S, sr=sr)[0]
        
        # Combine features into a multi-dimensional representation
        features = np.vstack([
            centroids[event_frames], 
            bandwidths[event_frames],
            rolloff[event_frames]
        ]).T
        
        num_classes = self.invariants.PRIME_ARCHETYPE_COUNT
        
        # If we don't have enough unique points, use simple factorization
        if len(np.unique(features[:,0])) < num_classes:
            self.logger.log(f"  ↳ Using simple factorization (insufficient unique features)")
            return pd.factorize(features[:,0])[0]

        # Use K-means clustering to find timbral archetypes
        try:
            kmeans = KMeans(n_clusters=num_classes, random_state=0, n_init='auto').fit(features)
            self.logger.log(f"  ↳ Clustered into {num_classes} timbral archetypes")
            return kmeans.labels_
        except Exception as e:
            self.logger.log(f"⚠️  Clustering failed: {e}, using first feature")
            return pd.factorize(features[:,0])[0]

    def analyze_signal_comprehensive(self, y: np.ndarray, sr: int) -> dict:
        """Returns a comprehensive analysis including intermediate results."""
        if y is None or y.size == 0:
            return {}
            
        analysis = {}
        
        # Get the main T-vectors
        analysis['t_vectors'] = self.analyze_signal(y, sr)
        
        # Also return some intermediate results for insight
        events = self._extract_resonant_events(y, sr)
        if len(events) > 0:
            ng, gaps = self._calculate_resonant_coordinates(events)
            analysis['events'] = events
            analysis['gaps'] = gaps
            analysis['ng_stream'] = ng
            
        return analysis
</div>
</div>

<!-- MODULE: analysis_sentiment.py -->
<div class="module" id="analysis_sentiment">
<h3>analysis_sentiment.py - Sentiment Analysis</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# analysis_sentiment.py: Tri-Resonance Synthesis Engine - Secure Local Sentiment Analysis
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# This module performs secure, offline sentiment analysis using a local
# transformer model and applies the Prime Translation Framework to the results.
# ==============================================================================

import numpy as np
import os
from invariants import Invariants
from scipy.ndimage import gaussian_filter1d

# Configure environment for transformers
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
os.environ['TRANSFORMERS_OFFLINE'] = '1'

try:
    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    pipeline = None

class SecureSentimentAnalyzer:
    """
    Performs sentiment analysis on lyrics using a local transformer model.
    Operates completely offline for security and privacy.
    """
    def __init__(self, logger, invariants: Invariants):
        self.logger = logger
        self.invariants = invariants
        self.sentiment_pipeline = None
        self.model_loaded = False

        if not TRANSFORMERS_AVAILABLE:
            self.logger.log("❌ CRITICAL: 'transformers' library not found. Disabling sentiment analysis.")
            self.logger.log("   To enable: pip install transformers torch")
            return

        model_name = "distilbert-base-uncased-finetuned-sst-2-english"
        try:
            self.logger.log(f"📚 Loading local transformer model: '{model_name}'")
            self.logger.log("   (First run may download model ~250MB)")
            
            # Load model and tokenizer explicitly for better control
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSequenceClassification.from_pretrained(model_name)
            
            self.sentiment_pipeline = pipeline(
                'sentiment-analysis',
                model=model,
                tokenizer=tokenizer,
                device=-1  # Use CPU for compatibility
            )
            
            self.model_loaded = True
            self.logger.log("✅ Local sentiment model loaded successfully for offline use.")
            
        except Exception as e:
            self.logger.log(f"❌ CRITICAL: Failed to load sentiment model: {e}")
            self.logger.log("   Sentiment analysis disabled. Using fallback methods.")
            self.sentiment_pipeline = None

    def analyze(self, lyrics: str) -> np.ndarray:
        """Analyzes lyrics to produce a 5D sentiment fingerprint."""
        if not self.model_loaded or not lyrics or not lyrics.strip():
            return self._fallback_sentiment_analysis(lyrics)

        self.logger.start_section("LYRIC SENTIMENT ANALYSIS")

        # Preprocess lyrics
        lines = [line.strip() for line in lyrics.split('\n') 
                if line.strip() and len(line.strip()) > 5]
                
        if not lines:
            self.logger.log("⚠️  No analyzable lines found in lyrics")
            self.logger.end_section()
            return np.full(5, self.invariants.IANNOTTI_INVARIANT_ZERO)
            
        self.logger.log(f"📝 Analyzing {len(lines)} lyrical lines...")

        try:
            # Process in batches to avoid memory issues
            batch_size = 32
            all_results = []
            
            for i in range(0, len(lines), batch_size):
                batch = lines[i:i + batch_size]
                batch_results = self.sentiment_pipeline(
                    batch, 
                    truncation=True, 
                    max_length=512, 
                    padding=True
                )
                all_results.extend(batch_results)
                
        except Exception as e:
            self.logger.log(f"❌ ERROR during sentiment analysis: {e}")
            self.logger.end_section()
            return self._fallback_sentiment_analysis(lyrics)

        # Convert results to numerical sentiment stream
        sentiment_stream = np.array([
            result['score'] if result['label'] == 'POSITIVE' else -result['score'] 
            for result in all_results
        ])
        
        self.logger.log(f"  ↳ Sentiment range: [{np.min(sentiment_stream):.3f}, {np.max(sentiment_stream):.3f}]")

        # Apply Prime Translation Framework to sentiment stream
        fingerprint = self._apply_prime_translation_to_sentiment(sentiment_stream)
        
        self.logger.log_vector("Generated 5D Sentiment Fingerprint", fingerprint)
        self.logger.end_section()
        return fingerprint

    def _apply_prime_translation_to_sentiment(self, sentiment_stream: np.ndarray) -> np.ndarray:
        """Applies the Prime Translation Framework to sentiment data."""
        if len(sentiment_stream) < 5:
            self.logger.log("⚠️  Not enough sentiment data for dimensional lift")
            mean_s, std_s = np.mean(sentiment_stream), np.std(sentiment_stream)
            return np.array([
                self.invariants.IANNOTTI_INVARIANT_ZERO,
                self.invariants.IANNOTTI_INVARIANT_ZERO, 
                self.invariants.IANNOTTI_INVARIANT_ZERO,
                mean_s,
                std_s
            ])

        # Normalize sentiment stream to create NG-like series
        mean_s, std_s = np.mean(sentiment_stream), np.std(sentiment_stream)
        ng = (sentiment_stream - mean_s) / (std_s + self.invariants.IANNOTTI_INVARIANT_ZERO)
        
        # Apply multi-scale smoothing (dimensional lift)
        smoothed = {}
        for width in self.invariants.PRIME_TRANSLATION_WIDTHS:
            if len(ng) >= width:
                sigma = width / 2.355
                smoothed[width] = gaussian_filter1d(ng, sigma=sigma, mode='reflect')

        if not smoothed:
            self.logger.log("⚠️  Not enough data for meaningful smoothing")
            return np.array([
                self.invariants.IANNOTTI_INVARIANT_ZERO,
                self.invariants.IANNOTTI_INVARIANT_ZERO,
                self.invariants.IANNOTTI_INVARIANT_ZERO, 
                mean_s,
                std_s
            ])

        # Compute T-Vector components for sentiment
        # 1. OI (Overall Irregularity)
        oi = sum(np.mean(s**2) for s in smoothed.values())
        
        # 2. Rho (Relative prominence of medium-term patterns)
        rho_num = np.mean(np.abs(smoothed.get(23, self.invariants.IANNOTTI_INVARIANT_ZERO)))
        rho_den = sum(np.mean(np.abs(s)) for s in smoothed.values())
        rho = rho_num / (rho_den + self.invariants.IANNOTTI_INVARIANT_ZERO)

        # 3. Gamma (Local stability)
        gamma = np.mean(np.abs(ng))
        
        # 4. R (Central sentiment)
        r = mean_s
        
        # 5. R_dev (Sentiment variability)
        r_dev = std_s
        
        fingerprint = np.array([oi, rho, gamma, r, r_dev])
        fingerprint = np.nan_to_num(fingerprint, nan=self.invariants.IANNOTTI_INVARIANT_ZERO)

        return fingerprint

    def _fallback_sentiment_analysis(self, lyrics: str) -> np.ndarray:
        """Fallback method when transformer model is unavailable."""
        self.logger.log("🔄 Using fallback sentiment analysis")
        
        if not lyrics or not lyrics.strip():
            return np.full(5, self.invariants.IANNOTTI_INVARIANT_ZERO)
            
        # Simple lexicon-based approach
        positive_words = {'love', 'happy', 'joy', 'beautiful', 'wonderful', 'amazing', 'good', 'great'}
        negative_words = {'hate', 'sad', 'pain', 'terrible', 'awful', 'bad', 'horrible'}
        
        words = lyrics.lower().split()
        total_words = len(words)
        
        if total_words == 0:
            return np.full(5, self.invariants.IANNOTTI_INVARIANT_ZERO)
            
        positive_count = sum(1 for word in words if word in positive_words)
        negative_count = sum(1 for word in words if word in negative_words)
        
        # Simple sentiment score
        sentiment_score = (positive_count - negative_count) / total_words
        
        # Create a simple fingerprint
        fingerprint = np.array([
            abs(sentiment_score),  # OI-like
            0.5,                  # Rho-like (neutral)
            abs(sentiment_score), # Gamma-like  
            sentiment_score,      # R (sentiment)
            abs(sentiment_score)  # R_dev
        ])
        
        self.logger.log(f"  ↳ Fallback analysis: sentiment={sentiment_score:.3f}")
        return fingerprint

    def analyze_emotional_arc(self, lyrics: str) -> dict:
        """Analyzes the emotional arc of the lyrics over time."""
        if not self.model_loaded or not lyrics:
            return {}
            
        lines = [line.strip() for line in lyrics.split('\n') if line.strip()]
        if len(lines) < 3:
            return {}
            
        try:
            results = self.sentiment_pipeline(lines, truncation=True, max_length=512, padding=True)
            sentiments = [r['score'] if r['label'] == 'POSITIVE' else -r['score'] for r in results]
            
            arc_analysis = {
                'start_sentiment': sentiments[0],
                'end_sentiment': sentiments[-1],
                'peak_sentiment': max(sentiments),
                'valley_sentiment': min(sentiments),
                'emotional_range': max(sentiments) - min(sentiments),
                'trend': sentiments[-1] - sentiments[0]
            }
            
            return arc_analysis
            
        except Exception as e:
            self.logger.log(f"⚠️  Error analyzing emotional arc: {e}")
            return {}
</div>
</div>

<!-- MODULE: analysis_bounded_features.py -->
<div class="module" id="analysis_bounded">
<h3>analysis_bounded_features.py - Bounded Features</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# analysis_bounded_features.py: Tri-Resonance Synthesis Engine - Bounded Equation Feature Extractor
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# This module acts as an interface to the Symbolic Engine, evaluating the
# Bounded Equations for a given audio signal.
# ==============================================================================

import numpy as np
from invariants import Invariants
from analysis_base_metrics import BaseMetricsExtractor
from symbolic_engine import SymbolicEngine

class BoundedFeatureExtractor:
    """
    Evaluates the symbolic Bounded Equations using base metrics from an audio signal.
    Transforms raw audio into a rich feature space defined by mathematical symmetries.
    """
    def __init__(self, logger, invariants: Invariants, base_metrics_extractor: BaseMetricsExtractor, symbolic_engine: SymbolicEngine):
        self.logger = logger
        self.invariants = invariants
        self.base_metrics_extractor = base_metrics_extractor
        self.symbolic_engine = symbolic_engine
        self.feature_count = len(self.symbolic_engine.equations)
        self.logger.log(f"BoundedFeatureExtractor initialized with {self.feature_count} symbolic equations.")

    def extract_features(self, y: np.ndarray, sr: int) -> np.ndarray:
        """Applies all loaded equations to the audio's base metrics."""
        if self.feature_count == 0 or y is None or y.size == 0:
            self.logger.log("⚠️  No equations available or empty audio signal")
            return np.array([])

        self.logger.start_section("EVALUATING BOUNDED EQUATION SYMMETRIES")
        
        # Extract base audio metrics
        base_metrics_values = self.base_metrics_extractor.get_base_metrics(y, sr)
        
        # Create numerical context for symbolic evaluation
        numerical_context = self.symbolic_engine.create_context_from_metrics(base_metrics_values)
        
        # Evaluate all equations
        feature_vector = []
        equation_results = {}
        
        for code, expr in sorted(self.symbolic_engine.equations.items()):
            value = self.symbolic_engine.evaluate_symbolic_expr(expr, numerical_context)
            feature_vector.append(value)
            equation_results[code] = value
            
        self.logger.log(f"📊 Evaluated {len(feature_vector)} Bounded Equation features")
        
        # Log some statistics about the features
        if feature_vector:
            feature_array = np.array(feature_vector)
            self.logger.log(f"  ↳ Feature range: [{np.min(feature_array):.3e}, {np.max(feature_array):.3e}]")
            self.logger.log(f"  ↳ Feature mean: {np.mean(feature_array):.3e} ± {np.std(feature_array):.3e}")
            
        self.logger.end_section()
        return np.array(feature_vector)
        
    def get_structured_features(self, y: np.ndarray, sr: int) -> dict:
        """Returns features grouped into their conceptual families."""
        if self.feature_count == 0 or y is None or y.size == 0:
            return {}
            
        self.logger.log("📚 Computing structured feature families...")
        
        base_metrics_values = self.base_metrics_extractor.get_base_metrics(y, sr)
        numerical_context = self.symbolic_engine.create_context_from_metrics(base_metrics_values)
        
        families = self.symbolic_engine.get_equation_families()
        structured_features = {}
        
        for family_name, eq_codes in families.items():
            if eq_codes:
                family_values = []
                for code in eq_codes:
                    value = self.symbolic_engine.evaluate_symbolic_expr(
                        self.symbolic_engine.equations[code], numerical_context)
                    family_values.append(value)
                    
                structured_features[family_name] = (np.mean(family_values) if family_values 
                                                  else self.invariants.IANNOTTI_INVARIANT_ZERO)
            else:
                structured_features[family_name] = self.invariants.IANNOTTI_INVARIANT_ZERO
                
        # Log family strengths
        for family, value in structured_features.items():
            if value > self.invariants.IANNOTTI_INVARIANT_ZERO:
                self.logger.log(f"  📈 {family}: {value:.3e}")
                
        return structured_features

    def generate_emergent_features(self, y: np.ndarray, sr: int, num_new_features: int = 5) -> np.ndarray:
        """Generates new features by combining existing equation families."""
        if self.feature_count == 0 or y is None or y.size == 0:
            return np.array([])
            
        self.logger.log(f"🔬 Generating {num_new_features} emergent features...")
        
        base_metrics_values = self.base_metrics_extractor.get_base_metrics(y, sr)
        numerical_context = self.symbolic_engine.create_context_from_metrics(base_metrics_values)
        
        families = self.symbolic_engine.get_equation_families()
        non_empty_families = [f for f in families if families[f]]
        
        if len(non_empty_families) < 2:
            self.logger.log("⚠️  Need at least 2 equation families for emergence")
            return np.array([])
            
        emergent_features = []
        attempts = 0
        max_attempts = num_new_features * 3
        
        while len(emergent_features) < num_new_features and attempts < max_attempts:
            attempts += 1
            
            # Randomly select two different families
            family1, family2 = np.random.choice(non_empty_families, 2, replace=False)
            
            # Generate emergent equation
            emergent_name, emergent_expr = self.symbolic_engine.generate_emergent_equation(family1, family2)
            
            if emergent_expr is not None:
                # Evaluate the new equation
                value = self.symbolic_engine.evaluate_symbolic_expr(emergent_expr, numerical_context)
                emergent_features.append(value)
                self.logger.log(f"  🎯 {emergent_name}: {value:.3e}")
                
        self.logger.log(f"  ↳ Generated {len(emergent_features)} emergent features")
        return np.array(emergent_features)

    def get_feature_interpretation(self, feature_values: np.ndarray, top_k: int = 5) -> dict:
        """
        Provides interpretation of what the feature values might mean musically.
        Returns the most significant features and their potential meanings.
        """
        if len(feature_values) == 0:
            return {}
            
        # Get equation codes in the same order as feature values
        equation_codes = sorted(self.symbolic_engine.equations.keys())
        
        # Find top-k most significant features
        abs_values = np.abs(feature_values)
        if len(abs_values) < top_k:
            top_k = len(abs_values)
            
        top_indices = np.argpartition(abs_values, -top_k)[-top_k:]
        top_indices = top_indices[np.argsort(-abs_values[top_indices])]
        
        interpretation = {}
        for idx in top_indices:
            if idx < len(equation_codes):
                code = equation_codes[idx]
                value = feature_values[idx]
                
                # Simple interpretation based on feature value and equation type
                if 'phi' in code.lower() or 'ϕ' in code:
                    meaning = "Golden ratio symmetry"
                elif 'pi' in code.lower():
                    meaning = "Circular/periodic structure"  
                elif 'zeta' in code.lower():
                    meaning = "Prime number resonance"
                elif 'log' in code.lower():
                    meaning = "Logarithmic scaling"
                elif 'exp' in code.lower():
                    meaning = "Exponential growth/decay"
                else:
                    meaning = "Algebraic structure"
                    
                interpretation[code] = {
                    'value': value,
                    'magnitude': abs(value),
                    'meaning': meaning,
                    'sign': 'positive' if value > 0 else 'negative'
                }
                
        return interpretation
</div>
</div>

<!-- MODULE: analysis_base_metrics.py -->
<div class="module" id="analysis_base">
<h3>analysis_base_metrics.py - Base Audio Metrics</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# analysis_base_metrics.py: Tri-Resonance Synthesis Engine - Base Audio Metrics
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# A dedicated helper module to extract the fundamental audio metrics that serve
# as variables for the Bounded Equations and other analysis modules.
# ==============================================================================

import numpy as np
import librosa
from invariants import Invariants
from scipy.special import jv  # Bessel function
from scipy import stats

class BaseMetricsExtractor:
    """
    Extracts a set of fundamental audio metrics to serve as variables for the
    symbolic engine and other analysis components.
    
    This is the translation layer from raw audio to abstract mathematical variables.
    """
    def __init__(self, logger, invariants: Invariants):
        self.logger = logger
        self.invariants = invariants
        self.logger.log("BaseMetricsExtractor initialized - extracting audio essence.")

    def get_base_metrics(self, y: np.ndarray, sr: int) -> dict:
        """
        Extracts a comprehensive dictionary of metrics from an audio signal.
        These metrics (P, W, V, T, Λ, etc.) become the variables for the Bounded Equations.
        """
        if y is None or y.size == 0:
            return self._get_empty_metrics()
            
        self.logger.log("🎵 Extracting base audio metrics...")

        try:
            # --- CORE METRICS (From Framework) ---
            
            # P (Potential/Coherence): 1.0 - spectral flatness
            # High P = tonal, low P = noisy
            spectral_flatness = np.mean(librosa.feature.spectral_flatness(y=y))
            P = 1.0 - spectral_flatness
            
            # W (Waste/Entropy): Normalized spectral bandwidth
            # High W = spread out frequencies, low W = focused frequencies
            spectral_bw = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))
            W = spectral_bw / (sr / 2.0)  # Normalize to [0,1]
            
            # V (Value/Energy): RMS energy
            # Represents overall loudness/energy
            rms = librosa.feature.rms(y=y)
            V = np.mean(rms)
            
            # T (Time/Tempo): Normalized tempo
            # Represents rhythmic speed
            onset_env = librosa.onset.onset_strength(y=y, sr=sr)
            tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)[0]
            T = tempo / 120.0  # Normalize around 120 BPM

            # Λ (Lambda/Legitimacy): Harmonic-to-percussive ratio
            # High Λ = harmonic, low Λ = percussive
            y_harmonic, y_percussive = librosa.effects.hpss(y)
            harmonic_power = np.sum(y_harmonic**2)
            total_power = np.sum(y**2)
            Lambda = harmonic_power / (total_power + self.invariants.IANNOTTI_INVARIANT_ZERO)
            
            # --- EXPANDED METRICS ---
            
            # Spectral characteristics
            spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr))
            zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y))
            spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))
            
            # Temporal characteristics
            autocorr = np.correlate(y, y, mode='full')
            autocorr = autocorr[len(y)-1:] / np.max(autocorr)
            temporal_stability = np.mean(autocorr[:100])  # Short-term stability
            
            # Statistical moments
            skewness = stats.skew(y)
            kurtosis = stats.kurtosis(y)
            
            # --- MATHEMATICAL CONSTANTS ---
            metrics = {
                # Core framework variables
                'P': P, 'W': W, 'V': V, 'T': T, 'Lambda': Lambda,
                
                # Expanded audio metrics
                'contrast': spectral_contrast,
                'zcr': zero_crossing_rate, 
                'rolloff': spectral_rolloff,
                'stability': temporal_stability,
                'skewness': skewness,
                'kurtosis': kurtosis,
                
                # Mathematical constants
                'i': 1j,
                'phi': self.invariants.PHI,
                'pi': self.invariants.PI, 
                'e': self.invariants.E,
                'zeta3': self.invariants.APERY_CONSTANT,
                'sqrt2': self.invariants.SQRT2,
                
                # Complexity measures
                'O1': 1.0,
                'On': float(len(y)),
                'log_n': np.log(len(y) + 1),
                
                # Special functions
                'gamma_half': np.sqrt(self.invariants.PI),  # Γ(1/2)
                'bessel_pi': jv(0, self.invariants.PI),    # J₀(π)
                
                # Signal properties
                'max_amplitude': np.max(np.abs(y)),
                'dynamic_range': np.max(y) - np.min(y),
                'crest_factor': np.max(np.abs(y)) / (np.std(y) + self.invariants.IANNOTTI_INVARIANT_ZERO)
            }
            
            # Add Greek aliases for symbolic evaluation
            metrics['Λ'] = metrics['Lambda']
            metrics['ϕ'] = metrics['phi']
            metrics['ζ(3)'] = metrics['zeta3']
            
            self._log_metric_summary(metrics)
            return metrics
            
        except Exception as e:
            self.logger.log(f"❌ Error extracting base metrics: {e}")
            return self._get_empty_metrics()

    def _get_empty_metrics(self) -> dict:
        """Returns a set of neutral metrics for empty/invalid signals."""
        return {
            'P': 0.5, 'W': 0.5, 'V': self.invariants.IANNOTTI_INVARIANT_ZERO, 
            'T': 1.0, 'Lambda': 0.5, 'Λ': 0.5,
            'contrast': 0.5, 'zcr': 0.5, 'rolloff': 0.5, 'stability': 0.5,
            'skewness': 0.0, 'kurtosis': 0.0,
            'i': 1j, 'phi': self.invariants.PHI, 'ϕ': self.invariants.PHI,
            'pi': self.invariants.PI, 'e': self.invariants.E, 
            'zeta3': self.invariants.APERY_CONSTANT, 'ζ(3)': self.invariants.APERY_CONSTANT,
            'sqrt2': self.invariants.SQRT2,
            'O1': 1.0, 'On': 1000.0, 'log_n': np.log(1000),
            'gamma_half': np.sqrt(self.invariants.PI),
            'bessel_pi': jv(0, self.invariants.PI),
            'max_amplitude': 0.0, 'dynamic_range': 0.0, 'crest_factor': 0.0
        }

    def _log_metric_summary(self, metrics: dict):
        """Logs a summary of the extracted metrics."""
        self.logger.log("📊 Base Metrics Summary:")
        self.logger.log(f"  🎵 P (Coherence): {metrics['P']:.3f}")
        self.logger.log(f"  🗑️  W (Entropy): {metrics['W']:.3f}")
        self.logger.log(f"  💪 V (Energy): {metrics['V']:.3f}")
        self.logger.log(f"  ⏱️  T (Tempo): {metrics['T']:.3f}")
        self.logger.log(f"  🎻 Λ (Harmonicity): {metrics['Lambda']:.3f}")
        self.logger.log(f"  📈 Contrast: {metrics['contrast']:.3f}")
        self.logger.log(f"  🔁 Stability: {metrics['stability']:.3f}")

    def get_metric_ranges(self) -> dict:
        """Returns typical ranges for each metric for normalization reference."""
        return {
            'P': (0.0, 1.0),      # Coherence (noise -> tone)
            'W': (0.0, 1.0),      # Spectral bandwidth (focused -> spread)
            'V': (0.0, 1.0),      # Energy (silence -> loud)
            'T': (0.1, 3.0),      # Tempo (very slow -> very fast)
            'Lambda': (0.0, 1.0), # Harmonicity (percussive -> harmonic)
            'contrast': (0.0, 10.0),   # Spectral contrast
            'zcr': (0.0, 0.5),    # Zero-crossing rate
            'stability': (0.0, 1.0),   # Temporal stability
            'skewness': (-2.0, 2.0),   # Amplitude distribution skew
            'kurtosis': (-2.0, 10.0),  # Amplitude distribution kurtosis
        }

    def normalize_metrics(self, metrics: dict) -> dict:
        """Normalizes metrics to typical ranges for better equation behavior."""
        ranges = self.get_metric_ranges()
        normalized = metrics.copy()
        
        for key, (min_val, max_val) in ranges.items():
            if key in normalized and isinstance(normalized[key], (int, float)):
                # Clip to range and normalize to [0,1]
                clipped = np.clip(normalized[key], min_val, max_val)
                normalized[key] = (clipped - min_val) / (max_val - min_val)
                
        return normalized
</div>
</div>

<!-- MODULE: analysis_gestural.py -->
<div class="module" id="analysis_gestural">
<h3>analysis_gestural.py - Gestural Analysis</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# analysis_gestural.py: Tri-Resonance Synthesis Engine - Gestural Analyzer
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# A new module to analyze music in terms of "gestures" - directed paths
# through a mathematical space, as per "Cool Math for Hot Music".
# ==============================================================================

import numpy as np
import librosa
from scipy.signal import find_peaks
from invariants import Invariants

class GesturalAnalyzer:
    """
    Analyzes the gestural properties of a musical signal.
    Gestures are the shapes and motions that give music its expressive quality.
    """
    def __init__(self, logger, invariants: Invariants):
        self.logger = logger
        self.invariants = invariants
        self.logger.log("GesturalAnalyzer initialized - capturing musical motion.")

    def analyze_pitch_gesture(self, y: np.ndarray, sr: int) -> dict:
        """Analyzes the overall melodic gesture of a signal."""
        self.logger.log("🎶 Analyzing pitch gestures...")
        
        try:
            # Extract pitch contour using PYIN for better accuracy
            f0, voiced_flag, voiced_probs = librosa.pyin(y, 
                                                        fmin=librosa.note_to_hz('C2'),
                                                        fmax=librosa.note_to_hz('C7'),
                                                        sr=sr,
                                                        frame_length=2048)
            
            # Use only voiced frames (where pitch is detected)
            voiced_f0 = f0[voiced_flag]
            
            if len(voiced_f0) < 10:
                self.logger.log("⚠️  Insufficient pitch data for gestural analysis")
                return self._get_neutral_gesture()
                
            pitch_contour = voiced_f0
            
            # 1. Average Velocity (speed of pitch change in octaves/second)
            pitch_changes = np.diff(np.log2(pitch_contour + 1e-9))  # Log changes in octaves
            velocity = np.mean(np.abs(pitch_changes)) * sr / 2048  # Convert to octaves/sec
            
            # 2. Total Curvature (how "twisty" the melody is)
            if len(pitch_contour) >= 3:
                # Second derivative approximation
                acceleration = np.diff(pitch_contour, n=2)
                curvature = np.sum(np.abs(acceleration)) / len(acceleration)
            else:
                curvature = 0.0
                
            # 3. Extrema Count (number of melodic peaks and valleys)
            # Normalize contour for peak detection
            norm_contour = (pitch_contour - np.mean(pitch_contour)) / (np.std(pitch_contour) + 1e-9)
            peaks, _ = find_peaks(norm_contour, height=0.5, distance=5)
            valleys, _ = find_peaks(-norm_contour, height=0.5, distance=5)
            extrema_count = len(peaks) + len(valleys)
            
            # 4. Gestural Smoothness (inverse of jaggedness)
            jerk = np.diff(pitch_contour, n=3) if len(pitch_contour) >= 4 else np.array([0])
            smoothness = 1.0 / (1.0 + np.mean(np.abs(jerk)))
            
            gesture = {
                'avg_velocity': float(velocity),
                'total_curvature': float(curvature),
                'extrema_count': int(extrema_count),
                'smoothness': float(smoothness),
                'contour_length': len(pitch_contour)
            }
            
            self.logger.log(f"  ↳ Pitch: velocity={velocity:.3f}, curvature={curvature:.3f}, extrema={extrema_count}")
            return gesture
            
        except Exception as e:
            self.logger.log(f"❌ Error in pitch gesture analysis: {e}")
            return self._get_neutral_gesture()

    def analyze_rhythmic_gesture(self, y: np.ndarray, sr: int, events: np.ndarray) -> dict:
        """Analyzes the gestural properties of rhythm."""
        self.logger.log("🥁 Analyzing rhythmic gestures...")
        
        if len(events) < 3:
            self.logger.log("⚠️  Insufficient events for rhythmic analysis")
            return {'ioi_stability': 0.5, 'syncopation_score': 0.5, 'groove_consistency': 0.5}
            
        try:
            # Inter-Onset Interval (IOI) stability
            iois = np.diff(events)
            if np.mean(iois) > self.invariants.IANNOTTI_INVARIANT_ZERO:
                ioi_stability = 1.0 - (np.std(iois) / np.mean(iois))
            else:
                ioi_stability = 0.5
                
            # Syncopation Score 
            onset_env = librosa.onset.onset_strength(y=y, sr=sr)
            tempo, beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)
            beat_times = librosa.frames_to_time(beats, sr=sr)
            
            syncopation = 0.0
            if len(beat_times) > 0:
                for onset_time in events:
                    # Find nearest beat
                    beat_distances = np.abs(onset_time - beat_times)
                    min_dist = np.min(beat_distances)
                    
                    # Syncopation increases with distance from beat
                    beat_period = np.mean(np.diff(beat_times)) if len(beat_times) > 1 else 0.5
                    if beat_period > 0:
                        syncopation += min_dist / beat_period
                        
                syncopation /= len(events)
                
            # Groove consistency (regularity of rhythmic patterns)
            if len(iois) >= 4:
                # Look for repeating patterns in IOIs
                autocorr = np.correlate(iois, iois, mode='full')
                autocorr = autocorr[len(iois)-1:] / np.max(autocorr)
                
                # Find secondary peaks indicating pattern repetition
                peaks, _ = find_peaks(autocorr[1:], height=0.3, distance=2)  # Skip first peak (self-correlation)
                groove_consistency = len(peaks) / min(10, len(iois))  # Normalize
            else:
                groove_consistency = 0.5
                
            gesture = {
                'ioi_stability': float(np.clip(ioi_stability, 0, 1)),
                'syncopation_score': float(np.clip(syncopation, 0, 1)),
                'groove_consistency': float(np.clip(groove_consistency, 0, 1))
            }
            
            self.logger.log(f"  ↳ Rhythm: stability={ioi_stability:.3f}, syncopation={syncopation:.3f}")
            return gesture
            
        except Exception as e:
            self.logger.log(f"❌ Error in rhythmic gesture analysis: {e}")
            return {'ioi_stability': 0.5, 'syncopation_score': 0.5, 'groove_consistency': 0.5}

    def analyze_dynamic_gesture(self, y: np.ndarray, sr: int) -> dict:
        """Analyzes dynamic (loudness) gestures."""
        self.logger.log("📢 Analyzing dynamic gestures...")
        
        try:
            # Extract RMS envelope
            rms = librosa.feature.rms(y=y)[0]
            hop_length = 512
            times = librosa.times_like(rms, sr=sr, hop_length=hop_length)
            
            if len(rms) < 10:
                return {'dynamic_range': 0.5, 'attack_slope': 0.5, 'decay_slope': 0.5}
                
            # Dynamic range (normalized)
            dynamic_range = (np.max(rms) - np.min(rms)) / (np.max(rms) + 1e-9)
            
            # Find attack and decay slopes
            peaks, _ = find_peaks(rms, height=np.mean(rms), distance=5)
            
            attack_slopes = []
            decay_slopes = []
            
            for peak in peaks[:5]:  # Analyze first 5 peaks
                if peak > 0:
                    # Attack: from valley before peak to peak
                    pre_valley = np.argmin(rms[max(0, peak-10):peak])
                    if pre_valley < peak:
                        attack = (rms[peak] - rms[pre_valley]) / (peak - pre_valley)
                        attack_slopes.append(attack)
                        
                if peak < len(rms) - 1:
                    # Decay: from peak to valley after peak
                    post_valley = peak + np.argmin(rms[peak:min(len(rms), peak+10)])
                    if post_valley > peak:
                        decay = (rms[peak] - rms[post_valley]) / (post_valley - peak)
                        decay_slopes.append(decay)
                        
            avg_attack = np.mean(attack_slopes) if attack_slopes else 0.0
            avg_decay = np.mean(decay_slopes) if decay_slopes else 0.0
            
            # Normalize slopes
            max_slope = np.max(rms) / 10.0  # Reasonable maximum slope
            attack_slope = np.clip(avg_attack / max_slope, 0, 1)
            decay_slope = np.clip(avg_decay / max_slope, 0, 1)
            
            gesture = {
                'dynamic_range': float(dynamic_range),
                'attack_slope': float(attack_slope),
                'decay_slope': float(decay_slope)
            }
            
            self.logger.log(f"  ↳ Dynamics: range={dynamic_range:.3f}, attack={attack_slope:.3f}")
            return gesture
            
        except Exception as e:
            self.logger.log(f"❌ Error in dynamic gesture analysis: {e}")
            return {'dynamic_range': 0.5, 'attack_slope': 0.5, 'decay_slope': 0.5}

    def analyze_signal_gestures(self, y: np.ndarray, sr: int, events: np.ndarray) -> np.ndarray:
        """Produces a comprehensive feature vector for all gestural properties."""
        self.logger.log("🎭 Computing comprehensive gestural signature...")
        
        pitch_gesture = self.analyze_pitch_gesture(y, sr)
        rhythm_gesture = self.analyze_rhythmic_gesture(y, sr, events)
        dynamic_gesture = self.analyze_dynamic_gesture(y, sr)
        
        # Combine all gestural features into a single vector
        feature_vector = np.array([
            pitch_gesture['avg_velocity'],
            pitch_gesture['total_curvature'], 
            pitch_gesture['smoothness'],
            rhythm_gesture['ioi_stability'],
            rhythm_gesture['syncopation_score'],
            dynamic_gesture['dynamic_range'],
            dynamic_gesture['attack_slope']
        ])
        
        feature_vector = np.nan_to_num(feature_vector, nan=0.5)
        
        self.logger.log(f"  ↳ Gestural signature: {len(feature_vector)}D vector")
        return feature_vector

    def _get_neutral_gesture(self) -> dict:
        """Returns neutral values for when analysis fails."""
        return {
            'avg_velocity': 0.5,
            'total_curvature': 0.5,
            'extrema_count': 5,
            'smoothness': 0.5,
            'contour_length': 100
        }

    def get_gestural_archetypes(self) -> dict:
        """Returns common gestural archetypes for reference."""
        return {
            'lyrical': {'velocity': 0.3, 'curvature': 0.8, 'smoothness': 0.9},
            'energetic': {'velocity': 0.8, 'curvature': 0.6, 'smoothness': 0.7},
            'static': {'velocity': 0.1, 'curvature': 0.2, 'smoothness': 0.95},
            'erratic': {'velocity': 0.9, 'curvature': 0.9, 'smoothness': 0.3}
        }
</div>
</div>

<!-- MODULE: analysis_classical_harmony.py -->
<div class="module" id="analysis_classical">
<h3>analysis_classical_harmony.py - Classical Harmony</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# analysis_classical_harmony.py: Tri-Resonance Synthesis Engine - Classical Harmony Analyzer
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# A new module to analyze music based on the principles of classical harmony,
# specifically Rameau's "Fundamental Bass" and traditional voice leading.
# ==============================================================================

import numpy as np
import librosa
from scipy.signal import find_peaks
from invariants import Invariants

class ClassicalHarmonyAnalyzer:
    """
    Analyzes music according to the principles of Rameau's "Treatise on Harmony".
    Focuses on fundamental bass, chord progressions, and voice leading.
    """
    def __init__(self, logger, invariants: Invariants):
        self.logger = logger
        self.invariants = invariants
        self.logger.log("ClassicalHarmonyAnalyzer initialized - guardian of traditional harmony.")
        
        # Western diatonic scale degrees (0=C, 2=D, 4=E, 5=F, 7=G, 9=A, 11=B)
        self.DIATONIC_DEGREES = {0, 2, 4, 5, 7, 9, 11}
        
        # Common chord types and their intervals (in semitones from root)
        self.CHORD_TYPES = {
            'major': [0, 4, 7],
            'minor': [0, 3, 7], 
            'diminished': [0, 3, 6],
            'augmented': [0, 4, 8],
            'major_7th': [0, 4, 7, 11],
            'minor_7th': [0, 3, 7, 10],
            'dominant_7th': [0, 4, 7, 10]
        }

    def extract_chroma_features(self, y: np.ndarray, sr: int) -> np.ndarray:
        """
        Extracts chroma features which represent the energy in each pitch class.
        This is the foundation for harmonic analysis.
        """
        try:
            # Compute chromagram with CQT for better frequency resolution
            chroma = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=512)
            return chroma
        except Exception as e:
            self.logger.log(f"❌ Error computing chroma features: {e}")
            return np.zeros((12, 10))  # Fallback empty chromagram

    def find_chord_sequence(self, chroma: np.ndarray) -> list:
        """
        Identifies a sequence of chords from chroma features.
        Returns list of (root, chord_type, confidence) tuples.
        """
        if chroma.shape[1] == 0:
            return []
            
        chords = []
        for t in range(chroma.shape[1]):
            frame = chroma[:, t]
            
            # Find most likely root and chord type
            best_root = 0
            best_type = 'major'
            best_score = 0.0
            
            for root in range(12):
                for chord_name, intervals in self.CHORD_TYPES.items():
                    # Score based on how well the chord template matches
                    score = 0.0
                    for interval in intervals:
                        pitch_class = (root + interval) % 12
                        score += frame[pitch_class]
                        
                    # Normalize by chord size
                    score /= len(intervals)
                    
                    if score > best_score:
                        best_score = score
                        best_root = root
                        best_type = chord_name
                        
            chords.append((best_root, best_type, best_score))
            
        return chords

    def calculate_fundamental_bass_consonance(self, vector: np.ndarray) -> float:
        """
        A sophisticated proxy for Rameau-style analysis.
        High scores for vectors that imply strong fundamental bass motion
        and traditional harmonic progressions.
        """
        if vector.size < 2:
            return 0.5  # Neutral score for trivial vectors
        
        self.logger.log("🎻 Analyzing classical harmonic consonance...")
        
        # Treat the vector as a sequence of "chord roots" or harmonic centers
        # Scale to 12-note chromatic space
        harmonic_centers = (vector * 6) % 12  # Map to 0-11 range
        
        consonance_score = 0.0
        progression_strength = 0.0
        voice_leading_smoothness = 0.0
        
        if len(harmonic_centers) >= 2:
            # Analyze interval relationships between successive harmonic centers
            intervals = []
            for i in range(1, len(harmonic_centers)):
                interval = (harmonic_centers[i] - harmonic_centers[i-1]) % 12
                intervals.append(min(interval, 12 - interval))  # Get smallest interval
                
            # Consonant intervals in order of consonance (perfect 5th, perfect 4th, major 3rd, etc.)
            consonant_intervals = [7, 5, 4, 3, 9, 8]  # P5, P4, M3, m3, M6, m6
            
            for interval in intervals:
                # Find closest consonant interval
                distances = [abs(interval - cons) for cons in consonant_intervals]
                min_distance = min(distances)
                
                # Higher score for smaller distances to consonance
                interval_score = np.exp(-min_distance * 0.5)
                consonance_score += interval_score
                
            consonance_score /= len(intervals)
            
            # Progression strength: reward strong cadential patterns
            if len(harmonic_centers) >= 3:
                # Look for authentic cadence (V-I) or other strong progressions
                for i in range(2, len(harmonic_centers)):
                    if (harmonic_centers[i-1] == 7 and harmonic_centers[i] == 0):  # G to C
                        progression_strength += 0.3
                    elif (harmonic_centers[i-1] == 2 and harmonic_centers[i] == 0):  # D to C
                        progression_strength += 0.2
                        
                progression_strength = min(progression_strength, 1.0)
                
            # Voice leading smoothness: reward small melodic motions
            voice_leads = np.abs(np.diff(harmonic_centers))
            smooth_motions = np.sum(voice_leads <= 2)  # Steps or small jumps
            voice_leading_smoothness = smooth_motions / len(voice_leads) if voice_leads.size > 0 else 0.5
            
        else:
            consonance_score = 0.5
            progression_strength = 0.5
            voice_leading_smoothness = 0.5
            
        # Combined score with weights
        final_score = (0.5 * consonance_score + 
                      0.3 * progression_strength + 
                      0.2 * voice_leading_smoothness)
        
        self.logger.log(f"  ↳ Classical consonance: {final_score:.3f} " +
                       f"(intervals: {consonance_score:.3f}, " +
                       f"progressions: {progression_strength:.3f}, " +
                       f"voice leading: {voice_leading_smoothness:.3f})")
        
        return float(final_score)

    def analyze_harmonic_rhythm(self, chords: list) -> dict:
        """
        Analyzes the rate and regularity of chord changes (harmonic rhythm).
        """
        if len(chords) < 2:
            return {'change_rate': 0.5, 'regularity': 0.5}
            
        # Extract chord roots and ignore type/confidence for now
        roots = [root for root, _, _ in chords]
        
        # Calculate rate of chord changes
        changes = 0
        for i in range(1, len(roots)):
            if roots[i] != roots[i-1]:
                changes += 1
                
        change_rate = changes / (len(roots) - 1)
        
        # Calculate regularity of changes
        if changes > 0:
            change_indices = [i for i in range(1, len(roots)) if roots[i] != roots[i-1]]
            change_intervals = np.diff(change_indices)
            if len(change_intervals) > 0:
                regularity = 1.0 - (np.std(change_intervals) / np.mean(change_intervals))
            else:
                regularity = 1.0
        else:
            regularity = 1.0  # Very regular if no changes
            
        return {
            'change_rate': float(change_rate),
            'regularity': float(regularity)
        }

    def identify_key(self, chroma: np.ndarray) -> tuple:
        """
        Identifies the most likely musical key from chroma features.
        Returns (key_index, mode, confidence).
        """
        if chroma.shape[1] == 0:
            return (0, 'major', 0.0)
            
        # Average chroma across time
        avg_chroma = np.mean(chroma, axis=1)
        
        # Key profiles for major and minor keys
        # These are typical distributions of pitch class usage in each key
        major_profile = [6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88]
        minor_profile = [6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17]
        
        best_key = 0
        best_mode = 'major'
        best_correlation = 0.0
        
        for key in range(12):
            # Rotate profiles for this key
            major_rotated = np.roll(major_profile, key)
            minor_rotated = np.roll(minor_profile, key)
            
            # Calculate correlation with observed chroma
            major_corr = np.corrcoef(avg_chroma, major_rotated)[0, 1]
            minor_corr = np.corrcoef(avg_chroma, minor_rotated)[0, 1]
            
            if major_corr > best_correlation:
                best_correlation = major_corr
                best_key = key
                best_mode = 'major'
                
            if minor_corr > best_correlation:
                best_correlation = minor_corr
                best_key = key
                best_mode = 'minor'
                
        return (best_key, best_mode, best_correlation)

    def get_scale_degrees(self, key: int, mode: str) -> set:
        """Returns the scale degrees for a given key and mode."""
        if mode == 'major':
            intervals = [0, 2, 4, 5, 7, 9, 11]  # Major scale
        else:  # minor
            intervals = [0, 2, 3, 5, 7, 8, 10]  # Natural minor scale
            
        return {(key + interval) % 12 for interval in intervals}

    def calculate_diatonic_fit(self, chords: list, key: int, mode: str) -> float:
        """
        Calculates how well the chord progression fits the diatonic scale of the key.
        """
        if not chords:
            return 0.5
            
        scale_degrees = self.get_scale_degrees(key, mode)
        
        diatonic_chords = 0
        for root, chord_type, _ in chords:
            if root in scale_degrees:
                # Simple check: major chords on I, IV, V; minor on ii, iii, vi
                if ((chord_type == 'major' and root in [key, (key+5)%12, (key+7)%12]) or
                    (chord_type == 'minor' and root in [(key+2)%12, (key+4)%12, (key+9)%12])):
                    diatonic_chords += 1
                else:
                    # Still give partial credit for being in scale
                    diatonic_chords += 0.5
                    
        return diatonic_chords / len(chords)
</div>
</div>

<!-- MODULE: synthesis_adaptive_calculus.py -->
<div class="module" id="synthesis_adaptive">
<h3>synthesis_adaptive_calculus.py - Adaptive Calculus</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# synthesis_adaptive_calculus.py: Tri-Resonance Synthesis Engine - Adaptive Calculus Synthesis Core
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# Implements the Ψ–Gradient Flow using an Adam optimizer to find a new,
# high-harmony state vector on the manifold.
# ==============================================================================

import numpy as np
from harmony_functional import HarmonyFunctional
from invariants import Invariants

class AdaptiveCalculus:
    """
    Implements the Ψ–Gradient Flow for synthesizing new states.
    This is the core optimization engine that navigates the musical manifold
    to find beautiful new compositions.
    """
    def __init__(self, logger, harmony_engine: HarmonyFunctional):
        self.logger = logger
        self.harmony_engine = harmony_engine
        self.invariants = Invariants()
        self.logger.log("AdaptiveCalculus (Ψ–Gradient Flow Engine) initialized - navigating musical beauty.")

    def _calculate_gradient(self, state: np.ndarray, sources: list, h=1e-5) -> np.ndarray:
        """Numerically approximates the gradient of the Harmony Functional."""
        grad = np.full(state.shape, self.invariants.IANNOTTI_INVARIANT_ZERO)
        
        for i in range(len(state)):
            # Create perturbed states
            state_p, state_m = state.copy(), state.copy()
            state_p[i] += h
            state_m[i] -= h
            
            # Calculate harmony at perturbed points
            h_p = self.harmony_engine.calculate(state_p, sources)
            h_m = self.harmony_engine.calculate(state_m, sources)
            
            # Finite difference gradient
            grad[i] = (h_p - h_m) / (2 * h)
            
        return grad

    def run_synthesis(self, inputs: list, s_type: str, n_steps=200, lr_init=0.1, lr_final=0.001) -> tuple:
        """
        Executes the Ψ–Gradient Flow using an Adam optimizer.
        Finds a high-harmony state vector through gradient ascent.
        """
        self.logger.start_section(f"Ψ–GRADIENT FLOW SYNTHESIS: Type='{s_type}'")
        
        # Filter valid inputs
        valid_inputs = [i for i in inputs if i is not None and i.size > 0]
        if not valid_inputs:
            self.logger.log("❌ No valid inputs; cannot start synthesis.")
            self.logger.end_section()
            return np.array([]), []

        # Initialize state as mean of inputs
        dim = len(valid_inputs[0])
        current_state = np.mean(valid_inputs, axis=0)
            
        # Calculate initial harmony
        initial_h = self.harmony_engine.calculate(current_state, valid_inputs)
        self.logger.log(f"🎯 Starting gradient ascent: {n_steps} steps, Initial Harmony: {initial_h:.6f}")
        
        # Store evolution frames for visualization
        frames = [current_state.copy()]
        
        # Adam optimizer parameters
        b1, b2, eps = 0.9, 0.999, 1e-8
        m = np.full(dim, self.invariants.IANNOTTI_INVARIANT_ZERO)  # First moment
        v = np.full(dim, self.invariants.IANNOTTI_INVARIANT_ZERO)  # Second moment

        # Main optimization loop
        for step in range(1, n_steps + 1):
            # Calculate gradient
            grad = self._calculate_gradient(current_state, valid_inputs)
            
            # Check for invalid gradient
            if not np.all(np.isfinite(grad)):
                self.logger.log(f"⚠️  Invalid gradient at step {step}. Halting optimization.")
                break
            
            # Cosine annealing learning rate
            lr = lr_final + 0.5 * (lr_init - lr_final) * (1 + np.cos((step / n_steps) * np.pi))
            
            # Adam update
            m = b1 * m + (1 - b1) * grad
            v = b2 * v + (1 - b2) * (grad**2)
            
            # Bias correction
            m_hat = m / (1 - b1**step)
            v_hat = v / (1 - b2**step)
            
            # Parameter update
            update = lr * m_hat / (np.sqrt(v_hat) + eps)
            current_state += update

            # Maintain reasonable vector scale
            norm = np.linalg.norm(current_state)
            if norm > self.invariants.MAX_VECTOR_NORM:
                current_state = (current_state / norm) * self.invariants.MAX_VECTOR_NORM

            # Logging and frame capture
            if step % 20 == 0 or step == n_steps:
                h = self.harmony_engine.calculate(current_state, valid_inputs)
                self.logger.log(f"📈 Step {step:03d}/{n_steps} | Harmony: {h:9.6f} | LR: {lr:.5f}")
                
            if step % 10 == 0:
                frames.append(current_state.copy())
            
        # Final evaluation
        final_h = self.harmony_engine.calculate(current_state, valid_inputs)
        improvement = final_h - initial_h
        
        self.logger.log(f"✅ Synthesis complete. Final Harmony: {final_h:.6f} " +
                       f"(Δ: {improvement:+.6f})", header=True)
        self.logger.end_section()
        
        return current_state, frames

    def run_multi_start_synthesis(self, inputs: list, s_type: str, num_starts: int = 5, 
                                n_steps: int = 100) -> tuple:
        """
        Runs synthesis from multiple starting points and returns the best result.
        Helps avoid local optima in the harmony landscape.
        """
        self.logger.start_section(f"MULTI-START SYNTHESIS: {num_starts} starting points")
        
        best_state = None
        best_harmony = -float('inf')
        best_frames = []
        
        for i in range(num_starts):
            self.logger.log(f"🔁 Starting point {i+1}/{num_starts}")
            
            # Add small random perturbation to inputs for diversity
            perturbed_inputs = []
            for inp in inputs:
                if inp is not None and inp.size > 0:
                    perturbation = np.random.normal(0, 0.1, inp.shape)
                    perturbed = inp + perturbation
                    perturbed_inputs.append(perturbed)
                else:
                    perturbed_inputs.append(inp)
                    
            # Run synthesis from this starting point
            state, frames = self.run_synthesis(perturbed_inputs, s_type, n_steps=n_steps)
            harmony = self.harmony_engine.calculate(state, inputs) if state.size > 0 else -1e6
            
            if harmony > best_harmony:
                best_harmony = harmony
                best_state = state
                best_frames = frames
                
            self.logger.log(f"  ↳ Result harmony: {harmony:.6f}")
            
        self.logger.log(f"🏆 Best result: {best_harmony:.6f}")
        self.logger.end_section()
        
        return best_state, best_frames

    def calculate_harmony_landscape(self, center: np.ndarray, sources: list, 
                                  radius: float = 1.0, resolution: int = 20) -> np.ndarray:
        """
        Samples the harmony landscape around a center point.
        Useful for understanding the local geometry of the musical manifold.
        """
        if center.size < 2:
            return np.array([])
            
        self.logger.log(f"🗺️  Sampling harmony landscape around center (radius: {radius})")
        
        # Sample in 2D slice for visualization
        landscape = np.zeros((resolution, resolution))
        
        # Choose two random directions for the slice
        dir1 = np.random.randn(center.size)
        dir2 = np.random.randn(center.size)
        
        # Orthogonalize
        dir2 = dir2 - np.dot(dir1, dir2) * dir1 / np.dot(dir1, dir1)
        
        # Normalize
        dir1 = dir1 / np.linalg.norm(dir1)
        dir2 = dir2 / np.linalg.norm(dir2)
        
        # Sample harmony values
        for i in range(resolution):
            for j in range(resolution):
                # Map grid coordinates to vector space
                x = -radius + (2 * radius * i) / (resolution - 1)
                y = -radius + (2 * radius * j) / (resolution - 1)
                
                sample_point = center + x * dir1 + y * dir2
                harmony = self.harmony_engine.calculate(sample_point, sources)
                landscape[i, j] = harmony
                
        return landscape

    def find_harmony_basin_center(self, inputs: list, s_type: str, 
                                num_iterations: int = 3) -> np.ndarray:
        """
        Finds the center of a high-harmony basin by running multiple
        synthesis processes and averaging the results.
        """
        self.logger.start_section("FINDING HARMONY BASIN CENTER")
        
        results = []
        harmonies = []
        
        for i in range(num_iterations):
            self.logger.log(f"🔄 Iteration {i+1}/{num_iterations}")
            state, _ = self.run_synthesis(inputs, s_type, n_steps=150)
            
            if state.size > 0:
                harmony = self.harmony_engine.calculate(state, inputs)
                results.append(state)
                harmonies.append(harmony)
                self.logger.log(f"  ↳ Harmony: {harmony:.6f}")
                
        if not results:
            self.logger.log("❌ No successful synthesis iterations")
            self.logger.end_section()
            return np.array([])
            
        # Weighted average by harmony scores
        weights = np.array(harmonies)
        weights = weights - np.min(weights)  # Make all positive
        weights = weights / (np.sum(weights) + 1e-9)
        
        center = np.zeros_like(results[0])
        for state, weight in zip(results, weights):
            center += weight * state
            
        final_harmony = self.harmony_engine.calculate(center, inputs)
        self.logger.log(f"🎯 Basin center harmony: {final_harmony:.6f}")
        self.logger.end_section()
        
        return center
</div>
</div>

<!-- MODULE: synthesis_generative_model.py -->
<div class="module" id="synthesis_generative">
<h3>synthesis_generative_model.py - Generative Model</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# synthesis_generative_model.py: Tri-Resonance Synthesis Engine - Generative Synthesis Model
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# This module translates a Denotator object into an audio waveform using
# a hybrid of additive and procedural synthesis.
# ==============================================================================

import numpy as np
import librosa
from invariants import Invariants
from synthesis_procedural_textures import ProceduralTextureSynthesizer
from musical_structures import Denotator
from physics_model import PhysicsModel

class GenerativeModel:
    """
    Translates a Denotator's state vector into an audio waveform.
    This is where abstract mathematical states become concrete sound.
    """
    def __init__(self, logger, invariants: Invariants, texture_synthesizer: ProceduralTextureSynthesizer, physics_model: PhysicsModel):
        self.logger = logger
        self.invariants = invariants
        self.texture_synthesizer = texture_synthesizer
        self.physics_model = physics_model
        self.sr = 44100
        self.logger.log("GenerativeModel initialized - bringing mathematical states to life.")

    def _map_vector_to_params(self, vector: np.ndarray, duration: int) -> dict:
        """
        Maps the state vector to a rich set of musical parameters.
        This is the crucial translation from abstract space to concrete sound.
        """
        if vector is None or vector.size == 0: 
            vector = np.full(20, self.invariants.IANNOTTI_INVARIANT_ZERO)
        
        def get_val(idx, default=0.0):
            return vector[idx] if idx < len(vector) else default

        # Start with duration
        p = { 'duration_seconds': duration }
        
        # --- CORE PTF DIMENSIONS (first 5) for macro structure ---
        p['harmonic_complexity'] = int(np.interp(get_val(0), [0, 20], [5, 40]))
        p['thematic_variation'] = 1.0 - np.clip(get_val(1), 0, 1)
        p['dynamic_range'] = np.interp(get_val(2), [0, 2], [0.2, 0.95])
        p['base_hz'] = np.interp(get_val(3), 
                               [-self.invariants.PRIME_ARCHETYPE_COUNT, self.invariants.PRIME_ARCHETYPE_COUNT], 
                               [60.0, 500.0])
        p['rhythmic_density'] = np.interp(get_val(4), [-1, 1], [0.05, 0.95])
        
        # --- SENTIMENT DIMENSIONS (next 5) for performance qualities ---
        p['timbral_richness'] = np.clip(get_val(5), 0.1, 1.0)
        p['vibrato_rate'] = np.interp(get_val(6), [0, 1], [0.5, 8.0])
        p['tempo_bpm'] = np.interp(get_val(7), [0, 1.5], [40, 180])
        p['mood_bias'] = np.clip(get_val(8), -1, 1)  # -1=dark/minor, +1=bright/major
        p['vibrato_depth'] = np.interp(get_val(9), [-1, 1], [0.0, 0.025])
        
        # --- BOUNDED FEATURE DIMENSIONS for esoteric controls ---
        p['reverb_wetness'] = np.interp(get_val(10, 0.3), [0, 1], [0.1, 0.8])
        p['attack_time_ms'] = np.interp(get_val(11, 0.2), [0, 1], [5, 150])
        p['detune_amount'] = np.interp(get_val(12, 0.1), [0, 1], [0.0, 0.02])
        p['noise_balance'] = np.interp(get_val(13, 0.2), [0, 1], [0.0, 0.3])
        
        # Ensure physical plausibility
        p['base_hz'] = np.clip(p['base_hz'], 20.0, 5000.0)
        
        self.logger.log(f"  🎛️  Mapped to parameters: {p['harmonic_complexity']} partials, " +
                       f"{p['base_hz']:.1f}Hz, {p['tempo_bpm']:.0f}BPM, " +
                       f"mood={p['mood_bias']:.2f}")
        
        return p

    def _synthesize_additive_core(self, params: dict, sr: int) -> np.ndarray:
        """
        Generates the fundamental harmonic structure using additive synthesis.
        Based on Rameau's corps sonore - the physical reality of vibrating strings.
        """
        duration = int(params["duration_seconds"] * sr)
        t = np.linspace(0, params["duration_seconds"], duration, endpoint=False)
        
        # Get harmonic series based on mood
        # Dark mood uses more complex ratios, bright mood uses simpler integer ratios
        zeta_ratios = self.invariants.RIEMANN_ZETA_ZEROS / self.invariants.RIEMANN_ZETA_ZEROS[0]
        int_ratios = np.arange(1, len(zeta_ratios) + 1)
        
        # Blend between complex and simple ratios based on mood
        mood = (params["mood_bias"] + 1) / 2  # Map from [-1,1] to [0,1]
        ratios = (1 - mood) * zeta_ratios + mood * int_ratios
        
        # Apply Pythagorean comma for just intonation flavor
        ratios = ratios * self.invariants.PYTHAGOREAN_COMMA
        
        num_partials = min(params["harmonic_complexity"], len(ratios))
        waveform = np.full(duration, self.invariants.IANNOTTI_INVARIANT_ZERO, dtype=float)
        
        self.logger.log(f"  🎻 Generating {num_partials} harmonic partials...")
        
        for i in range(num_partials):
            freq = params["base_hz"] * ratios[i]
            
            # Skip if beyond Nyquist
            if freq > sr / 2: 
                continue

            # Amplitude envelope with harmonic roll-off and variation
            base_amp = 1.0 / (i + 1.0)**0.8  # Slightly gentler roll-off than natural
            variation = 0.5 + 0.5 * np.sin(2 * np.pi * t / (params['duration_seconds'] / (i+1)))
            amp = base_amp * variation
            
            # Vibrato for naturalness
            vibrato = 1 + params["vibrato_depth"] * np.sin(2 * np.pi * params["vibrato_rate"] * t)
            
            # Small detune for richness
            detune = 1 + params["detune_amount"] * np.sin(2 * np.pi * 0.7 * t)
            
            # Add this partial to the waveform
            partial = amp * np.sin(2 * np.pi * freq * vibrato * detune * t)
            waveform += partial

        # Apply amplitude envelope
        attack_len = int(sr * params['attack_time_ms'] / 1000)
        decay_len = max(1, duration - attack_len)
        
        if attack_len > 0:
            envelope = np.concatenate([
                np.linspace(0, 1, attack_len),
                np.linspace(1, 0, decay_len)
            ])
        else:
            envelope = np.linspace(1, 0, duration)
            
        waveform *= envelope
        
        return librosa.util.normalize(waveform)

    def render(self, denotator: Denotator, duration: int = 45) -> tuple:
        """
        Renders a final audio waveform from a Denotator object.
        This is the culmination of the entire synthesis process.
        """
        self.logger.start_section(f"RENDERING DENOTATOR: '{denotator.name}'")
        
        # Map vector to synthesis parameters
        params = self._map_vector_to_params(denotator.vector, duration)
        
        # Generate core harmonic structure
        additive_core = self._synthesize_additive_core(params, self.sr)
        
        # Add procedural textures
        grain_texture = self.texture_synthesizer.synthesize_granular_texture(additive_core, params, self.sr)
        noise_texture = self.texture_synthesizer.synthesize_filtered_noise(params, self.sr)
        
        # Mix components
        final = (0.6 * additive_core + 
                 0.3 * params['timbral_richness'] * grain_texture +
                 0.1 * params['noise_balance'] * noise_texture)
        
        # Apply reverb if requested
        if params['reverb_wetness'] > 0:
            self.logger.log(f"  🌊 Applying reverb (wetness: {params['reverb_wetness']:.2f})")
            impulse_len = int(self.sr * 1.5)
            impulse_response = np.random.randn(impulse_len) * np.exp(-np.arange(impulse_len) / (self.sr * 0.3))
            impulse_response = librosa.util.normalize(impulse_response)
            reverb = np.convolve(final, impulse_response, 'same')
            final = (1 - params['reverb_wetness']) * final + params['reverb_wetness'] * reverb

        # Final normalization
        final = librosa.util.normalize(final)
        
        self.logger.log(f"  ✅ Rendered {duration}s audio at {self.sr}Hz")
        self.logger.end_section()
        
        return final, self.sr

    def render_evolution(self, frames: list, duration: int = 30) -> tuple:
        """
        Renders an audio representation of the evolution process.
        Each frame becomes a segment of the final audio.
        """
        if not frames or len(frames) < 2:
            return self.render(Denotator("evolution_fallback", self.logger, self.invariants, None), duration)
            
        self.logger.start_section("RENDERING EVOLUTION SEQUENCE")
        
        segment_duration = duration / len(frames)
        all_segments = []
        
        for i, frame in enumerate(frames):
            self.logger.log(f"  🎞️  Rendering frame {i+1}/{len(frames)}")
            
            # Create temporary denotator for this frame
            frame_denotator = Denotator(f"frame_{i}", self.logger, self.invariants, None, frame)
            segment_audio, _ = self.render(frame_denotator, segment_duration)
            all_segments.append(segment_audio)
            
        # Crossfade between segments for smooth evolution
        final_audio = self._crossfade_segments(all_segments, self.sr)
        final_audio = librosa.util.normalize(final_audio)
        
        self.logger.log(f"  ✅ Rendered evolution sequence: {len(frames)} frames")
        self.logger.end_section()
        
        return final_audio, self.sr

    def _crossfade_segments(self, segments: list, sr: int, crossfade_duration: float = 0.1) -> np.ndarray:
        """Applies crossfades between audio segments for smooth transitions."""
        if not segments:
            return np.array([])
            
        crossfade_samples = int(sr * crossfade_duration)
        result = segments[0]
        
        for i in range(1, len(segments)):
            current_segment = segments[i]
            
            # Ensure segments are long enough for crossfade
            if len(result) < crossfade_samples or len(current_segment) < crossfade_samples:
                result = np.concatenate([result, current_segment])
                continue
                
            # Apply crossfade
            fade_out = np.linspace(1, 0, crossfade_samples)
            fade_in = np.linspace(0, 1, crossfade_samples)
            
            # Crossfade the end of previous segment with start of current segment
            result[-crossfade_samples:] = (result[-crossfade_samples:] * fade_out + 
                                         current_segment[:crossfade_samples] * fade_in)
            
            # Append the rest of the current segment
            result = np.concatenate([result, current_segment[crossfade_samples:]])
            
        return result
</div>
</div>

<!-- MODULE: synthesis_procedural_textures.py -->
<div class="module" id="synthesis_textures">
<h3>synthesis_procedural_textures.py - Procedural Textures</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# synthesis_procedural_textures.py: Tri-Resonance Synthesis Engine - Procedural Textures
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# A dedicated module for generating rich, procedural audio textures using
# techniques like granular synthesis and filtered noise.
# ==============================================================================

import numpy as np
import librosa
from scipy.signal import butter, lfilter, firwin
from scipy.ndimage import gaussian_filter1d
from invariants import Invariants
from humanization_psychoacoustics import PsychoacousticModel

class ProceduralTextureSynthesizer:
    """
    Generates rich, evolving textures using granular and noise synthesis.
    These textures add depth, warmth, and complexity to the core harmonic structure.
    """
    def __init__(self, logger, invariants: Invariants, psychoacoustics: PsychoacousticModel):
        self.logger = logger
        self.invariants = invariants
        self.psychoacoustics = psychoacoustics
        self.logger.log("ProceduralTextureSynthesizer initialized - crafting sonic textures.")

    def synthesize_granular_texture(self, source: np.ndarray, params: dict, sr: int) -> np.ndarray:
        """
        Generates a texture using granular synthesis from a source waveform.
        Granular synthesis creates complex textures by rearranging small pieces of sound.
        """
        duration = len(source)
        texture = np.full(duration, self.invariants.IANNOTTI_INVARIANT_ZERO, dtype=float)
        
        # Calculate grain parameters based on timbral richness
        grain_dur_ms = np.interp(params.get("timbral_richness", 0.5), [0, 1], [20, 300])
        grain_dur_samps = int(sr * grain_dur_ms / 1000)
        
        # Number of grains based on rhythmic density
        num_grains = int(params.get("rhythmic_density", 0.5) * (params["duration_seconds"] * 40))
        
        if grain_dur_samps <= 1 or num_grains == 0:
            self.logger.log("⚠️  Grain parameters too small for granular synthesis")
            return texture

        self.logger.log(f"  🧊 Generating {num_grains} grains ({grain_dur_ms:.0f}ms each)")
        
        for i in range(num_grains):
            # Random grain position in source
            start = np.random.randint(0, duration - grain_dur_samps)
            grain = source[start : start + grain_dur_samps].copy()
            
            # Apply windowing to avoid clicks
            window = np.hanning(len(grain))
            grain *= window
            
            # Random playback speed variation (within reasonable bounds)
            speed_variation = 0.8 + 0.4 * np.random.random()
            if speed_variation != 1.0:
                grain = librosa.effects.time_stretch(grain, rate=speed_variation)
                # Ensure grain doesn't become too long
                if len(grain) > grain_dur_samps * 1.5:
                    grain = grain[:grain_dur_samps]
            
            # Random amplitude
            amp = 0.3 + 0.7 * np.random.random()
            grain *= amp
            
            # Random position in output
            output_pos = np.random.randint(0, duration - len(grain))
            
            # Additive mixing (can create dense textures)
            texture[output_pos : output_pos + len(grain)] += grain

        # Normalize to prevent clipping
        texture = librosa.util.normalize(texture)
        
        return texture

    def synthesize_filtered_noise(self, params: dict, sr: int) -> np.ndarray:
        """
        Generates a "colored" noise texture by filtering white noise.
        Different colors of noise have different spectral characteristics.
        """
        duration = int(params["duration_seconds"] * sr)
        
        # Generate white noise
        noise = np.random.normal(0, 0.2, duration)
        
        # Choose noise color based on mood
        mood = params.get("mood_bias", 0.0)
        
        if mood < -0.5:
            # Dark mood -> brown noise (more bass)
            colored_noise = self._make_brown_noise(noise)
        elif mood < 0:
            # Neutral-dark -> pink noise (natural)
            colored_noise = self._make_pink_noise(noise, sr)
        elif mood < 0.5:
            # Neutral-bright -> white noise (flat spectrum)
            colored_noise = noise
        else:
            # Bright mood -> blue noise (more treble)
            colored_noise = self._make_blue_noise(noise, sr)
            
        # Additional filtering based on mood
        if mood < 0:
            btype = 'lowpass'
            cutoff = np.interp(mood, [-1, 0], [200, 2000])
        else:
            btype = 'highpass'
            cutoff = np.interp(mood, [0, 1], [500, 5000])
        
        # Apply filter
        colored_noise = self._apply_filter(colored_noise, sr, btype, cutoff)
        
        self.logger.log(f"  🎨 Generated {btype} filtered noise (cutoff: {cutoff:.0f}Hz)")
        return librosa.util.normalize(colored_noise)

    def _make_pink_noise(self, white_noise: np.ndarray, sr: int) -> np.ndarray:
        """
        Generates pink noise (1/f noise) by filtering white noise.
        Pink noise has equal energy per octave, sounds natural.
        """
        # Simple IIR filter approximation of pink noise
        b = [0.049922035, -0.095993537, 0.050612699, -0.004408786]
        a = [1, -2.494956002, 2.017265875, -0.522189400]
        
        pink = lfilter(b, a, white_noise)
        return pink

    def _make_brown_noise(self, white_noise: np.ndarray) -> np.ndarray:
        """
        Generates brown noise (1/f² noise) by integrating white noise.
        Brown noise has even more bass than pink noise.
        """
        brown = np.cumsum(white_noise)
        return brown

    def _make_blue_noise(self, white_noise: np.ndarray, sr: int) -> np.ndarray:
        """
        Generates blue noise (f noise) by differentiating white noise.
        Blue noise has more high frequencies.
        """
        blue = np.diff(white_noise, prepend=0)
        return blue

    def _apply_filter(self, signal: np.ndarray, sr: int, btype: str, cutoff: float) -> np.ndarray:
        """Applies a Butterworth filter to the signal."""
        nyquist = 0.5 * sr
        normal_cutoff = np.clip(cutoff / nyquist, 0.001, 0.999)
        
        try:
            b, a = butter(4, normal_cutoff, btype=btype, analog=False)
            filtered = lfilter(b, a, signal)
            return filtered
        except Exception as e:
            self.logger.log(f"⚠️  Filter failed: {e}, returning original signal")
            return signal

    def synthesize_waveset_texture(self, source: np.ndarray, params: dict, sr: int) -> np.ndarray:
        """
        Generates texture using waveset synthesis - rearranging zero-crossing segments.
        Creates rhythmic and textural variations from the source material.
        """
        if len(source) < sr * 0.1:  # Need reasonable length
            return np.zeros_like(source)
            
        # Find zero crossings
        zero_crossings = np.where(np.diff(np.signbit(source)))[0]
        
        if len(zero_crossings) < 10:
            return np.zeros_like(source)
            
        # Create wavesets (segments between zero crossings)
        wavesets = []
        for i in range(len(zero_crossings) - 1):
            start = zero_crossings[i]
            end = zero_crossings[i + 1]
            waveset = source[start:end]
            wavesets.append(waveset)
            
        # Rearrange wavesets based on rhythmic density
        texture = np.zeros_like(source)
        pos = 0
        
        for i in range(len(wavesets)):
            # Sometimes skip wavesets for rhythmic variation
            if np.random.random() < params.get("rhythmic_density", 0.5):
                waveset = wavesets[i]
                
                # Sometimes reverse waveset
                if np.random.random() < 0.3:
                    waveset = waveset[::-1]
                    
                # Copy to output
                if pos + len(waveset) < len(texture):
                    texture[pos:pos + len(waveset)] = waveset
                    pos += len(waveset)
                    
        return librosa.util.normalize(texture)

    def synthesize_modal_texture(self, frequencies: np.ndarray, decays: np.ndarray, 
                               duration: float, sr: int) -> np.ndarray:
        """
        Generates texture using modal synthesis - exciting resonant filters.
        Good for bell-like, metallic, or wooden sounds.
        """
        t = np.linspace(0, duration, int(sr * duration), endpoint=False)
        texture = np.zeros_like(t)
        
        for freq, decay in zip(frequencies, decays):
            if freq > sr / 2:
                continue
                
            # Generate decaying sinusoid for this mode
            envelope = np.exp(-decay * t)
            oscillator = np.sin(2 * np.pi * freq * t)
            mode = envelope * oscillator
            
            # Random phase and amplitude
            phase_shift = np.random.random() * 2 * np.pi
            amp = 0.5 + 0.5 * np.random.random()
            
            texture += amp * np.sin(2 * np.pi * freq * t + phase_shift) * envelope
            
        return librosa.util.normalize(texture)
</div>
</div>

<!-- MODULE: synthesis_geometric_transforms.py -->
<div class="module" id="synthesis_geometric">
<h3>synthesis_geometric_transforms.py - Geometric Transforms</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# synthesis_geometric_transforms.py: Tri-Resonance Synthesis Engine - Geometric Transformation Engine
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# Applies advanced geometric transformations like Homography to Denotator objects.
# Creates sophisticated variations by changing the geometric perspective on musical space.
# ==============================================================================

import numpy as np
from invariants import Invariants
from advanced_geometry import AdvancedGeometry
from musical_structures import Denotator

class GeometricTransformer:
    """
    Orchestrates the application of geometric transformations to Denotators.
    These transformations create variations that preserve deep structural relationships
    while exploring new regions of musical space.
    """
    def __init__(self, logger, advanced_geometry: AdvancedGeometry):
        self.logger = logger
        self.invariants = Invariants()
        self.advanced_geometry = advanced_geometry
        self.logger.log("GeometricTransformer initialized - exploring musical space through geometry.")

    def apply_homographic_variation(self, denotator: Denotator, intensity: float = 0.5) -> Denotator:
        """
        Applies Homography to a Denotator's state vector.
        Homography changes the 'perspective' on the musical space, creating
        sophisticated variations that preserve certain geometric relationships.
        """
        if denotator.vector is None or denotator.vector.size == 0:
            return denotator
            
        self.logger.log(f"🔄 Applying homographic variation (intensity: {intensity:.3f})")

        # Extract musical properties for content-aware transformation
        music_properties = denotator.get_gestural_properties()
        
        intensity = np.clip(intensity, 0.0, 1.0)
        
        # Apply the homography transformation
        transformed_vector = self.advanced_geometry.apply_homography_transformation(
            denotator.vector, music_properties)

        # Blend with original based on intensity
        final_vector = (1.0 - intensity) * denotator.vector + intensity * transformed_vector
        
        # Ensure result is valid
        if not np.all(np.isfinite(final_vector)):
            self.logger.log("⚠️  Homography resulted in non-finite values. Reverting.")
            final_vector = denotator.vector

        # Create a new Denotator for the transformed state
        new_name = f"{denotator.name}_homography_{intensity:.2f}"
        transformed_denotator = Denotator(new_name, denotator.logger, denotator.invariants, 
                                        denotator.harmony_engine, initial_vector=final_vector)
        
        # Copy relevant fingerprints and metadata
        for fp_type, fp_data in denotator.fingerprints.items():
            if fp_data.size > 0:
                transformed_denotator.add_fingerprint(fp_type, fp_data)
                
        transformed_denotator.metadata.update(denotator.metadata)
        transformed_denotator.metadata['transformation'] = f'homography_{intensity:.2f}'
        
        return transformed_denotator

    def create_chaotic_perturbation(self, denotator: Denotator, chaos_factor: float = 0.02) -> Denotator:
        """
        Applies a small, bounded chaotic perturbation to a Denotator's vector.
        Chaotic perturbations can help escape local optima and discover new regions.
        """
        vector = denotator.vector
        if vector is None or vector.size == 0:
            return denotator
        
        self.logger.log(f"🌀 Applying chaotic perturbation (chaos: {chaos_factor:.3f})")
        
        # Use logistic map for deterministic chaos
        r = 3.9  # Chaotic regime
        seed = (np.linalg.norm(vector) + np.sum(vector)) % 1.0
        seed = max(seed, 0.01)
        
        perturbation = np.full(vector.shape, self.invariants.IANNOTTI_INVARIANT_ZERO, dtype=float)
        x = seed
        
        for i in range(len(vector)):
            x = r * x * (1.0 - x)  # Logistic map
            perturbation[i] = (x - 0.5) * 2.0  # Map to [-1, 1]
            
        # Scale perturbation by chaos factor and vector norm
        perturbation *= chaos_factor * (np.linalg.norm(vector) + self.invariants.IANNOTTI_INVARIANT_ZERO)
        perturbed_vector = vector + perturbation

        # Ensure result is valid
        if not np.all(np.isfinite(perturbed_vector)):
            self.logger.log("⚠️  Chaotic perturbation resulted in non-finite values. Reverting.")
            perturbed_vector = vector

        # Create new Denotator
        new_name = f"{denotator.name}_chaos_{chaos_factor:.3f}"
        perturbed_denotator = Denotator(new_name, denotator.logger, denotator.invariants,
                                      denotator.harmony_engine, initial_vector=perturbed_vector)
        
        # Copy fingerprints and metadata
        for fp_type, fp_data in denotator.fingerprints.items():
            if fp_data.size > 0:
                perturbed_denotator.add_fingerprint(fp_type, fp_data)
                
        perturbed_denotator.metadata.update(denotator.metadata)
        perturbed_denotator.metadata['transformation'] = f'chaos_{chaos_factor:.3f}'
        
        return perturbed_denotator

    def apply_conformal_mapping(self, denotator: Denotator, center: np.ndarray = None,
                              scale: float = 1.0, rotation: float = 0.0) -> Denotator:
        """
        Applies a conformal mapping (angle-preserving transformation) to a Denotator.
        Conformal mappings preserve local structure while allowing global deformation.
        """
        vector = denotator.vector
        if vector is None or vector.size == 0:
            return denotator
            
        self.logger.log(f"📐 Applying conformal mapping (scale: {scale:.2f}, rotation: {rotation:.2f}π)")
        
        # Use provided center or calculate centroid of similar vectors
        if center is None:
            center = np.full_like(vector, 0.5)  # Default center
            
        # Simple conformal mapping: scaling and rotation
        displacement = vector - center
        
        # Apply rotation in 2D subspaces
        if displacement.size >= 2:
            for i in range(0, displacement.size - 1, 2):
                x, y = displacement[i], displacement[i+1]
                # 2D rotation
                cos_theta, sin_theta = np.cos(rotation * np.pi), np.sin(rotation * np.pi)
                displacement[i] = x * cos_theta - y * sin_theta
                displacement[i+1] = x * sin_theta + y * cos_theta
                
        # Apply scaling
        displacement *= scale
        
        mapped_vector = center + displacement
        
        # Create new Denotator
        new_name = f"{denotator.name}_conformal_s{scale:.2f}_r{rotation:.2f}"
        mapped_denotator = Denotator(new_name, denotator.logger, denotator.invariants,
                                   denotator.harmony_engine, initial_vector=mapped_vector)
        
        # Copy fingerprints and metadata
        for fp_type, fp_data in denotator.fingerprints.items():
            if fp_data.size > 0:
                mapped_denotator.add_fingerprint(fp_type, fp_data)
                
        mapped_denotator.metadata.update(denotator.metadata)
        mapped_denotator.metadata['transformation'] = f'conformal_s{scale:.2f}_r{rotation:.2f}'
        
        return mapped_denotator

    def create_geodesic_variation(self, denotator1: Denotator, denotator2: Denotator,
                                interpolation_factor: float = 0.5) -> Denotator:
        """
        Creates a new Denotator by interpolating along the geodesic between two Denotators.
        The geodesic is the shortest path on the musical manifold.
        """
        vec1, vec2 = denotator1.vector, denotator2.vector
        
        if vec1 is None or vec2 is None or vec1.size != vec2.size:
            self.logger.log("⚠️  Cannot compute geodesic: vector size mismatch")
            return denotator1
            
        self.logger.log(f"🛣️  Creating geodesic variation (factor: {interpolation_factor:.2f})")
        
        # Simple linear interpolation for now
        # In a full implementation, this would follow manifold curvature
        t = np.clip(interpolation_factor, 0.0, 1.0)
        interpolated_vector = (1 - t) * vec1 + t * vec2
        
        # Create new Denotator
        new_name = f"geodesic_{denotator1.name}_{denotator2.name}_{t:.2f}"
        geodesic_denotator = Denotator(new_name, denotator1.logger, denotator1.invariants,
                                     denotator1.harmony_engine, initial_vector=interpolated_vector)
        
        # Combine fingerprints (average where possible)
        for fp_type in set(denotator1.fingerprints.keys()) | set(denotator2.fingerprints.keys()):
            fp1 = denotator1.get_fingerprint(fp_type)
            fp2 = denotator2.get_fingerprint(fp_type)
            
            if fp1.size > 0 and fp2.size > 0 and fp1.size == fp2.size:
                combined_fp = (1 - t) * fp1 + t * fp2
                geodesic_denotator.add_fingerprint(fp_type, combined_fp)
            elif fp1.size > 0:
                geodesic_denotator.add_fingerprint(fp_type, fp1)
            elif fp2.size > 0:
                geodesic_denotator.add_fingerprint(fp_type, fp2)
                
        # Combine metadata
        geodesic_denotator.metadata.update({
            'source1': denotator1.name,
            'source2': denotator2.name,
            'interpolation_factor': t,
            'transformation': 'geodesic_interpolation'
        })
        
        return geodesic_denotator

    def apply_systolic_compression(self, denotator: Denotator, compression_factor: float = 0.8) -> Denotator:
        """
        Applies systolic geometry compression to emphasize periodic structure.
        This can reveal hidden rhythmic and harmonic patterns.
        """
        vector = denotator.vector
        if vector is None or vector.size < 20:
            return denotator
            
        self.logger.log(f"💓 Applying systolic compression (factor: {compression_factor:.2f})")
        
        # Calculate systolic ratio
        systolic_ratio = self.advanced_geometry.calculate_systolic_geometry(vector)
        
        # Emphasize the fundamental period
        if systolic_ratio > 0.1:  # Has detectable periodicity
            period = int(systolic_ratio * len(vector))
            
            # Create a template from the fundamental period
            if period > 0 and period < len(vector):
                template = vector[:period]
                
                # Compress by emphasizing the template
                compressed_vector = np.tile(template, int(np.ceil(len(vector) / period)))[:len(vector)]
                
                # Blend with original
                final_vector = (1 - compression_factor) * vector + compression_factor * compressed_vector
            else:
                final_vector = vector
        else:
            final_vector = vector
            
        # Create new Denotator
        new_name = f"{denotator.name}_systolic_{compression_factor:.2f}"
        compressed_denotator = Denotator(new_name, denotator.logger, denotator.invariants,
                                       denotator.harmony_engine, initial_vector=final_vector)
        
        # Copy fingerprints and metadata
        for fp_type, fp_data in denotator.fingerprints.items():
            if fp_data.size > 0:
                compressed_denotator.add_fingerprint(fp_type, fp_data)
                
        compressed_denotator.metadata.update(denotator.metadata)
        compressed_denotator.metadata['transformation'] = f'systolic_{compression_factor:.2f}'
        compressed_denotator.metadata['systolic_ratio'] = systolic_ratio
        
        return compressed_denotator
</div>
</div>

<!-- MODULE: humanization_filters.py -->
<div class="module" id="humanization_filters">
<h3>humanization_filters.py - Humanization Filters</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# humanization_filters.py: Tri-Resonance Synthesis Engine - Psychoacoustic Humanization Filters
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# Implements the final stage of applying filters based on the physics of the
# human ear to eliminate the "uncanny valley" of perfect audio.
# ==============================================================================

import numpy as np
import librosa
from scipy.signal import lfilter, find_peaks, resample
from scipy.ndimage import gaussian_filter1d
from invariants import Invariants
from humanization_psychoacoustics import PsychoacousticModel

class HumanizationFilters:
    """
    Applies filters to a rendered waveform to make it sound more natural.
    These filters simulate the imperfections and characteristics of human performance
    and perception, avoiding the sterile perfection of pure digital synthesis.
    """
    def __init__(self, logger, invariants: Invariants, psychoacoustics: PsychoacousticModel):
        self.logger = logger
        self.invariants = invariants
        self.psychoacoustics = psychoacoustics
        self.logger.log("HumanizationFilters initialized - adding human warmth to digital perfection.")

    def apply_all_filters(self, y: np.ndarray, sr: int) -> np.ndarray:
        """Applies the full chain of humanization filters."""
        if y is None or y.size == 0:
            return np.array([])
            
        self.logger.start_section("APPLYING HUMANIZATION FILTERS")

        # Store original for comparison
        original_rms = np.sqrt(np.mean(y**2))
        
        # Apply filters in sequence
        y_h = y.copy()
        
        # 1. Subconscious perturbations (below conscious perception threshold)
        y_h = self._apply_subconscious_perturbations(y_h, sr)
        
        # 2. Equal loudness contour (Fletcher-Munson curves)
        y_h = self._apply_equal_loudness_contour(y_h, sr)
        
        # 3. Analog warmth simulation
        y_h = self._add_analog_warmth(y_h)
        
        # 4. Micro-dynamics and expressiveness
        y_h = self._apply_micro_dynamics(y_h, sr)
        
        # 5. Spatialization and air absorption
        y_h = self._apply_spatial_characteristics(y_h, sr)
        
        # Maintain overall loudness
        processed_rms = np.sqrt(np.mean(y_h**2))
        if processed_rms > self.invariants.IANNOTTI_INVARIANT_ZERO:
            y_h = y_h * (original_rms / processed_rms)
        
        self.logger.log(f"  ✅ Applied 5-stage humanization pipeline")
        self.logger.end_section()
        
        return librosa.util.normalize(y_h)

    def _apply_subconscious_perturbations(self, y: np.ndarray, sr: int) -> np.ndarray:
        """
        Introduces minute, calculated imperfections below the threshold of conscious perception.
        These create the 'feel' of human performance without being noticeable as errors.
        """
        y_p = y.copy()
        
        # 1. Micro-timing fluctuations (groove)
        onsets = librosa.onset.onset_detect(y=y, sr=sr, units='samples', 
                                          pre_max=20, post_max=20, 
                                          pre_avg=50, post_avg=10, 
                                          delta=0.1, wait=10)
        
        max_shift_ms = self.psychoacoustics.JND['timing_ms'] * 0.4  # Below JND
        max_shift_samps = int(sr * max_shift_ms / 1000)
        
        for onset in onsets:
            # Small random timing shift
            shift = np.random.randint(-max_shift_samps, max_shift_samps + 1)
            if shift == 0: 
                continue
                
            # Apply shift to a small window around the onset
            win = 2048
            start = max(0, onset - win // 2)
            end = min(len(y_p), onset + win // 2)
            
            if end - start > win // 2:  # Ensure reasonable window size
                segment = y_p[start:end].copy()
                y_p[start:end] = np.roll(segment, shift)

        # 2. "Hidden notes" based on auditory masking
        # Add very quiet partials that are masked by louder ones
        S = np.abs(librosa.stft(y_p))
        S_db = librosa.amplitude_to_db(S, ref=np.max)
        
        # Find prominent spectral peaks
        spectral_peaks = []
        for t in range(S_db.shape[1]):
            peaks, _ = find_peaks(S_db[:, t], height=-30, distance=5)
            spectral_peaks.extend([(freq_idx, t) for freq_idx in peaks])
            
        # Add masked partials near prominent peaks
        t = np.arange(len(y_p)) / sr
        for freq_idx, time_idx in spectral_peaks[:10]:  # Limit to first 10 peaks
            freq = librosa.fft_frequencies(sr=sr, n_fft=2048)[freq_idx]
            
            # Small detune (below pitch JND)
            detune_cents = (np.random.rand() - 0.5) * self.psychoacoustics.JND['pitch_cents'] * 0.8
            detuned_freq = freq * (2**(detune_cents / 1200))
            
            # Very quiet amplitude (below masking threshold)
            amp = np.max(np.abs(y_p)) * 10**(self.psychoacoustics.MASKING_THRESHOLD_DB / 20)
            
            # Add to signal
            y_p += amp * np.sin(2 * np.pi * detuned_freq * t)

        # 3. Infrasonic and ultrasonic content
        # These frequencies are felt more than heard
        infra_pulse = 0.001 * np.sin(2 * np.pi * self.psychoacoustics.HEARING_RANGE['infrasonic_hz'] * t)
        air_tone = 0.0005 * np.sin(2 * np.pi * self.psychoacoustics.HEARING_RANGE['ultrasonic_hz'] * t)
        
        y_p = y_p + infra_pulse + air_tone
        
        self.logger.log(f"  🎭 Added subconscious perturbations: {len(onsets)} timing shifts, hidden notes")
        
        return y_p

    def _apply_equal_loudness_contour(self, y: np.ndarray, sr: int) -> np.ndarray:
        """
        Applies Fletcher-Munson equal loudness contour correction.
        Human hearing is less sensitive to very low and very high frequencies.
        """
        # Apply a "smile" EQ curve - boost lows and highs slightly
        y_eq = y.copy()
        
        # Low shelf (boost bass)
        b_low, a_low = self.psychoacoustics.create_shelf_filter(120, 2.0, sr, 'low')
        y_eq = lfilter(b_low, a_low, y_eq)
        
        # High shelf (boost treble)
        b_high, a_high = self.psychoacoustics.create_shelf_filter(8000, 1.5, sr, 'high')
        y_eq = lfilter(b_high, a_high, y_eq)
        
        # Gentle mid dip
        b_mid, a_mid = self.psychoacoustics.create_shelf_filter(1000, -1.0, sr, 'low')
        y_eq = lfilter(b_mid, a_mid, y_eq)
        
        self.logger.log("  👂 Applied equal loudness contour correction")
        
        return y_eq

    def _add_analog_warmth(self, y: np.ndarray) -> np.ndarray:
        """
        Simulates the warmth and saturation characteristics of analog equipment.
        """
        # Soft clipping saturation
        y_warm = np.tanh(y * 1.3)
        
        # Add very slight even harmonic distortion for tube-like warmth
        even_harmonics = 0.02 * y**2
        y_warm = y_warm + even_harmonics
        
        # Gentle low-frequency emphasis
        y_warm = gaussian_filter1d(y_warm, sigma=2)
        
        self.logger.log("  🔥 Added analog warmth and saturation")
        
        return y_warm

    def _apply_micro_dynamics(self, y: np.ndarray, sr: int) -> np.ndarray:
        """
        Applies micro-level dynamic variations that mimic human performance.
        """
        y_dyn = y.copy()
        
        # Extract RMS envelope
        frame_length = 1024
        hop_length = 256
        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
        
        # Add small random variations to the envelope
        envelope_variation = 0.05  # 5% variation
        rms_varied = rms * (1 + envelope_variation * (np.random.rand(len(rms)) - 0.5))
        
        # Resample back to audio rate
        times = librosa.times_like(rms, sr=sr, hop_length=hop_length)
        target_times = np.arange(len(y)) / sr
        
        # Interpolate envelope
        from scipy.interpolate import interp1d
        if len(times) > 1:
            envelope_interp = interp1d(times, rms_varied, kind='linear', 
                                     bounds_error=False, fill_value='extrapolate')
            varied_envelope = envelope_interp(target_times)
            
            # Normalize and apply
            if np.max(np.abs(varied_envelope)) > self.invariants.IANNOTTI_INVARIANT_ZERO:
                varied_envelope = varied_envelope / np.max(np.abs(varied_envelope))
                y_dyn = y_dyn * varied_envelope
        
        self.logger.log("  🎹 Applied micro-dynamic variations")
        
        return y_dyn

    def _apply_spatial_characteristics(self, y: np.ndarray, sr: int) -> np.ndarray:
        """
        Simulates spatial characteristics like air absorption and room ambience.
        """
        y_spatial = y.copy()
        
        # High-frequency roll-off simulating air absorption
        # High frequencies attenuate more over distance
        b_air, a_air = self.psychoacoustics.create_shelf_filter(5000, -1.0, sr, 'high')
        y_spatial = lfilter(b_air, a_air, y_spatial)
        
        # Very subtle stereo widening (if needed for future stereo implementation)
        # For now, just maintain mono compatibility
        
        # Tiny amount of early reflections for "room feel"
        reflection_delay = int(sr * 0.005)  # 5ms delay
        if len(y_spatial) > reflection_delay:
            reflection = 0.1 * np.roll(y_spatial, reflection_delay)
            reflection[:reflection_delay] = 0
            y_spatial = y_spatial + reflection
            
        self.logger.log("  🏠 Applied spatial characteristics and air absorption")
        
        return y_spatial

    def apply_custom_humanization(self, y: np.ndarray, sr: int, 
                                humanization_profile: dict) -> np.ndarray:
        """
        Applies humanization based on a specific performance profile.
        Profiles can emulate different performance styles: robotic, human, expressive, etc.
        """
        if humanization_profile.get('style') == 'robotic':
            # Minimal humanization - keep it clean and precise
            return y
        elif humanization_profile.get('style') == 'human':
            # Standard humanization as above
            return self.apply_all_filters(y, sr)
        elif humanization_profile.get('style') == 'expressive':
            # Extra humanization for very expressive performance
            y_expressive = y.copy()
            
            # More timing variation
            onsets = librosa.onset.onset_detect(y=y, sr=sr, units='samples')
            for onset in onsets:
                shift = np.random.randint(-int(sr*0.01), int(sr*0.01)+1)  # Up to 10ms
                if abs(shift) > 10:
                    win = 4096
                    start = max(0, onset - win//2)
                    end = min(len(y_expressive), onset + win//2)
                    segment = y_expressive[start:end].copy()
                    y_expressive[start:end] = np.roll(segment, shift)
                    
            # More dynamic variation
            rms = librosa.feature.rms(y=y_expressive)[0]
            varied_rms = rms * (1 + 0.1 * (np.random.rand(len(rms)) - 0.5))
            
            # Apply varied dynamics
            # ... implementation would resample and apply as in _apply_micro_dynamics
            
            return y_expressive
        else:
            return self.apply_all_filters(y, sr)
</div>
</div>

<!-- MODULE: humanization_psychoacoustics.py -->
<div class="module" id="humanization_psycho">
<h3>humanization_psychoacoustics.py - Psychoacoustics</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# humanization_psychoacoustics.py: Tri-Resonance Synthesis Engine - Psychoacoustic Model
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# A dedicated module to store the data and models of the human ear, including
# JND, masking thresholds, and equal-loudness contours.
# ==============================================================================

import numpy as np
from scipy.signal import butter, cheby1, bessel

class PsychoacousticModel:
    """
    Stores data and models related to the physics of human hearing.
    These models ensure our synthetic audio respects the limitations and
    characteristics of human auditory perception.
    """
    def __init__(self):
        # --- Just-Noticeable Difference (JND) thresholds ---
        # The smallest changes humans can detect in various auditory dimensions
        self.JND = {
            'pitch_cents': 5.0,      # Smallest detectable pitch change (in cents)
            'timing_ms': 10.0,       # Smallest detectable timing change (in milliseconds)
            'loudness_db': 1.0,      # Smallest detectable loudness change (in dB)
            'phase_degrees': 30.0,   # Phase difference detection threshold
            'bandwidth_hz': 5.0,     # Bandwidth change detection
        }

        # --- Human hearing range ---
        self.HEARING_RANGE = {
            'infrasonic_hz': 16,     # Below this, vibration more than sound
            'low_hz': 20,            # Traditional low end of hearing
            'high_hz': 20000,        # Traditional high end (varies with age)
            'ultrasonic_hz': 22000,  # Above this, generally inaudible
            'sensitive_range': (1000, 4000),  # Most sensitive frequency range
        }

        # --- Auditory masking thresholds ---
        # A sound quieter than this (relative to a masker) will be inaudible
        self.MASKING_THRESHOLD_DB = -25.0  # 25 dB below masker

        # --- Critical bands (Bark scale approximations) ---
        # Human hearing divides frequency into critical bands where masking occurs
        self.CRITICAL_BANDS = [
            (20, 100),    # 1
            (100, 200),   # 2
            (200, 300),   # 3
            (300, 400),   # 4
            (400, 510),   # 5
            (510, 630),   # 6
            (630, 770),   # 7
            (770, 920),   # 8
            (920, 1080),  # 9
            (1080, 1270), # 10
            (1270, 1480), # 11
            (1480, 1720), # 12
            (1720, 2000), # 13
            (2000, 2320), # 14
            (2320, 2700), # 15
            (2700, 3150), # 16
            (3150, 3700), # 17
            (3700, 4400), # 18
            (4400, 5300), # 19
            (5300, 6400), # 20
            (6400, 7700), # 21
            (7700, 9500), # 22
            (9500, 12000),# 23
            (12000, 15500),# 24
            (15500, 20000) # 25
        ]

        # --- Equal loudness contours (Fletcher-Munson) ---
        # How loud different frequencies need to be to sound equally loud
        self.EQUAL_LOUDNESS_CONTOURS = {
            # Frequency (Hz): relative gain (dB) needed at low listening levels
            20: +20.0,    # Bass needs significant boost at low volumes
            50: +12.0,
            100: +6.0,
            200: +2.0,
            500: 0.0,     # Reference point
            1000: 0.0,
            2000: -1.0,
            5000: -3.0,
            10000: -5.0,
            15000: -8.0,
            20000: -20.0  # High frequencies need less boost
        }

        # --- Temporal masking ---
        # How sound can mask other sounds that occur just before or after
        self.TEMPORAL_MASKING = {
            'pre_masking_ms': 20,    # Backward masking duration
            'post_masking_ms': 100,  # Forward masking duration
        }

    def create_shelf_filter(self, cutoff: float, gain_db: float, sr: int, ftype: str):
        """
        Designs a second-order shelving filter.
        Based on the Audio EQ Cookbook by Robert Bristow-Johnson.
        
        Args:
            cutoff: Shelf cutoff frequency in Hz
            gain_db: Gain at shelf in dB
            sr: Sample rate in Hz
            ftype: 'low' for low shelf, 'high' for high shelf
            
        Returns:
            b, a: Filter coefficients
        """
        A = 10**(gain_db / 40.0)  # Convert dB to linear gain
        w0 = 2 * np.pi * cutoff / sr  # Normalized frequency
        cos_w0 = np.cos(w0)
        sin_w0 = np.sin(w0)
        
        # Use a Q of 0.707 for smooth response
        S = 1.0  # Shelf slope parameter
        alpha = sin_w0 / 2.0 * np.sqrt((A + 1/A) * (1/S - 1) + 2)

        if ftype == 'low':
            b0 = A * ((A + 1) - (A - 1) * cos_w0 + 2 * np.sqrt(A) * alpha)
            b1 = 2 * A * ((A - 1) - (A + 1) * cos_w0)
            b2 = A * ((A + 1) - (A - 1) * cos_w0 - 2 * np.sqrt(A) * alpha)
            a0 = (A + 1) + (A - 1) * cos_w0 + 2 * np.sqrt(A) * alpha
            a1 = -2 * ((A - 1) + (A + 1) * cos_w0)
            a2 = (A + 1) + (A - 1) * cos_w0 - 2 * np.sqrt(A) * alpha
        else:  # high shelf
            b0 = A * ((A + 1) + (A - 1) * cos_w0 + 2 * np.sqrt(A) * alpha)
            b1 = -2 * A * ((A - 1) + (A + 1) * cos_w0)
            b2 = A * ((A + 1) + (A - 1) * cos_w0 - 2 * np.sqrt(A) * alpha)
            a0 = (A + 1) - (A - 1) * cos_w0 + 2 * np.sqrt(A) * alpha
            a1 = 2 * ((A - 1) - (A + 1) * cos_w0)
            a2 = (A + 1) - (A - 1) * cos_w0 - 2 * np.sqrt(A) * alpha
            
        # Normalize coefficients
        return np.array([b0/a0, b1/a0, b2/a0]), np.array([1.0, a1/a0, a2/a0])

    def get_critical_band(self, frequency: float) -> tuple:
        """
        Returns the critical band that contains the given frequency.
        """
        for band_low, band_high in self.CRITICAL_BANDS:
            if band_low <= frequency <= band_high:
                return (band_low, band_high)
        return (self.CRITICAL_BANDS[0][0], self.CRITICAL_BANDS[-1][1])

    def calculate_masking_threshold(self, masker_freq: float, masker_level_db: float, 
                                  probe_freq: float) -> float:
        """
        Calculates the masking threshold for a probe frequency given a masker.
        Returns the level in dB that the probe must exceed to be audible.
        """
        # Simple masking model based on critical bands
        masker_band = self.get_critical_band(masker_freq)
        probe_band = self.get_critical_band(probe_freq)
        
        # Check if in same critical band
        if masker_band == probe_band:
            # Strong masking within same band
            threshold = masker_level_db + self.MASKING_THRESHOLD_DB
        else:
            # Weaker masking across bands
            band_distance = abs(self.CRITICAL_BANDS.index(masker_band) - 
                              self.CRITICAL_BANDS.index(probe_band))
            attenuation = band_distance * 5.0  # 5 dB per critical band
            threshold = masker_level_db + self.MASKING_THRESHOLD_DB + attenuation
            
        return threshold

    def get_loudness_correction(self, frequency: float, reference_level: float = 80.0) -> float:
        """
        Returns the loudness correction in dB for a given frequency.
        Based on equal loudness contours.
        """
        # Find the two closest frequencies in our contour data
        frequencies = sorted(self.EQUAL_LOUDNESS_CONTOURS.keys())
        
        if frequency <= frequencies[0]:
            return self.EQUAL_LOUDNESS_CONTOURS[frequencies[0]]
        elif frequency >= frequencies[-1]:
            return self.EQUAL_LOUDNESS_CONTOURS[frequencies[-1]]
            
        # Linear interpolation between known points
        for i in range(len(frequencies) - 1):
            if frequencies[i] <= frequency <= frequencies[i + 1]:
                f_low, f_high = frequencies[i], frequencies[i + 1]
                gain_low = self.EQUAL_LOUDNESS_CONTOURS[f_low]
                gain_high = self.EQUAL_LOUDNESS_CONTOURS[f_high]
                
                # Linear interpolation
                t = (frequency - f_low) / (f_high - f_low)
                return gain_low + t * (gain_high - gain_low)
                
        return 0.0

    def is_audible(self, frequency: float, level_db: float, maskers: list = None) -> bool:
        """
        Determines if a sound would be audible given possible maskers.
        """
        # Check if within hearing range
        if not (self.HEARING_RANGE['low_hz'] <= frequency <= self.HEARING_RANGE['high_hz']):
            return False
            
        # Absolute threshold of hearing (simplified)
        absolute_threshold = self.get_absolute_threshold(frequency)
        if level_db < absolute_threshold:
            return False
            
        # Check masking if maskers are provided
        if maskers:
            for masker_freq, masker_level in maskers:
                masking_threshold = self.calculate_masking_threshold(
                    masker_freq, masker_level, frequency)
                if level_db < masking_threshold:
                    return False
                    
        return True

    def get_absolute_threshold(self, frequency: float) -> float:
        """
        Returns the absolute threshold of hearing in dB SPL for a given frequency.
        Simplified model based on standard audiometric data.
        """
        # Minimum audible field (MAF) approximation
        if frequency < 100:
            return 30.0
        elif frequency < 1000:
            return 20.0 - (frequency - 100) * 0.01  # Rough approximation
        elif frequency < 3000:
            return 0.0  # Most sensitive range
        elif frequency < 8000:
            return (frequency - 3000) * 0.005
        else:
            return 25.0 + (frequency - 8000) * 0.01

    def create_perceptual_eq_curve(self, target_frequencies: list, sr: int) -> list:
        """
        Creates an EQ curve that compensates for human hearing characteristics.
        """
        eq_curve = []
        for freq in target_frequencies:
            correction = self.get_loudness_correction(freq)
            eq_curve.append((freq, correction))
            
        return eq_curve
</div>
</div>

<!-- MODULE: utils_source_manager.py -->
<div class="module" id="utils_source">
<h3>utils_source_manager.py - Source Manager</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# utils_source_manager.py: Tri-Resonance Synthesis Engine - Automated Source Separation
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# This module handles the automated loading and separation of source files,
# with adaptive logic to handle various input configurations.
# ==============================================================================

import os
import librosa
import warnings
import numpy as np

# Try to import Spleeter for source separation
try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        from spleeter.separator import Separator
    SPLEETER_AVAILABLE = True
except ImportError:
    SPLEETER_AVAILABLE = False
    Separator = None

class SourceManager:
    """
    Manages finding, loading, and separating audio sources and lyrics.
    Handles various input configurations gracefully, from single files to complex multi-track setups.
    """
    def __init__(self, logger):
        self.logger = logger
        self.separator = None
        
        # Initialize Spleeter if available
        if SPLEETER_AVAILABLE:
            try:
                # Use 2-stem model (vocals + accompaniment) for efficiency
                self.separator = Separator('spleeter:2stems-16kHz', multiprocess=False)
                self.logger.log("✅ Spleeter source separator initialized (2stems-16kHz)")
            except Exception as e:
                self.logger.log(f"⚠️  Spleeter failed to initialize: {e}")
                self.separator = None
        else:
            self.logger.log("⚠️  Spleeter not available. To enable: pip install spleeter")
            self.logger.log("   Source separation disabled. Using full audio mixes.")

    def find_sources(self, input_dir: str) -> list:
        """Finds all valid source files (audio or text) in the input directory."""
        sources = {}
        
        if not os.path.exists(input_dir):
            self.logger.log(f"❌ Input directory '{input_dir}' not found")
            return []
            
        self.logger.log(f"📁 Scanning directory: {input_dir}")
        
        for f in sorted(os.listdir(input_dir)):
            base_name = os.path.splitext(f)[0]
            if base_name not in sources:
                sources[base_name] = {'name': base_name, 'files': []}
            
            full_path = os.path.join(input_dir, f)
            
            # Categorize files
            if f.lower().endswith(('.wav', '.mp3', '.flac', '.aiff', '.m4a')):
                sources[base_name]['files'].append(('audio', full_path))
            elif f.lower().endswith('.txt'):
                sources[base_name]['files'].append(('lyrics', full_path))
            elif f.lower().endswith(('.mid', '.midi')):
                sources[base_name]['files'].append(('midi', full_path))
                
        # Convert to list of source dictionaries
        source_list = []
        for base_name, source_data in sources.items():
            source_info = {'name': base_name}
            
            for file_type, file_path in source_data['files']:
                if file_type == 'audio':
                    source_info['audio_path'] = file_path
                elif file_type == 'lyrics':
                    source_info['lyric_path'] = file_path
                elif file_type == 'midi':
                    source_info['midi_path'] = file_path
                    
            source_list.append(source_info)
            
        self.logger.log(f"📊 Found {len(source_list)} potential sources")
        return source_list

    def process_input_directory(self, input_dir: str) -> list:
        """Processes all sources in a directory, handling different configurations."""
        self.logger.start_section("PROCESSING INPUT SOURCES")
        
        sources = self.find_sources(input_dir)
        if not sources:
            self.logger.log("❌ No valid source files found")
            self.logger.end_section()
            return []
        
        prepared_sources = []
        for source_info in sources:
            prepared = self.prepare_source(source_info)
            if prepared:
                prepared_sources.append(prepared)
                
        self.logger.log(f"✅ Successfully prepared {len(prepared_sources)} sources")
        self.logger.end_section()
        return prepared_sources

    def prepare_source(self, source_info: dict) -> dict:
        """Loads audio, lyrics, and performs separation for a single source."""
        self.logger.start_section(f"PREPARING SOURCE: {source_info['name']}")
        
        # Initialize with default values
        source_info['sr'] = 44100
        source_info['music_y'] = np.array([])
        source_info['vocals_y'] = np.array([])
        source_info['lyrics'] = ""
        source_info['midi_data'] = None
        
        # Load lyrics if available
        lyric_path = source_info.get('lyric_path')
        if lyric_path and os.path.exists(lyric_path):
            try:
                with open(lyric_path, 'r', encoding='utf-8') as f:
                    source_info['lyrics'] = f.read()
                self.logger.log(f"📝 Loaded lyrics: {len(source_info['lyrics'])} characters")
            except Exception as e:
                self.logger.log(f"⚠️  Could not read lyrics: {e}")
                
        # Load MIDI if available (placeholder for future implementation)
        midi_path = source_info.get('midi_path')
        if midi_path and os.path.exists(midi_path):
            self.logger.log("🎹 MIDI file found (MIDI processing not yet implemented)")
            # Future: Add MIDI to audio conversion here
            
        # Load and process audio
        audio_path = source_info.get('audio_path')
        if audio_path and os.path.exists(audio_path):
            try:
                self.logger.log(f"🎵 Loading audio: {os.path.basename(audio_path)}")
                
                # Load audio with librosa
                y, sr = librosa.load(audio_path, sr=44100, mono=True)
                source_info['sr'] = sr
                source_info['original_audio'] = y
                
                # Perform source separation if lyrics are present and Spleeter is available
                if source_info['lyrics'] and self.separator:
                    self.logger.log("🎤 Separating vocals and accompaniment...")
                    
                    try:
                        # Spleeter expects specific input format
                        waveform = np.expand_dims(y, axis=-1)
                        prediction = self.separator.separate(waveform)
                        
                        source_info['music_y'] = prediction['accompaniment'].flatten()
                        source_info['vocals_y'] = prediction['vocals'].flatten()
                        
                        self.logger.log("✅ Source separation successful")
                        
                    except Exception as e:
                        self.logger.log(f"⚠️  Spleeter separation failed: {e}")
                        self.logger.log("   Using full mix as music component")
                        source_info['music_y'] = y
                else:
                    # No separation - use full mix
                    if not source_info['lyrics']:
                        self.logger.log("ℹ️  No lyrics found, using full audio as music")
                    else:
                        self.logger.log("ℹ️  Spleeter unavailable, using full audio as music")
                    source_info['music_y'] = y
                    
                self.logger.log(f"📊 Audio: {len(y)/sr:.1f}s, {sr}Hz, " +
                               f"music: {len(source_info['music_y'])} samples")
                               
                if 'vocals_y' in source_info and source_info['vocals_y'].size > 0:
                    self.logger.log(f"🎤 Vocals: {len(source_info['vocals_y'])} samples")

            except Exception as e:
                self.logger.log(f"❌ Failed to load audio: {e}")
                self.logger.end_section()
                return None
        else:
            self.logger.log("⚠️  No audio file found for this source")
            self.logger.end_section()
            return None
            
        self.logger.end_section()
        return source_info

    def validate_source(self, source_info: dict) -> bool:
        """Validates that a source has the minimum required data for processing."""
        if not source_info:
            return False
            
        # Must have at least music audio
        if 'music_y' not in source_info or source_info['music_y'].size == 0:
            return False
            
        # Audio must be long enough (at least 1 second)
        if len(source_info['music_y']) < source_info['sr']:
            self.logger.log(f"⚠️  Source '{source_info['name']}' too short, skipping")
            return False
            
        return True

    def get_source_summary(self, source_info: dict) -> dict:
        """Returns a summary of the source's characteristics."""
        if not self.validate_source(source_info):
            return {}
            
        summary = {
            'name': source_info['name'],
            'duration': len(source_info['music_y']) / source_info['sr'],
            'sample_rate': source_info['sr'],
            'has_lyrics': bool(source_info.get('lyrics')),
            'has_vocals': 'vocals_y' in source_info and source_info['vocals_y'].size > 0,
            'has_midi': 'midi_data' in source_info and source_info['midi_data'] is not None
        }
        
        # Add audio characteristics
        if source_info['music_y'].size > 0:
            music_rms = np.sqrt(np.mean(source_info['music_y']**2))
            summary['music_energy'] = music_rms
            
        if 'vocals_y' in source_info and source_info['vocals_y'].size > 0:
            vocals_rms = np.sqrt(np.mean(source_info['vocals_y']**2))
            summary['vocals_energy'] = vocals_rms
            
        return summary

    def create_silence_source(self, duration: float = 10.0, sr: int = 44100) -> dict:
        """
        Creates a silent source for when no audio is available.
        Useful for testing or when working with lyrics-only inputs.
        """
        silence = np.zeros(int(duration * sr))
        
        return {
            'name': 'silence',
            'sr': sr,
            'music_y': silence,
            'vocals_y': np.array([]),
            'lyrics': '',
            'is_silence': True
        }

    def mix_sources(self, sources: list, weights: list = None) -> dict:
        """
        Mixes multiple sources into a single composite source.
        Useful for creating ensemble inputs or testing combinations.
        """
        if not sources:
            return self.create_silence_source()
            
        if weights is None:
            weights = [1.0 / len(sources)] * len(sources)
            
        # Find common sample rate and duration
        common_sr = sources[0]['sr']
        max_duration = max(len(s['music_y']) for s in sources)
        
        # Mix audio
        mixed_audio = np.zeros(max_duration)
        
        for source, weight in zip(sources, weights):
            audio = source['music_y']
            if len(audio) < max_duration:
                # Pad with silence
                audio = np.pad(audio, (0, max_duration - len(audio)))
            else:
                # Truncate
                audio = audio[:max_duration]
                
            mixed_audio += weight * audio
            
        # Combine lyrics
        combined_lyrics = "\n\n".join(s.get('lyrics', '') for s in sources if s.get('lyrics'))
        
        return {
            'name': f"mix_{'_'.join(s['name'] for s in sources)}",
            'sr': common_sr,
            'music_y': mixed_audio,
            'vocals_y': np.array([]),
            'lyrics': combined_lyrics,
            'is_mix': True,
            'source_components': [s['name'] for s in sources]
        }
</div>
</div>

<!-- MODULE: utils_output_manager.py -->
<div class="module" id="utils_output">
<h3>utils_output_manager.py - Output Manager</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# utils_output_manager.py: Tri-Resonance Synthesis Engine - Output Management Utilities
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# This module handles the creation of the full output suite, including
# README, logs, audio, and all visualizations (static and animated).
# ==============================================================================

import os
import datetime
import numpy as np
import shutil
from PIL import Image, ImageDraw, ImageFont
import soundfile as sf
import imageio
import json
import matplotlib.pyplot as plt
from utils_logger import ProcessLogger
from invariants import Invariants
from math_core import MathCore
from musical_structures import Denotator

class OutputGenerator:
    """
    Handles creation and writing of all final output files.
    Generates the complete 21-part output package for each synthesis run.
    """
    def __init__(self, base_dir: str, logger: ProcessLogger):
        self.timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = os.path.join(base_dir, "output", f"run_{self.timestamp}")
        self.logger = logger
        self.invariants = Invariants()
        
        # Create output directory
        os.makedirs(self.output_dir, exist_ok=True)
        self.logger.master_log_path = os.path.join(self.output_dir, f"log_master_{self.timestamp}.txt")
        self.logger.log(f"📁 Created output directory: {self.output_dir}")

    def copy_sources_to_output(self, sources: list):
        """Copies original source files to the output directory for reference."""
        self.logger.log("📋 Copying source files to output directory")
        
        for source in sources:
            # Copy audio files
            if 'audio_path' in source and os.path.exists(source['audio_path']):
                shutil.copy(source['audio_path'], self.output_dir)
                
            # Copy lyric files  
            if 'lyric_path' in source and os.path.exists(source['lyric_path']):
                shutil.copy(source['lyric_path'], self.output_dir)
                
            # Copy MIDI files
            if 'midi_path' in source and os.path.exists(source['midi_path']):
                shutil.copy(source['midi_path'], self.output_dir)

    def write_audio(self, name: str, audio_data: np.ndarray, sr: int):
        """Writes audio data to WAV file."""
        filepath = os.path.join(self.output_dir, f"{name}.wav")
        self.logger.log(f"🎵 Writing audio: {filepath}")
        
        try:
            sf.write(filepath, audio_data, sr)
            self.logger.log(f"  ✅ Audio written: {len(audio_data)/sr:.1f}s, {sr}Hz")
        except Exception as e:
            self.logger.log(f"❌ ERROR writing audio {filepath}: {e}")

    def generate_geometry_visualization(self, name: str, vector: np.ndarray):
        """Generates a geometric visualization of a state vector."""
        filepath = os.path.join(self.output_dir, f"{name}_Geometry.png")
        self.logger.log(f"🎨 Generating geometric visualization: {filepath}")
        
        if vector is None or vector.size == 0:
            self.logger.log("⚠️  Empty vector provided for visualization")
            return

        try:
            vector_len = len(vector)
            
            # Choose grid dimensions that are visually pleasing
            width = int(np.ceil(np.sqrt(vector_len)))
            height = int(np.ceil(vector_len / width))
            
            # Pad vector to fit grid
            padded_vector = np.full(width * height, self.invariants.IANNOTTI_INVARIANT_ZERO)
            padded_vector[:vector_len] = vector
            grid = padded_vector.reshape((height, width))
            
            # Normalize for visualization
            min_val, max_val = np.min(grid), np.max(grid)
            if abs(max_val - min_val) < self.invariants.IANNOTTI_INVARIANT_ZERO:
                normalized_grid = np.full(grid.shape, 128, dtype=np.uint8)
            else:
                normalized_grid = 255 * (grid - min_val) / (max_val - min_val)
                normalized_grid = normalized_grid.astype(np.uint8)
            
            # Create image
            img = Image.fromarray(normalized_grid, 'L')
            
            # Scale for better visibility
            scale = max(self.invariants.MIN_GRID_SCALE, 
                       min(self.invariants.MAX_GRID_SCALE, 512 // width))
            img = img.resize((width * scale, height * scale), Image.NEAREST)
            
            # Add border and title
            border_size = 20
            bordered_img = Image.new('L', 
                                   (width * scale + 2 * border_size, 
                                    height * scale + 2 * border_size), 
                                   255)
            bordered_img.paste(img, (border_size, border_size))
            
            # Try to add text label
            try:
                draw = ImageDraw.Draw(bordered_img)
                # Use default font
                font = ImageFont.load_default()
                draw.text((10, 5), f"{name} ({vector_len}D)", fill=0, font=font)
            except:
                pass  # Skip text if font not available
                
            bordered_img.save(filepath)
            
        except Exception as e:
            self.logger.log(f"❌ ERROR generating visualization {name}: {e}")

    def generate_evolution_gif(self, name: str, frame_vectors: list):
        """Generates an animated GIF showing the evolution of a state vector."""
        filepath = os.path.join(self.output_dir, f"{name}_Evolution.gif")
        self.logger.log(f"🎞️  Generating evolutionary GIF: {filepath}")

        # Filter valid frames
        valid_frames = [v for v in frame_vectors if v is not None and v.size > 0]
        if len(valid_frames) < 2:
            self.logger.log("⚠️  Not enough valid frames for GIF generation")
            return

        try:
            images = []
            
            # Use first frame to determine dimensions
            first_frame = valid_frames[0]
            vector_len = len(first_frame)
            width = int(np.ceil(np.sqrt(vector_len)))
            height = int(np.ceil(vector_len / width))
            
            # Calculate global min/max for consistent coloring
            all_frames_flat = np.concatenate(valid_frames)
            global_min, global_max = np.min(all_frames_flat), np.max(all_frames_flat)
            
            for i, vector in enumerate(valid_frames):
                current_len = len(vector)
                current_width = int(np.ceil(np.sqrt(current_len)))
                current_height = int(np.ceil(current_len / current_width))
                
                # Pad current vector
                padded = np.full(current_width * current_height, self.invariants.IANNOTTI_INVARIANT_ZERO)
                padded[:current_len] = vector
                grid = padded.reshape((current_height, current_width))

                # Normalize using global range
                if abs(global_max - global_min) < self.invariants.IANNOTTI_INVARIANT_ZERO:
                    norm_grid = np.full(grid.shape, 128, dtype=np.uint8)
                else:
                    norm_grid = 255 * (grid - global_min) / (global_max - global_min)
                    norm_grid = norm_grid.astype(np.uint8)

                # Create and scale image
                img = Image.fromarray(norm_grid, 'L')
                scale = max(8, 256 // current_width)
                img = img.resize((current_width * scale, current_height * scale), Image.NEAREST)
                
                # Add frame number
                try:
                    draw = ImageDraw.Draw(img)
                    font = ImageFont.load_default()
                    draw.text((5, 5), f"Frame {i}", fill=255, font=font)
                except:
                    pass
                    
                images.append(np.array(img))

            # Create GIF
            imageio.mimsave(filepath, images, duration=self.invariants.GIF_FRAME_DURATION, loop=0)
            self.logger.log(f"  ✅ GIF created: {len(images)} frames")
            
        except Exception as e:
            self.logger.log(f"❌ ERROR generating GIF {name}: {e}")

    def write_process_log(self, name: str, denotator: Denotator, math_engine: MathCore):
        """Writes a detailed process log for a synthesis result."""
        filepath = os.path.join(self.output_dir, f"{name}_Log.txt")
        self.logger.log(f"📝 Writing process log: {filepath}")
        
        try:
            log_data = denotator.to_dict()
            log_data['vector_sha256'] = math_engine.hash_vector(denotator.vector)
            log_data['analysis_timestamp'] = datetime.datetime.now().isoformat()
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, indent=2, cls=NumpyEncoder, ensure_ascii=False)
                
        except Exception as e:
            self.logger.log(f"❌ ERROR writing process log {name}: {e}")

    def generate_readme(self, input_names: list):
        """Generates a comprehensive README file for the output package."""
        filepath = os.path.join(self.output_dir, "README.md")
        
        content = f"""# Tri-Resonance Synthesis Engine - Run Output
**Run Timestamp:** {self.timestamp}
**Engine Version:** 12.0 (Definitive Build)

## Overview
This directory contains the complete output from a single run of the Tri-Resonance Synthesis Engine. The engine has analyzed your input sources and generated new musical compositions through mathematical synthesis.

## Input Sources
{len(input_names)} source(s) were processed:
{chr(10).join(f"- `{name}`" for name in input_names)}

## Output Files Guide

### Audio Outputs (4 files)
- `Song_X.wav` - Biomimetic synthesis (human-like creative process)
- `Song_Y.wav` - Platonic synthesis (mathematical ideal form)  
- `Song_Z.wav` - Recursive synthesis (synthesis of syntheses)
- `Song_G.wav` - Concordance Gap (sonification of creative tension)

### Geometric Visualizations (8 files)
- `Song_[X,Y,Z,G]_Geometry.png` - 2D maps of each song's state vector
- `Song_[X,Y,Z,G]_PostAnalysis_Geometry.png` - Analysis of the rendered audio

### Evolutionary Visualizations (4 files)  
- `Song_[X,Y,Z,G]_Evolution.gif` - Animated evolution of each synthesis process

### Process Logs (8 files)
- `Song_[X,Y,Z,G]_Log.txt` - Detailed metadata and analysis for each song
- `Song_[X,Y,Z,G]_PostAnalysis_Log.txt` - Post-rendering analysis

### Master Files (1 file)
- `log_master_{self.timestamp}.txt` - Complete process log of the entire run

## The Synthesis Process

This engine implements a **Tri-Resonance Architecture**:

1. **Biomimetic Path (Song X)**: Models human creative intuition
2. **Platonic Path (Song Y)**: Pursues mathematical perfection  
3. **Recursive Synthesis (Song Z)**: Combines X and Y for higher-order creation
4. **Concordance Gap (Song G)**: Sonifies the tension between intuition and ideal

Each song exists as a point in a high-dimensional "musical manifold" - a mathematical space where similar musical ideas are close together and different ideas are far apart.

## Technical Details

- **Sample Rate**: 44.1 kHz
- **Synthesis Method**: Hybrid additive + procedural synthesis
- **Analysis Framework**: Prime Translation Framework + Bounded Equations
- **Optimization**: Ψ–Gradient Flow with Adam optimizer
- **Humanization**: Psychoacoustic filters for natural sound

## Interpretation Guide

- Higher-dimensional vectors generally represent more complex musical ideas
- Similar geometric patterns suggest similar musical structures  
- The evolution GIFs show how each composition developed over time
- Song G represents the "creative tension" between different approaches

## Next Steps

Listen to the audio files to experience the synthesized compositions. Compare the geometric visualizations to understand their mathematical relationships. The process logs provide detailed technical information for further analysis.

---
*Generated by Tri-Resonance Synthesis Engine v12.0*
"""
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
            
        self.logger.log("✅ Generated comprehensive README.md")

    def write_creation_story(self, story: dict):
        """Writes the creation story for this synthesis run."""
        filepath = os.path.join(self.output_dir, "Creation_Story.md")
        
        content = f"""# Creation Story: {story.get('title', 'A Musical Journey')}

**Timestamp:** {story.get('timestamp', 'Unknown')}

## The Beginning

{story.get('story', 'This is the story of a musical creation born from mathematical principles and human creativity.')}

## Technical Genesis

This composition emerged from the interaction of multiple synthesis pathways:

- **Biomimetic Intelligence**: The engine's imitation of human-like creative intuition
- **Platonic Forms**: Pursuit of mathematical perfection and elegance  
- **Recursive Emergence**: Higher-order patterns arising from simpler interactions
- **Creative Tension**: The productive conflict between different approaches

## Philosophical Context

This work exists at the intersection of mathematics and art, computation and creativity. It demonstrates how rigorous mathematical frameworks can give rise to expressive artistic outcomes, bridging the perceived gap between logic and emotion.

## The Compositions

Four distinct musical entities were born from this process, each with its own character and purpose. Together, they form a complete ecosystem of musical ideas, exploring different regions of the vast space of possible sounds.

## Legacy

This creation now takes its place in the Grand Atlas of musical space, a permanent record of this particular configuration of mathematical and creative forces. Future explorations may reference this work, build upon it, or react against it - such is the nature of artistic evolution.

---
*"The most beautiful thing we can experience is the mysterious. It is the source of all true art and science."* - Albert Einstein
"""
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
            
        self.logger.log("📖 Wrote creation story")

    def generate_summary_report(self, sources: list, results: dict):
        """Generates a summary report of the entire synthesis run."""
        filepath = os.path.join(self.output_dir, "Summary_Report.md")
        
        # This would compile statistics about the run
        # For now, create a simple summary
        content = f"""# Synthesis Run Summary

## Run Information
- **Timestamp**: {self.timestamp}
- **Sources Processed**: {len(sources)}
- **Songs Generated**: 4
- **Total Output Files**: 21

## Source Analysis
{chr(10).join(f"- {s['name']}: {len(s.get('music_y', []))/s.get('sr', 1):.1f}s" for s in sources)}

## Generated Compositions
- **Song X**: Biomimetic synthesis 
- **Song Y**: Platonic idealization
- **Song Z**: Recursive emergence
- **Song G**: Creative tension

## Quality Metrics
*Harmony scores and other quality metrics would be included here*

## Technical Performance
*Processing times and resource usage would be included here*

---
*Summary generated automatically by Tri-Resonance Engine*
"""
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
            
        self.logger.log("📊 Generated summary report")

class NumpyEncoder(json.JSONEncoder):
    """Custom JSON encoder for numpy data types"""
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.complexfloating):
            return {'real': obj.real, 'imag': obj.imag}
        return json.JSONEncoder.default(self, obj)
</div>
</div>

<!-- MODULE: utils_logger.py -->
<div class="module" id="utils_logger">
<h3>utils_logger.py - Logger</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# utils_logger.py: Tri-Resonance Synthesis Engine - Logger Utility
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# A dedicated module for handling detailed, hierarchical logging.
# ==============================================================================

import datetime
import os
import numpy as np
import sys

class ProcessLogger:
    """
    A comprehensive class to build detailed, hierarchical log files.
    Provides both console output and detailed file logging with timing information.
    """
    def __init__(self, log_to_console: bool = True):
        self.log_content = []
        self.indent_level = 0
        self.start_time = datetime.datetime.now()
        self.master_log_path = None  # Set by OutputManager
        self.log_to_console = log_to_console
        
        self.log(f"🚀 Process Log Initialized: {self.start_time.isoformat()}", header=True)

    def start_section(self, name: str):
        """Starts a new hierarchical section in the log."""
        self.log(name, header=True)
        self.indent_level += 1

    def end_section(self):
        """Ends the current hierarchical section."""
        self.indent_level = max(0, self.indent_level - 1)
        self.log_content.append("")  # Empty line for separation

    def log(self, message: str, header: bool = False):
        """Adds a message to the log with proper indentation and timing."""
        time_delta = (datetime.datetime.now() - self.start_time).total_seconds()
        indent = "  " * self.indent_level
        
        if header:
            # Create a header block
            header_line = "=" * (80 - len(indent))
            self.log_content.append(f"\n{indent}{header_line}")
            self.log_content.append(f"{indent}[{time_delta:8.3f}s] :: {message.upper()}")
            self.log_content.append(f"{indent}{header_line}")
            
            if self.log_to_console:
                print(f"\n{indent}{message.upper()}")
                print(f"{indent}{'-' * len(message)}")
        else:
            # Regular log message
            log_line = f"{indent}[{time_delta:8.3f}s] -- {message}"
            self.log_content.append(log_line)
            
            if self.log_to_console:
                print(log_line)

    def log_vector(self, name: str, vector: np.ndarray):
        """Logs information about a numpy vector."""
        if vector is None or vector.size == 0:
            self.log(f"Vector '{name}': Empty or None")
            return
            
        norm = np.linalg.norm(vector)
        self.log(f"Vector '{name}': shape={vector.shape}, norm={norm:.4f}, "
                f"range=[{np.min(vector):.3e}, {np.max(vector):.3e}]")

    def log_dict(self, name: str, data: dict, max_items: int = 10):
        """Logs the contents of a dictionary."""
        self.log(f"Dictionary '{name}': {len(data)} items")
        
        items_logged = 0
        for key, value in data.items():
            if items_logged >= max_items:
                self.log(f"  ... and {len(data) - max_items} more items")
                break
                
            if isinstance(value, (int, float, str, bool)):
                self.log(f"  {key}: {value}")
            elif isinstance(value, np.ndarray):
                self.log(f"  {key}: array{value.shape}")
            else:
                self.log(f"  {key}: {type(value).__name__}")
                
            items_logged += 1

    def log_progress(self, current: int, total: int, message: str = ""):
        """Logs progress information with a percentage."""
        percentage = (current / total) * 100 if total > 0 else 0
        progress_msg = f"Progress: {current}/{total} ({percentage:.1f}%)"
        if message:
            progress_msg = f"{message} - {progress_msg}"
        self.log(progress_msg)

    def log_warning(self, message: str):
        """Logs a warning message."""
        self.log(f"⚠️  WARNING: {message}")

    def log_error(self, message: str):
        """Logs an error message."""
        self.log(f"❌ ERROR: {message}")

    def log_success(self, message: str):
        """Logs a success message."""
        self.log(f"✅ SUCCESS: {message}")

    def log_debug(self, message: str):
        """Logs a debug message (typically for development)."""
        self.log(f"🐛 DEBUG: {message}")

    def capture_exception(self, exception: Exception, context: str = ""):
        """Captures and logs an exception with context."""
        error_msg = f"Exception captured"
        if context:
            error_msg += f" in {context}"
        error_msg += f": {type(exception).__name__}: {str(exception)}"
        
        self.log_error(error_msg)
        
        # Log stack trace for debugging
        import traceback
        stack_trace = traceback.format_exc()
        if stack_trace and stack_trace != "None\n":
            self.log("Stack trace:")
            for line in stack_trace.split('\n'):
                if line.strip():
                    self.log(f"  {line}")

    def get_log_summary(self) -> dict:
        """Returns a summary of the logging session."""
        total_duration = (datetime.datetime.now() - self.start_time).total_seconds()
        
        # Count different types of messages
        warnings = sum(1 for line in self.log_content if '⚠️' in line)
        errors = sum(1 for line in self.log_content if '❌' in line)
        successes = sum(1 for line in self.log_content if '✅' in line)
        
        return {
            'start_time': self.start_time.isoformat(),
            'total_duration_seconds': total_duration,
            'total_messages': len(self.log_content),
            'warnings': warnings,
            'errors': errors,
            'successes': successes,
            'final_indent_level': self.indent_level
        }

    def write_master_log(self):
        """Writes the complete log to the master log file."""
        if self.master_log_path is None:
            print("⚠️  No master log path set - cannot write log file")
            return
            
        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(self.master_log_path), exist_ok=True)
            
            with open(self.master_log_path, 'w', encoding='utf-8') as f:
                # Write header
                f.write("=" * 80 + "\n")
                f.write("TRI-RESONANCE SYNTHESIS ENGINE - MASTER PROCESS LOG\n")
                f.write("=" * 80 + "\n")
                f.write(f"Generated: {datetime.datetime.now().isoformat()}\n")
                f.write(f"Start time: {self.start_time.isoformat()}\n")
                
                # Write summary
                summary = self.get_log_summary()
                f.write(f"Total duration: {summary['total_duration_seconds']:.1f}s\n")
                f.write(f"Total messages: {summary['total_messages']}\n")
                f.write(f"Warnings: {summary['warnings']}, Errors: {summary['errors']}, "
                       f"Successes: {summary['successes']}\n")
                f.write("=" * 80 + "\n\n")
                
                # Write log content
                f.write("\n".join(self.log_content))
                
            print(f"📄 Master process log written to: {self.master_log_path}")
            
        except Exception as e:
            print(f"❌ FATAL: Could not write master log to {self.master_log_path}")
            print(f"Error: {e}")

    def create_snapshot(self) -> list:
        """Creates a snapshot of the current log state."""
        return self.log_content.copy()

    def restore_snapshot(self, snapshot: list):
        """Restores the log from a snapshot."""
        self.log_content = snapshot.copy()

    def clear(self):
        """Clears the log content (use with caution)."""
        self.log_content = []
        self.start_time = datetime.datetime.now()
        self.indent_level = 0
        self.log("Log cleared and reset", header=True)

class FileLogger(ProcessLogger):
    """
    Extended logger that also writes to a dedicated file in real-time.
    Useful for long-running processes where you want immediate file output.
    """
    def __init__(self, log_file_path: str, log_to_console: bool = True):
        super().__init__(log_to_console)
        self.log_file_path = log_file_path
        self._ensure_log_file()
        
    def _ensure_log_file(self):
        """Ensures the log file exists and is ready for writing."""
        try:
            os.makedirs(os.path.dirname(self.log_file_path), exist_ok=True)
            with open(self.log_file_path, 'w', encoding='utf-8') as f:
                f.write(f"Real-time log started: {datetime.datetime.now().isoformat()}\n")
                f.write("=" * 50 + "\n")
        except Exception as e:
            print(f"❌ Could not initialize log file: {e}")

    def log(self, message: str, header: bool = False):
        """Overridden to also write to file in real-time."""
        super().log(message, header)
        
        # Also write to file
        try:
            with open(self.log_file_path, 'a', encoding='utf-8') as f:
                if header:
                    f.write(f"\n{message.upper()}\n")
                else:
                    # Get the last line added by parent class
                    if self.log_content:
                        f.write(self.log_content[-1] + "\n")
        except Exception as e:
            print(f"❌ Could not write to log file: {e}")
</div>
</div>

<!-- MODULE: manifold_explorer.py -->
<div class="module" id="manifold_explorer">
<h3>manifold_explorer.py - Manifold Explorer</h3>
<button class="copy-btn" onclick="copyCode(this)">Copy Code</button>
<div class="code">
# manifold_explorer.py: Tri-Resonance Synthesis Engine - Manifold Explorer
#
# Version: 12.0 (Definitive Build)
# Date: October 8, 2025
#
# This module sends out "canary" probes to map the local geometric and sonic
# space around a central song-state (Denotator).
# ==============================================================================

import numpy as np
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from musical_structures import Denotator
from synthesis_generative_model import GenerativeModel
from utils_output_manager import OutputGenerator
from atlas_manager import AtlasManager

class ManifoldExplorer:
    """
    Generates and analyzes a cloud of probe vectors ("canaries") around a central point.
    Creates maps of the local musical space and sonifies key regions for exploration.
    """
    def __init__(self, logger, invariants, generator: GenerativeModel, 
                 output_manager: OutputGenerator, atlas_manager: AtlasManager):
        self.logger = logger
        self.invariants = invariants
        self.generator = generator
        self.output_manager = output_manager
        self.atlas_manager = atlas_manager
        self.logger.log("ManifoldExplorer initialized - mapping the musical cosmos.")

    def explore(self, central_denotator: Denotator, num_canaries: int = 100, 
                exploration_radius: float = 0.5, create_audio: bool = True):
        """
        Generates canary vectors, creates a 2D map, and sonifies key points.
        """
        self.logger.start_section(f"EXPLORING MUSICAL SPACE AROUND: '{central_denotator.name}'")
        
        center_vec = central_denotator.vector
        if center_vec is None or center_vec.size == 0:
            self.logger.log("⚠️  Cannot explore around an empty vector")
            self.logger.end_section()
            return

        # 1. Generate Canary Vectors
        canary_vectors = self._generate_canaries(center_vec, num_canaries, exploration_radius)
        
        # 2. Create the 2D "Map" Picture
        map_path = self._generate_manifold_map(central_denotator.name, canary_vectors, center_vec)
        
        # 3. Sonify Key Canaries to get a "feel" for the space
        if create_audio:
            self._sonify_canaries(central_denotator, canary_vectors)
            
        # 4. Add to Grand Atlas
        self.atlas_manager.add_exploration(central_denotator, canary_vectors)

        self.logger.log(f"🗺️  Exploration complete: {num_canaries} canaries, radius {exploration_radius}")
        self.logger.end_section()

    def _generate_canaries(self, center_vec: np.ndarray, num_canaries: int, 
                          radius: float) -> np.ndarray:
        """Generates canary vectors around the center point."""
        self.logger.log(f"🕊️  Generating {num_canaries} canary vectors...")
        
        canary_vectors = [center_vec]  # The center is point 0
        
        for i in range(num_canaries):
            # Create random direction vector
            direction = np.random.randn(len(center_vec))
            direction = direction / np.linalg.norm(direction)  # Normalize
            
            # Vary the distance - some close, some far
            distance = radius * (0.2 + 0.8 * np.random.random())  # 20-100% of radius
            
            # Create canary
            canary = center_vec + distance * direction
            canary_vectors.append(canary)
        
        canary_vectors = np.array(canary_vectors)
        
        self.logger.log(f"  ↳ Generated {len(canary_vectors)} vectors " +
                       f"(center + {num_canaries} canaries)")
        return canary_vectors

    def _generate_manifold_map(self, name: str, vectors: np.ndarray, 
                             center_vec: np.ndarray) -> str:
        """Projects high-dimensional vectors to 2D and saves an image."""
        filepath = self.output_manager.output_dir + f"/{name}_ManifoldMap.png"
        self.logger.log(f"🎨 Generating 2D manifold map: {filepath}")
        
        try:
            if len(vectors) < 3:
                self.logger.log("⚠️  Not enough vectors for manifold mapping")
                return ""
                
            # Use PCA for dimensionality reduction
            pca = PCA(n_components=2)
            transformed = pca.fit_transform(vectors)
            
            # Calculate some statistics
            center_2d = transformed[0]  # First point is the center
            canaries_2d = transformed[1:]
            
            explained_variance = pca.explained_variance_ratio_.sum()
            
            # Create the plot
            plt.figure(figsize=(12, 10), facecolor='black')
            ax = plt.gca()
            ax.set_facecolor('black')
            
            # Plot canaries with color based on distance from center
            distances = np.linalg.norm(canaries_2d - center_2d, axis=1)
            colors = cm.viridis(distances / np.max(distances))
            
            scatter = plt.scatter(canaries_2d[:, 0], canaries_2d[:, 1], 
                                c=colors, alpha=0.7, s=50, 
                                label=f'Canaries ({len(canaries_2d)})')
            
            # Plot center point
            plt.scatter(center_2d[0], center_2d[1], c='red', s=200, 
                       marker='*', edgecolors='white', linewidth=2,
                       label=f'Center: {name}')
            
            # Add some annotations for interesting points
            if len(canaries_2d) > 5:
                # Find farthest canary
                farthest_idx = np.argmax(distances)
                farthest = canaries_2d[farthest_idx]
                plt.annotate('Farthest', xy=farthest, xytext=(10, 10),
                           textcoords='offset points', color='white',
                           arrowprops=dict(arrowstyle='->', color='white', alpha=0.7))
                
                # Find closest canary
                closest_idx = np.argmin(distances)
                closest = canaries_2d[closest_idx]
                plt.annotate('Closest', xy=closest, xytext=(10, -15),
                           textcoords='offset points', color='white',
                           arrowprops=dict(arrowstyle='->', color='white', alpha=0.7))
            
            # Styling
            plt.title(f"Manifold Map for {name}\n"
                     f"PCA Explained Variance: {explained_variance:.1%}", 
                     color='white', fontsize=14, pad=20)
            plt.xlabel("Principal Component 1", color='white', fontsize=12)
            plt.ylabel("Principal Component 2", color='white', fontsize=12)
            
            ax.tick_params(axis='x', colors='white')
            ax.tick_params(axis='y', colors='white')
            
            # Legend
            legend = plt.legend(facecolor='#333333', edgecolor='white', 
                              labelcolor='white', fontsize=10)
            legend.get_frame().set_alpha(0.8)
            
            # Grid
            plt.grid(True, linestyle='--', alpha=0.3, color='white')
            
            # Colorbar for distances
            cbar = plt.colorbar(scatter)
            cbar.set_label('Distance from Center', color='white')
            cbar.ax.yaxis.set_tick_params(color='white')
            cbar.outline.set_edgecolor('white')
            plt.setp(plt.getp(cbar.ax, 'yticklabels'), color='white')
            
            plt.tight_layout()
            plt.savefig(filepath, dpi=150, facecolor='black', 
                       edgecolor='none', bbox_inches='tight')
            plt.close()
            
            self.logger.log(f"  ✅ Map saved: {explained_variance:.1%} variance explained")
            return filepath
            
        except Exception as e:
            self.logger.log(f"❌ ERROR generating manifold map: {e}")
            return ""

    def _sonify_canaries(self, central_denotator: Denotator, vectors: np.ndarray):
        """Renders short audio clips for the most representative canaries."""
        self.logger.log("🎵 Sonifying representative canaries...")
        
        center_vec = vectors[0]  # First vector is the center
        canary_vectors = vectors[1:]  # Rest are canaries
        
        # Calculate distances from center
        distances = np.array([np.linalg.norm(v - center_vec) for v in canary_vectors])
        
        # Select canaries to sonify
        indices_to_sonify = self._select_canaries_to_sonify(distances, max_sonifications=8)
        
        sonification_count = 0
        for i in indices_to_sonify:
            canary_vec = canary_vectors[i]
            distance = distances[i]
            canary_name = f"{central_denotator.name}_Canary_{i:03d}"
            
            # Create a temporary Denotator for rendering
            canary_denotator = Denotator(canary_name, self.logger, self.invariants, 
                                       central_denotator.harmony_engine, canary_vec)
            
            # Render a short, 5-second clip
            try:
                audio_clip, sr = self.generator.render(canary_denotator, duration=5)
                self.output_manager.write_audio(canary_name, audio_clip, sr)
                sonification_count += 1
                
                self.logger.log(f"  🎶 Sonified canary {i}: distance={distance:.3f}")
                
            except Exception as e:
                self.logger.log(f"⚠️  Failed to sonify canary {i}: {e}")
                
        self.logger.log(f"  ✅ Sonified {sonification_count} canaries")

    def _select_canaries_to_sonify(self, distances: np.ndarray, max_sonifications: int = 8) -> list:
        """Selects a diverse set of canaries to sonify."""
        if len(distances) <= max_sonifications:
            return list(range(len(distances)))
            
        # Select canaries at different distances
        sorted_indices = np.argsort(distances)
        
        # Choose representatives from different distance ranges
        selection = set()
        
        # Always include closest and farthest
        selection.add(sorted_indices[0])  # Closest
        selection.add(sorted_indices[-1]) # Farthest
        
        # Select from quartiles
        quartile_size = len(sorted_indices) // 4
        for q in range(1, 4):
            idx = sorted_indices[q * quartile_size]
            selection.add(idx)
            
        # Fill remaining slots with random selection
        while len(selection) < min(max_sonifications, len(distances)):
            random_idx = np.random.randint(0, len(distances))
            selection.add(random_idx)
            
        return list(selection)[:max_sonifications]

    def explore_direction(self, central_denotator: Denotator, direction: np.ndarray,
                         num_steps: int = 10, step_size: float = 0.1) -> list:
        """
        Explores a specific direction in the musical space.
        Returns the denotators along this exploration path.
        """
        self.logger.log(f"🧭 Exploring specific direction: {num_steps} steps")
        
        center_vec = central_denotator.vector
        if center_vec is None or center_vec.size == 0:
            return []
            
        # Ensure direction vector is normalized
        direction = direction / np.linalg.norm(direction)
        
        denotators_along_path = []
        
        for step in range(num_steps + 1):  # Include center (step 0)
            distance = step * step_size
            point_vec = center_vec + distance * direction
            
            point_name = f"{central_denotator.name}_dir_{step:02d}"
            point_denotator = Denotator(point_name, self.logger, self.invariants,
                                      central_denotator.harmony_engine, point_vec)
            
            denotators_along_path.append(point_denotator)
            
        self.logger.log(f"  ↳ Generated {len(denotators_along_path)} points along direction")
        return denotators_along_path

    def analyze_local_geometry(self, vectors: np.ndarray) -> dict:
        """
        Analyzes the local geometry of the point cloud.
        Returns metrics like density, curvature, and connectivity.
        """
        if len(vectors) < 5:
            return {}
            
        analysis = {}
        
        # Calculate density (average distance to k-nearest neighbors)
        from sklearn.neighbors import NearestNeighbors
        nbrs = NearestNeighbors(n_neighbors=min(5, len(vectors))